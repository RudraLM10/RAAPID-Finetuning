{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 31041,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def install_packages():\n",
        "    \"\"\"Installs required packages if not already present.\"\"\"\n",
        "    packages = [\"tiktoken\", \"matplotlib\"]\n",
        "    for package in packages:\n",
        "        try:\n",
        "            __import__(package)\n",
        "            print(f\"{package} is already installed.\")\n",
        "        except ImportError:\n",
        "            print(f\"Installing {package}...\")\n",
        "            try:\n",
        "                subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
        "                print(f\"{package} installed successfully.\")\n",
        "            except subprocess.CalledProcessError as e:\n",
        "                print(f\"Error installing {package}: {e}\")\n",
        "                # Optionally, exit or raise an error if a critical package fails to install\n",
        "                # sys.exit(f\"Failed to install critical package: {package}\")\n",
        "\n",
        "# Call package installation at the beginning\n",
        "# In a real Kaggle notebook, these might be !pip install commands in a cell.\n",
        "# For a script, this approach is more robust.\n",
        "# install_packages() # Uncomment if running in an environment where packages might be missing\n",
        "\n",
        "import json\n",
        "# import os # Already imported\n",
        "import urllib\n",
        "import ssl\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from functools import partial\n",
        "import tiktoken\n",
        "import time\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import logging\n",
        "from pathlib import Path\n",
        "\n",
        "import requests  # Make sure requests is installed\n",
        "# import json # Already imported\n",
        "# import numpy as np # Already imported\n",
        "import tensorflow as tf\n",
        "# from tqdm import tqdm # Already imported"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-28T12:32:33.415302Z",
          "iopub.execute_input": "2025-05-28T12:32:33.415914Z",
          "iopub.status.idle": "2025-05-28T12:32:53.061495Z",
          "shell.execute_reply.started": "2025-05-28T12:32:33.415859Z",
          "shell.execute_reply": "2025-05-28T12:32:53.060616Z"
        },
        "id": "n2H9wkjq5wIN"
      },
      "outputs": [],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "source": [
        "def download_and_load_gpt2(model_size, models_dir):\n",
        "    # Validate model size\n",
        "    allowed_sizes = (\"124M\", \"355M\", \"774M\", \"1558M\")\n",
        "    if model_size not in allowed_sizes:\n",
        "        raise ValueError(f\"Model size not in {allowed_sizes}\")\n",
        "\n",
        "    # Define paths\n",
        "    model_dir = os.path.join(models_dir, model_size)\n",
        "    base_url = \"https://openaipublic.blob.core.windows.net/gpt-2/models\"\n",
        "    filenames = [\n",
        "        \"checkpoint\", \"encoder.json\", \"hparams.json\",\n",
        "        \"model.ckpt.data-00000-of-00001\", \"model.ckpt.index\",\n",
        "        \"model.ckpt.meta\", \"vocab.bpe\"\n",
        "    ]\n",
        "\n",
        "    # Download files\n",
        "    os.makedirs(model_dir, exist_ok=True)\n",
        "    for filename in filenames:\n",
        "        file_url = os.path.join(base_url, model_size, filename)\n",
        "        file_path = os.path.join(model_dir, filename)\n",
        "        download_file(file_url, file_path)\n",
        "\n",
        "    ## We have reached here until now ---> we have downloaded the files on our local machine.\n",
        "\n",
        "    # Load settings and params\n",
        "    tf_ckpt_path = tf.train.latest_checkpoint(model_dir)\n",
        "    settings = json.load(open(os.path.join(model_dir, \"hparams.json\")))\n",
        "    params = load_gpt2_params_from_tf_ckpt(tf_ckpt_path, settings)\n",
        "\n",
        "    return settings, params\n",
        "\n",
        "def download_file(url, destination):\n",
        "    try:\n",
        "        # Send a GET request to download the file, disabling SSL verification\n",
        "        response = requests.get(url, stream=True, verify=False)\n",
        "\n",
        "        # Get the total file size from headers, defaulting to 0 if not present\n",
        "        file_size = int(response.headers.get(\"content-length\", 0))\n",
        "\n",
        "        # Check if file exists and has the same size\n",
        "        if os.path.exists(destination):\n",
        "            file_size_local = os.path.getsize(destination)\n",
        "            if file_size == file_size_local:\n",
        "                print(f\"File already exists and is up-to-date: {destination}\")\n",
        "                return\n",
        "\n",
        "        # Define the block size for reading the file\n",
        "        block_size = 1024  # 1 Kilobyte\n",
        "\n",
        "        # Initialize the progress bar with total file size\n",
        "        progress_bar_description = url.split(\"/\")[-1]  # Extract filename from URL\n",
        "        with tqdm(total=file_size, unit=\"iB\", unit_scale=True, desc=progress_bar_description) as progress_bar:\n",
        "            # Open the destination file in binary write mode\n",
        "            with open(destination, \"wb\") as file:\n",
        "                # Iterate over the file data in chunks\n",
        "                for chunk in response.iter_content(block_size):\n",
        "                    progress_bar.update(len(chunk))  # Update progress bar\n",
        "                    file.write(chunk)  # Write the chunk to the file\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error downloading the file: {e}\")\n",
        "        print(f\"Please check the URL: {url}\")\n",
        "\n",
        "def load_gpt2_params_from_tf_ckpt(ckpt_path, settings):\n",
        "    # Initialize parameters dictionary with empty blocks for each layer\n",
        "    params = {\"blocks\": [{} for _ in range(settings[\"n_layer\"])]}\n",
        "\n",
        "    # Iterate over each variable in the checkpoint\n",
        "    for name, _ in tf.train.list_variables(ckpt_path):\n",
        "        # Load the variable and remove singleton dimensions\n",
        "        variable_array = np.squeeze(tf.train.load_variable(ckpt_path, name))\n",
        "\n",
        "        # Process the variable name to extract relevant parts\n",
        "        variable_name_parts = name.split(\"/\")[1:]  # Skip the 'model/' prefix\n",
        "\n",
        "        # Identify the target dictionary for the variable\n",
        "        target_dict = params\n",
        "        if variable_name_parts[0].startswith(\"h\"):\n",
        "            layer_number = int(variable_name_parts[0][1:])\n",
        "            target_dict = params[\"blocks\"][layer_number]\n",
        "\n",
        "        # Recursively access or create nested dictionaries\n",
        "        for key in variable_name_parts[1:-1]:\n",
        "            target_dict = target_dict.setdefault(key, {})\n",
        "\n",
        "        # Assign the variable array to the last key\n",
        "        last_key = variable_name_parts[-1]\n",
        "        target_dict[last_key] = variable_array\n",
        "\n",
        "    return params"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-28T12:32:53.062780Z",
          "iopub.execute_input": "2025-05-28T12:32:53.063326Z",
          "iopub.status.idle": "2025-05-28T12:32:53.074014Z",
          "shell.execute_reply.started": "2025-05-28T12:32:53.063305Z",
          "shell.execute_reply": "2025-05-28T12:32:53.073298Z"
        },
        "id": "9_x-VmEV5wIS"
      },
      "outputs": [],
      "execution_count": 2
    },
    {
      "cell_type": "code",
      "source": [
        "# Import GRITAdapter components (assuming these are part of the script or accessible)\n",
        "# import torch # Already imported\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "# Configure basic logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
        "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Constants and Configuration\n",
        "RANDOM_SEED = 42\n",
        "OUTPUT_DIR = Path(\"./output_grit_finetune\")\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# Add MPS support for Apple Silicon if needed\n",
        "# if torch.backends.mps.is_available() and DEVICE == \"cpu\":\n",
        "#     DEVICE = \"mps\"\n",
        "#     logger.info(\"Using MPS device for Apple Silicon.\")\n",
        "\n",
        "\n",
        "# Create output directory if it doesn't exist\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def set_seeds(seed: int = RANDOM_SEED):\n",
        "    \"\"\"Sets random seeds for reproducibility.\"\"\"\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    logger.info(f\"Random seeds set to {seed}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-28T12:32:53.074820Z",
          "iopub.execute_input": "2025-05-28T12:32:53.075109Z",
          "iopub.status.idle": "2025-05-28T12:32:53.212140Z",
          "shell.execute_reply.started": "2025-05-28T12:32:53.075083Z",
          "shell.execute_reply": "2025-05-28T12:32:53.211449Z"
        },
        "id": "rQ4yzic25wIT"
      },
      "outputs": [],
      "execution_count": 3
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STAGE 0: INFRASTRUCTURE AND MODEL COMPONENTS (GRIT ADAPTER)"
      ],
      "metadata": {
        "id": "s5LnehKN5wIT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stage 1: Infrastructure Setup"
      ],
      "metadata": {
        "id": "FBL510Gt5wIV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-28T12:32:53.213895Z",
          "iopub.execute_input": "2025-05-28T12:32:53.214114Z",
          "iopub.status.idle": "2025-05-28T12:32:53.228344Z",
          "shell.execute_reply.started": "2025-05-28T12:32:53.214099Z",
          "shell.execute_reply": "2025-05-28T12:32:53.227679Z"
        },
        "id": "W-O07jIN5wIV"
      },
      "outputs": [],
      "execution_count": 4
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stage 1: Infrastructure Setup\n"
      ],
      "metadata": {
        "id": "8EgGDduL5wIV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DampedKFAC(nn.Module):\n",
        "    # Assuming 'd,d' were placeholders for feature dimensions\n",
        "    def __init__(self, in_features: int, out_features: int, ema_decay: float = 0.95, damping: tuple[float, float] = (1e-7, 1e-5)):\n",
        "        super().__init__()\n",
        "        self.ema_decay = ema_decay\n",
        "        self.damping = damping # Tuple (damping_A, damping_B)\n",
        "\n",
        "        # Using register_buffer for A_accum and B_accum\n",
        "        self.register_buffer('A_accum', torch.eye(in_features))\n",
        "        self.register_buffer('B_accum', torch.eye(out_features))\n",
        "        self.A_initialized = False\n",
        "        self.B_initialized = False\n",
        "\n",
        "    def update(self, current_A_cov: torch.Tensor, current_B_cov: torch.Tensor = None):\n",
        "        \"\"\"\n",
        "        Update A_accum or B_accum with new covariance.\n",
        "        If current_B_cov is None, only A_accum is updated.\n",
        "        If current_A_cov is None, only B_accum is updated.\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            if current_A_cov is not None:\n",
        "                if not self.A_initialized:\n",
        "                    self.A_accum.copy_(current_A_cov)\n",
        "                    self.A_initialized = True\n",
        "                else:\n",
        "                    self.A_accum.mul_(self.ema_decay).add_(current_A_cov, alpha=(1 - self.ema_decay))\n",
        "\n",
        "            if current_B_cov is not None:\n",
        "                if not self.B_initialized:\n",
        "                    self.B_accum.copy_(current_B_cov)\n",
        "                    self.B_initialized = True\n",
        "                else:\n",
        "                    self.B_accum.mul_(self.ema_decay).add_(current_B_cov, alpha=(1 - self.ema_decay))\n",
        "\n",
        "    def get_factors(self) -> tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"Returns the current A_accum and B_accum factors.\"\"\"\n",
        "        return self.A_accum, self.B_accum\n",
        "\n",
        "    def get_inverse_factors(self) -> tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"Computes and returns damped inverses (A_inv, B_inv).\"\"\"\n",
        "        with torch.no_grad():\n",
        "            device = self.A_accum.device\n",
        "\n",
        "            A_damped = self.A_accum + self.damping[0] * torch.eye(self.A_accum.size(0), device=device)\n",
        "            B_damped = self.B_accum + self.damping[1] * torch.eye(self.B_accum.size(0), device=device)\n",
        "\n",
        "            try:\n",
        "                A_inv = torch.linalg.pinv(A_damped)\n",
        "            except Exception as e:\n",
        "                # print(f\"Error inverting A_damped: {e}. Returning identity.\")\n",
        "                A_inv = torch.eye(self.A_accum.size(0), device=device)\n",
        "\n",
        "            try:\n",
        "                B_inv = torch.linalg.pinv(B_damped)\n",
        "            except Exception as e:\n",
        "                # print(f\"Error inverting B_damped: {e}. Returning identity.\")\n",
        "                B_inv = torch.eye(self.B_accum.size(0), device=device)\n",
        "\n",
        "            return A_inv, B_inv\n",
        "\n",
        "    def __repr__(self):\n",
        "        return (f\"DampedKFAC(in={self.A_accum.shape[0]}, out={self.B_accum.shape[0]}, \"\n",
        "                f\"ema_decay={self.ema_decay}, damping={self.damping}, \"\n",
        "                f\"A_init={self.A_initialized}, B_init={self.B_initialized})\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-28T12:32:53.229023Z",
          "iopub.execute_input": "2025-05-28T12:32:53.229222Z",
          "iopub.status.idle": "2025-05-28T12:32:53.249242Z",
          "shell.execute_reply.started": "2025-05-28T12:32:53.229207Z",
          "shell.execute_reply": "2025-05-28T12:32:53.248586Z"
        },
        "id": "RQU7dB_s5wIW"
      },
      "outputs": [],
      "execution_count": 5
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Curvature Estimation Framework Implementation\n",
        "# 1.1 K-FAC Layer Specialization\n",
        "\n",
        "# Stage 1: Infrastructure Setup\n",
        "\n",
        "class KFACLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Handles K-FAC's activation and gradient covariance estimation for a given module.\n",
        "    Uses an instance of DampedKFAC to manage EMA updates and factor inversions.\n",
        "    \"\"\"\n",
        "    def __init__(self, module: nn.Linear, layer_type: str, ema_decay: float = 0.95, kfac_damping: tuple[float, float] = (1e-7, 1e-5)):\n",
        "        super().__init__()\n",
        "        self.layer_type = layer_type\n",
        "        # self.grit_adapter_ref = grit_adapter_ref # REMOVED\n",
        "        # ema_decay and kfac_damping are now for the DampedKFAC instance\n",
        "\n",
        "        # REMOVED premature device setting logic\n",
        "        # try:\n",
        "        #     device = next(module.parameters()).device\n",
        "        # except StopIteration:\n",
        "        #     device = torch.device(\"cpu\")\n",
        "\n",
        "        in_features = module.in_features\n",
        "        out_features = module.out_features\n",
        "\n",
        "        # Instantiate DampedKFAC\n",
        "        # Make sure DampedKFAC is defined before this class is used.\n",
        "        self.kfac_manager = DampedKFAC(in_features, out_features, ema_decay, kfac_damping)\n",
        "        # self.kfac_manager.to(device) # REMOVED - let parent .to() handle this\n",
        "\n",
        "    def forward_hook(self, module: nn.Module, input_data: tuple[torch.Tensor], output_data: torch.Tensor):\n",
        "        \"\"\"\n",
        "        Hook to compute activation covariance (A_factor) using E[aa^T] estimator.\n",
        "        input_data is a tuple, input_data[0] is the actual input tensor.\n",
        "        Uses DampedKFAC to update A_accum.\n",
        "        Incorporates suggested reshaping for 3D/4D inputs.\n",
        "        \"\"\"\n",
        "        x = input_data[0] # .detach() <-- REMOVED .detach()\n",
        "\n",
        "        # Suggested reshaping\n",
        "        if x.ndim == 3:  # (batch_size, seq_len, dim)\n",
        "            act_reshaped = x.reshape(-1, x.shape[-1])  # (batch_size*seq_len, dim)\n",
        "        elif x.ndim == 4:  # Conv2d inputs (batch_size, channels, H, W)\n",
        "            # Reshape to (batch_size*H*W, channels) if module is Conv2d-like\n",
        "            # For nn.Linear, this path shouldn't be hit if input is already (B, Cin)\n",
        "            # This part needs care depending on whether KFACLayer is attached to Conv2d\n",
        "            # Original KFACLayer was for nn.Linear. If KFACLayer is generalized,\n",
        "            # input features for Conv2d (e.g. unfolded patches) would be different.\n",
        "            # For now, assuming if it's Linear, ndim won't be 4 unless an error.\n",
        "            # If it IS a Conv2D's activations flattened for a subsequent Linear, this is complex.\n",
        "            act_reshaped = x.reshape(-1, x.shape[1]) # (batch_size*H*W, channels)\n",
        "                                                   # This assumes x.shape[1] is 'in_features'\n",
        "        elif x.ndim > 2: # General case for >2D not explicitly handled by 3D/4D\n",
        "            act_reshaped = x.reshape(-1, x.shape[-1])\n",
        "        else: # ndim <= 2\n",
        "            act_reshaped = x\n",
        "\n",
        "        if act_reshaped.shape[0] == 0: return\n",
        "\n",
        "        current_A_factor_dim = self.kfac_manager.A_accum.shape[0]\n",
        "        if act_reshaped.shape[1] != current_A_factor_dim:\n",
        "            print(f\"Warning: Activation dimension mismatch for KFACLayer {self.layer_type}. Expected {current_A_factor_dim}, got {act_reshaped.shape[1]}. Skipping A_factor update.\")\n",
        "            return\n",
        "\n",
        "        # Covariance of activations: E[aa^T] estimated by (act.T @ act) / N\n",
        "        # Snippet: cov = (x.T @ x) / x.size(0)\n",
        "        # Using act_reshaped for x\n",
        "        cov_a = (act_reshaped.T @ act_reshaped) / act_reshaped.shape[0]\n",
        "\n",
        "        # Update using DampedKFAC. Current_B_cov is None as this is forward pass.\n",
        "        self.kfac_manager.update(current_A_cov=cov_a, current_B_cov=None)\n",
        "\n",
        "    def backward_hook(self, module: nn.Module, grad_input: tuple[torch.Tensor], grad_output_data: tuple[torch.Tensor]):\n",
        "        \"\"\"\n",
        "        Hook to compute gradient covariance (B_factor) using E[gg^T] estimator.\n",
        "        grad_output_data is a tuple, grad_output_data[0] is the gradient w.r.t. module's output.\n",
        "        Uses DampedKFAC to update B_accum.\n",
        "        \"\"\"\n",
        "\n",
        "        # The gradient g will be from the main loss (empirical Fisher) or\n",
        "        # from the KFAC-specific loss (true Fisher) depending on what backward pass triggered this hook.\n",
        "        # The training loop is responsible for orchestrating this.\n",
        "        g = grad_output_data[0]\n",
        "\n",
        "        # Reshaping for gradients (similar to activations)\n",
        "        if g.ndim > 2:\n",
        "            grad_reshaped = g.reshape(-1, g.shape[-1])\n",
        "        else:\n",
        "            grad_reshaped = g\n",
        "\n",
        "        if grad_reshaped.shape[0] == 0: return\n",
        "\n",
        "        current_B_factor_dim = self.kfac_manager.B_accum.shape[0]\n",
        "        if grad_reshaped.shape[1] != current_B_factor_dim:\n",
        "            print(f\"Warning: Gradient dimension mismatch for KFACLayer {self.layer_type}. Expected {current_B_factor_dim}, got {grad_reshaped.shape[1]}. Skipping B_factor update.\")\n",
        "            return\n",
        "\n",
        "        # Covariance of gradients: E[gg^T] estimated by (grad.T @ grad) / N\n",
        "        cov_g = (grad_reshaped.T @ grad_reshaped) / grad_reshaped.shape[0]\n",
        "\n",
        "        # Update using DampedKFAC. Current_A_cov is None as this is backward pass.\n",
        "        self.kfac_manager.update(current_A_cov=None, current_B_cov=cov_g)\n",
        "\n",
        "    # Expose factors for external use if needed (e.g. by FisherEigen or other parts)\n",
        "    @property\n",
        "    def A_factor(self):\n",
        "        return self.kfac_manager.A_accum\n",
        "\n",
        "    @property\n",
        "    def B_factor(self):\n",
        "        return self.kfac_manager.B_accum\n",
        "\n",
        "    def get_inverse_factors(self):\n",
        "        return self.kfac_manager.get_inverse_factors()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-28T12:32:53.250413Z",
          "iopub.execute_input": "2025-05-28T12:32:53.250994Z",
          "iopub.status.idle": "2025-05-28T12:32:53.268310Z",
          "shell.execute_reply.started": "2025-05-28T12:32:53.250972Z",
          "shell.execute_reply": "2025-05-28T12:32:53.267744Z"
        },
        "id": "QW150gyp5wIW"
      },
      "outputs": [],
      "execution_count": 6
    },
    {
      "cell_type": "code",
      "source": [
        "def instrument_layer(transformer_block: nn.Module, target_module_names: list[str] = ['att', 'ff', 'attn', 'mlp'], grit_adapter_ref: nn.Module = None) -> list[KFACLayer]:\n",
        "    \"\"\"\n",
        "    Instruments nn.Linear layers (or FlexLoRA wrapped nn.Linear layers)\n",
        "    within specified submodules of a transformer_block with KFACLayer handlers.\n",
        "    Returns a list of created KFACLayer handlers.\n",
        "    \"\"\"\n",
        "    kfac_handlers = []\n",
        "    block_class_name = transformer_block.__class__.__name__\n",
        "\n",
        "    # Get KFAC hyperparameters from GRITAdapter if available\n",
        "    kfac_ema_decay_from_adapter = 0.95  # Default\n",
        "    kfac_damping_from_adapter = (1e-7, 1e-5)  # Default\n",
        "    if grit_adapter_ref is not None:\n",
        "        if hasattr(grit_adapter_ref, 'kfac_ema_decay'):\n",
        "            kfac_ema_decay_from_adapter = grit_adapter_ref.kfac_ema_decay\n",
        "        if hasattr(grit_adapter_ref, 'kfac_damping'):\n",
        "            kfac_damping_from_adapter = grit_adapter_ref.kfac_damping\n",
        "\n",
        "    for target_name in target_module_names:\n",
        "        if hasattr(transformer_block, target_name):\n",
        "            sub_module = getattr(transformer_block, target_name)\n",
        "            # Iterate over named modules within this sub_module (e.g., layers in an FFN sequential)\n",
        "            for layer_name_in_submodule, current_layer_candidate in sub_module.named_modules():\n",
        "\n",
        "                actual_linear_to_instrument = None\n",
        "                # The module on which .kfac_handler will be set (e.g., the FlexLoRA instance or the nn.Linear itself)\n",
        "                module_for_handler_attribute = current_layer_candidate\n",
        "\n",
        "                if isinstance(current_layer_candidate, FlexLoRA):\n",
        "                    if isinstance(current_layer_candidate.base_layer, nn.Linear):\n",
        "                        actual_linear_to_instrument = current_layer_candidate.base_layer\n",
        "                    else:\n",
        "                        # This case should ideally not happen due to FlexLoRA's init check\n",
        "                        print(f\"Warning: FlexLoRA's base_layer for {layer_name_in_submodule} in {target_name} of block {block_class_name} is not nn.Linear. Got {type(current_layer_candidate.base_layer)}. Skipping KFAC.\")\n",
        "                        continue\n",
        "                elif isinstance(current_layer_candidate, nn.Linear):\n",
        "                    actual_linear_to_instrument = current_layer_candidate\n",
        "                else:\n",
        "                    # Not a direct nn.Linear or a FlexLoRA wrapping one at this level of named_modules iteration.\n",
        "                    # This skips other module types like GELU, Dropout, Sequential containers themselves, etc.\n",
        "                    continue\n",
        "\n",
        "                if actual_linear_to_instrument:\n",
        "                    # Check if the *actual linear layer* has already been instrumented by us\n",
        "                    if hasattr(actual_linear_to_instrument, '_kfac_instrumented_by_grit_'):\n",
        "                        continue\n",
        "\n",
        "                    # Construct a descriptive layer_type for KFACLayer\n",
        "                    hierarchical_name_part = \"\"\n",
        "                    if not layer_name_in_submodule:\n",
        "                        hierarchical_name_part = target_name\n",
        "                    else:\n",
        "                        hierarchical_name_part = f\"{target_name}.{layer_name_in_submodule}\"\n",
        "                    full_layer_name = f\"{block_class_name}.{hierarchical_name_part}\"\n",
        "\n",
        "                    kfac_layer_instance = KFACLayer(\n",
        "                        actual_linear_to_instrument,\n",
        "                        full_layer_name,\n",
        "                        ema_decay=kfac_ema_decay_from_adapter,\n",
        "                        kfac_damping=kfac_damping_from_adapter\n",
        "                    )\n",
        "\n",
        "                    actual_linear_to_instrument.register_forward_hook(kfac_layer_instance.forward_hook)\n",
        "                    actual_linear_to_instrument.register_full_backward_hook(kfac_layer_instance.backward_hook)\n",
        "\n",
        "                    # Set the handler on the FlexLoRA wrapper if present, or the linear layer itself\n",
        "                    module_for_handler_attribute.kfac_handler = kfac_layer_instance\n",
        "\n",
        "                    # Mark the *actual linear layer* as instrumented by this process\n",
        "                    actual_linear_to_instrument._kfac_instrumented_by_grit_ = True\n",
        "\n",
        "                    kfac_handlers.append(kfac_layer_instance)\n",
        "\n",
        "    if not kfac_handlers:\n",
        "        print(f\"Warning: No KFAC handlers were created for block {block_class_name} with target modules {target_module_names}. Check layer names and structure.\")\n",
        "\n",
        "    return kfac_handlers"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-28T12:32:53.269073Z",
          "iopub.execute_input": "2025-05-28T12:32:53.269336Z",
          "iopub.status.idle": "2025-05-28T12:32:53.288290Z",
          "shell.execute_reply.started": "2025-05-28T12:32:53.269312Z",
          "shell.execute_reply": "2025-05-28T12:32:53.287605Z"
        },
        "id": "4NeU3p5l5wIX"
      },
      "outputs": [],
      "execution_count": 7
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Low-Rank Adaptation Core\n"
      ],
      "metadata": {
        "id": "SPyys0tK5wIX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.1 Rank-Flexible LoRA Module\n",
        "\n",
        "class FlexLoRA(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements Low-Rank Adaptation (LoRA) for an nn.Linear layer.\n",
        "    The base layer's weights are frozen, and LoRA matrices (A and B) are trained.\n",
        "    Includes attributes for dynamic rank adjustment.\n",
        "    \"\"\"\n",
        "    def __init__(self, base_layer: nn.Linear, initial_rank: int = 8, min_rank: int = 4, max_rank: int = 16):\n",
        "        super().__init__()\n",
        "        if not isinstance(base_layer, nn.Linear):\n",
        "            raise ValueError(f\"FlexLoRA currently only supports nn.Linear, got {type(base_layer)}\")\n",
        "\n",
        "        self.base_layer = base_layer\n",
        "        self.initial_rank = initial_rank\n",
        "        self.current_rank = initial_rank # Initialize current_rank with initial_rank\n",
        "        self.min_rank = min_rank\n",
        "        self.max_rank = max_rank\n",
        "\n",
        "        self.base_layer.weight.requires_grad = False\n",
        "        if self.base_layer.bias is not None:\n",
        "            self.base_layer.bias.requires_grad = False\n",
        "\n",
        "        device = base_layer.weight.device\n",
        "        # LoRA parameters dimensions are based on current_rank (which is initial_rank at creation)\n",
        "        self.lora_A = nn.Parameter(torch.randn(base_layer.in_features, self.current_rank, device=device))\n",
        "        self.lora_B = nn.Parameter(torch.zeros(self.current_rank, base_layer.out_features, device=device))\n",
        "\n",
        "        self.bypass = nn.Identity()\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        base_out = self.base_layer(x)\n",
        "\n",
        "        if self.current_rank > 0: # Check current_rank for LoRA path\n",
        "            lora_adapt = (x @ self.lora_A) @ self.lora_B\n",
        "            return self.bypass(base_out) + lora_adapt\n",
        "        return base_out\n",
        "\n",
        "    def __repr__(self):\n",
        "        return (f\"{self.__class__.__name__}(initial_rank={self.initial_rank}, current_rank={self.current_rank}, \"\n",
        "                f\"min_rank={self.min_rank}, max_rank={self.max_rank}, \"\n",
        "                f\"base_layer={self.base_layer.__class__.__name__}({self.base_layer.in_features}x{self.base_layer.out_features}))\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-28T12:32:53.288982Z",
          "iopub.execute_input": "2025-05-28T12:32:53.289260Z",
          "iopub.status.idle": "2025-05-28T12:32:53.311456Z",
          "shell.execute_reply.started": "2025-05-28T12:32:53.289239Z",
          "shell.execute_reply": "2025-05-28T12:32:53.310899Z"
        },
        "id": "uobLnaNs5wIY"
      },
      "outputs": [],
      "execution_count": 8
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.2 Layer Modification Protocol\n",
        "\n",
        "def lora_inject(model: nn.Module, layers_to_adapt_substrings: list[str],\n",
        "                rank_range: tuple[int, int] = (8, 64)) -> nn.Module:\n",
        "    \"\"\"\n",
        "    Replaces specified nn.Linear layers in the model with FlexLoRA wrappers.\n",
        "    Args:\n",
        "        model: The model to modify.\n",
        "        layers_to_adapt_substrings: List of substrings. If a module's name contains\n",
        "                                    any of these, it will be adapted.\n",
        "        rank_range: Tuple of (min_rank, max_rank). min_rank is used for initial_rank.\n",
        "    Returns:\n",
        "        The modified model.\n",
        "    \"\"\"\n",
        "    min_r, max_r = rank_range\n",
        "    # Use the lower bound of the rank range as the initial rank for LoRA\n",
        "    # and pass min_r, max_r to FlexLoRA for its own min_rank, max_rank settings.\n",
        "    initial_r = min_r\n",
        "\n",
        "    for name, module in model.named_modules():\n",
        "        parent_name_parts = name.split('.')[:-1]\n",
        "        child_name = name.split('.')[-1]\n",
        "\n",
        "        parent_module = model\n",
        "        if parent_name_parts:\n",
        "            try:\n",
        "                parent_module = model.get_submodule(\".\".join(parent_name_parts))\n",
        "            except AttributeError:\n",
        "                continue\n",
        "\n",
        "        actual_module_to_check = getattr(parent_module, child_name, None)\n",
        "\n",
        "        if isinstance(actual_module_to_check, nn.Linear):\n",
        "            if any(sub in name for sub in layers_to_adapt_substrings):\n",
        "                if isinstance(actual_module_to_check, FlexLoRA):\n",
        "                    continue\n",
        "\n",
        "                original_layer = actual_module_to_check\n",
        "\n",
        "                lora_layer = FlexLoRA(original_layer,\n",
        "                                        initial_rank=initial_r,\n",
        "                                        min_rank=min_r,\n",
        "                                        max_rank=max_r)\n",
        "                setattr(parent_module, child_name, lora_layer)\n",
        "    return model"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-28T12:32:53.312189Z",
          "iopub.execute_input": "2025-05-28T12:32:53.312468Z",
          "iopub.status.idle": "2025-05-28T12:32:53.332073Z",
          "shell.execute_reply.started": "2025-05-28T12:32:53.312446Z",
          "shell.execute_reply": "2025-05-28T12:32:53.331397Z"
        },
        "id": "u1AvMw135wIY"
      },
      "outputs": [],
      "execution_count": 9
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Neural Reprojection Engine\n"
      ],
      "metadata": {
        "id": "pRhedTG55wIZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.1 Eigen Decomposition Module\n",
        "\n",
        "class FisherEigen(nn.Module):\n",
        "    \"\"\"\n",
        "    Handles Fisher block construction (via Kronecker product of KFAC's A and B factors)\n",
        "    and its eigendecomposition using LOBPCG.\n",
        "    \"\"\"\n",
        "    def __init__(self, layer_type: str = \"UnknownLayer\"):\n",
        "        super().__init__()\n",
        "        # Store A and B factors directly\n",
        "        self.A_s = None\n",
        "        self.B_s = None\n",
        "        self.layer_type = layer_type # For informative logging\n",
        "\n",
        "    def update_fisher(self, A: torch.Tensor, B: torch.Tensor):\n",
        "        \"\"\"\n",
        "        Updates the Fisher factors A_s and B_s.\n",
        "        Assumes A and B are symmetric, or uses (X + X.T)/2 for safety.\n",
        "        \"\"\"\n",
        "        A_sym = (A + A.T) / 2 if A.numel() > 0 else A\n",
        "        B_sym = (B + B.T) / 2 if B.numel() > 0 else B\n",
        "\n",
        "        if A_sym.numel() == 0 or B_sym.numel() == 0:\n",
        "            # Determine device and dtype for empty tensors\n",
        "            current_device = A.device if A.numel() > 0 else (B.device if B.numel() > 0 else torch.device('cpu'))\n",
        "            dtype_A = A.dtype if A.numel() > 0 else torch.float32\n",
        "            dtype_B = B.dtype if B.numel() > 0 else torch.float32\n",
        "\n",
        "            self.A_s = torch.empty(0, device=current_device, dtype=dtype_A)\n",
        "            self.B_s = torch.empty(0, device=current_device, dtype=dtype_B)\n",
        "            return\n",
        "\n",
        "        self.A_s = A_sym\n",
        "        self.B_s = B_sym\n",
        "\n",
        "    def decompose(self, num_requested_vectors: int) -> tuple[torch.Tensor | None, torch.Tensor | None]:\n",
        "        \"\"\"\n",
        "        Decomposes the Fisher block by direct eigendecomposition of factors A_s and B_s.\n",
        "        Forms Kronecker product eigenvalues and eigenvectors from factor eigenpairs.\n",
        "        Args:\n",
        "            num_requested_vectors: The suggested number of eigenvectors from GRITAdapter.\n",
        "        Returns:\n",
        "            Tuple of (eigenvalues, eigenvectors). Eigenvectors are column vectors, sorted by descending eigenvalues.\n",
        "            Returns (None, None) if decomposition is not possible or fails.\n",
        "        \"\"\"\n",
        "        if self.A_s is None or self.B_s is None or self.A_s.numel() == 0 or self.B_s.numel() == 0:\n",
        "            # print(f\"Warning: A_s or B_s is None or empty for layer '{self.layer_type}'. Skipping decomposition.\")\n",
        "            return None, None\n",
        "\n",
        "        device = self.A_s.device\n",
        "        # Assuming A_s and B_s have same dtype, which should be the case\n",
        "        # dtype = self.A_s.dtype\n",
        "        dim_A = self.A_s.shape[0]\n",
        "        dim_B = self.B_s.shape[0]\n",
        "        full_dim = dim_A * dim_B\n",
        "\n",
        "        if full_dim == 0:\n",
        "            # print(f\"Warning: Full dimension is 0 for layer '{self.layer_type}'. Skipping decomposition.\")\n",
        "            return None, None\n",
        "\n",
        "        # Determine the number of eigenvectors to compute, k\n",
        "        # k must be > 0 and <= full_dim.\n",
        "        k = max(1, min(num_requested_vectors, full_dim))\n",
        "        if k <= 0 :\n",
        "            # print(f\"Warning: Number of requested eigenvectors k={k} is not positive for layer '{self.layer_type}'. Skipping decomposition.\")\n",
        "            return None, None\n",
        "\n",
        "        # Fallback to the original method (direct eigendecomposition of factors)\n",
        "        # This is now the primary method.\n",
        "        return self._decompose_direct_factors(num_requested_vectors)\n",
        "\n",
        "    def _decompose_direct_factors(self, num_requested_vectors: int) -> tuple[torch.Tensor | None, torch.Tensor | None]:\n",
        "        \"\"\"\n",
        "        Decomposes Fisher block by direct eigendecomposition of factors A_s and B_s.\n",
        "        Forms Kronecker product eigenvalues and eigenvectors from factor eigenpairs.\n",
        "        Args:\n",
        "            num_requested_vectors: The suggested number of eigenvectors from GRITAdapter.\n",
        "        Returns:\n",
        "            Tuple of (eigenvalues, eigenvectors). Eigenvectors are column vectors, sorted by descending eigenvalues.\n",
        "            Returns (None, None) if decomposition is not possible or fails.\n",
        "        \"\"\"\n",
        "        if self.A_s is None or self.B_s is None or self.A_s.numel() == 0 or self.B_s.numel() == 0:\n",
        "            # print(f\"Warning: A_s or B_s is None or empty for layer '{self.layer_type}'. Skipping decomposition.\")\n",
        "            return None, None\n",
        "\n",
        "        device = self.A_s.device\n",
        "        # Assuming A_s and B_s have same dtype, which should be the case\n",
        "        # dtype = self.A_s.dtype\n",
        "        dim_A = self.A_s.shape[0]\n",
        "        dim_B = self.B_s.shape[0]\n",
        "        full_dim = dim_A * dim_B\n",
        "\n",
        "        if full_dim == 0:\n",
        "            # print(f\"Warning: Full dimension is 0 for layer '{self.layer_type}'. Skipping decomposition.\")\n",
        "            return None, None\n",
        "\n",
        "        # Determine the number of eigenvectors to compute, k\n",
        "        # k must be > 0 and <= full_dim.\n",
        "        k = max(1, min(num_requested_vectors, full_dim))\n",
        "        if k <= 0 :\n",
        "            # print(f\"Warning: Number of requested eigenvectors k={k} is not positive for layer '{self.layer_type}'. Skipping decomposition.\")\n",
        "            return None, None\n",
        "\n",
        "        try:\n",
        "            with torch.no_grad():\n",
        "                eigvals_A_asc, eigvecs_A_orig = torch.linalg.eigh(self.A_s)\n",
        "                eigvals_B_asc, eigvecs_B_orig = torch.linalg.eigh(self.B_s)\n",
        "\n",
        "                if eigvals_A_asc.numel() == 0 or eigvals_B_asc.numel() == 0:\n",
        "                    return None, None\n",
        "\n",
        "                eigvals_A = torch.flip(eigvals_A_asc, dims=[0])\n",
        "                eigvecs_A = torch.flip(eigvecs_A_orig, dims=[1])\n",
        "                eigvals_B = torch.flip(eigvals_B_asc, dims=[0])\n",
        "                eigvecs_B = torch.flip(eigvecs_B_orig, dims=[1])\n",
        "\n",
        "                all_kron_eigvals = (eigvals_A.unsqueeze(1) * eigvals_B.unsqueeze(0)).flatten()\n",
        "                sorted_kron_eigvals, sorted_flat_indices = torch.sort(all_kron_eigvals, descending=True)\n",
        "\n",
        "                top_k_eigvals = sorted_kron_eigvals[:k]\n",
        "                top_k_flat_indices = sorted_flat_indices[:k]\n",
        "\n",
        "                indices_A = top_k_flat_indices // dim_B\n",
        "                indices_B = top_k_flat_indices % dim_B\n",
        "\n",
        "                top_k_eigvecs_list = []\n",
        "                for i_val in range(len(top_k_flat_indices)):\n",
        "                    idx_a = indices_A[i_val]\n",
        "                    idx_b = indices_B[i_val]\n",
        "                    vec_a = eigvecs_A[:, idx_a]\n",
        "                    vec_b = eigvecs_B[:, idx_b]\n",
        "                    kron_vec = torch.kron(vec_b, vec_a)\n",
        "                    top_k_eigvecs_list.append(kron_vec)\n",
        "\n",
        "                if not top_k_eigvecs_list:\n",
        "                    return None, None\n",
        "                top_k_eigvecs = torch.stack(top_k_eigvecs_list, dim=1)\n",
        "                return top_k_eigvals, top_k_eigvecs\n",
        "        except Exception as e_direct:\n",
        "            print(f\"Error during fallback direct factor decomposition for layer '{self.layer_type}': {e_direct}. \"\n",
        "                  f\"Skipping decomposition entirely.\")\n",
        "            return None, None"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-28T12:32:53.334492Z",
          "iopub.execute_input": "2025-05-28T12:32:53.334729Z",
          "iopub.status.idle": "2025-05-28T12:32:53.355133Z",
          "shell.execute_reply.started": "2025-05-28T12:32:53.334716Z",
          "shell.execute_reply": "2025-05-28T12:32:53.354544Z"
        },
        "id": "jxNZ5hPx5wIZ"
      },
      "outputs": [],
      "execution_count": 10
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.2 Subspace Preservation System\n",
        "\n",
        "class SubspaceBuffer(nn.Module):\n",
        "    \"\"\"\n",
        "    Stores and manages projection masks and eigenvectors for multiple blocks/layers.\n",
        "    Uses a fixed buffer_size for eigenvector storage dimensions.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_blocks: int, buffer_size: int = 1024, device: str ='cpu'):\n",
        "        super().__init__()\n",
        "        self.num_blocks = num_blocks\n",
        "        self.buffer_size = buffer_size # Acts as max dimension and max number of vectors\n",
        "        self.device = torch.device(device)\n",
        "\n",
        "        self.subspace_module_list = nn.ModuleList() # MODIFIED HERE\n",
        "        for _ in range(num_blocks):\n",
        "            # Each buffer entry is a BufferDict\n",
        "            buffer_entry = nn.Module() # Using nn.Module to host buffers for easy device moving\n",
        "            buffer_entry.register_buffer(\n",
        "                'eigen_vectors',\n",
        "                torch.zeros(buffer_size, buffer_size, device=self.device) # (max_feature_dim, max_num_eigenvectors)\n",
        "            )\n",
        "            buffer_entry.register_buffer(\n",
        "                'projection_mask', # Indicates how many of the 'buffer_size' eigenvector slots are active\n",
        "                torch.zeros(buffer_size, device=self.device, dtype=torch.float32) # Using float for 0/1 values\n",
        "            )\n",
        "            self.subspace_module_list.append(buffer_entry) # MODIFIED HERE\n",
        "\n",
        "    def update_buffer(self, block_idx: int, new_eigenvectors: torch.Tensor):\n",
        "        \"\"\"\n",
        "        Updates the buffer for a specific block with new eigenvectors.\n",
        "        Args:\n",
        "            block_idx: Index of the layer/block.\n",
        "            new_eigenvectors: Tensor of shape (full_vector_dim, num_new_vecs).\n",
        "                              Assumes full_vector_dim <= buffer_size and num_new_vecs <= buffer_size.\n",
        "        \"\"\"\n",
        "        if not (0 <= block_idx < self.num_blocks):\n",
        "            raise IndexError(f\"block_idx {block_idx} out of range for {self.num_blocks} buffers.\")\n",
        "\n",
        "        current_buffer_entry = self.subspace_module_list[block_idx] # MODIFIED HERE\n",
        "\n",
        "        if new_eigenvectors is None or new_eigenvectors.numel() == 0:\n",
        "            # print(f\"Warning: No new eigenvectors provided for block {block_idx}. Buffer not updated.\")\n",
        "            current_buffer_entry.projection_mask.fill_(0.) # Clear mask if no new vectors\n",
        "            return\n",
        "\n",
        "        actual_feature_dim, num_new_vecs = new_eigenvectors.shape\n",
        "\n",
        "        if actual_feature_dim > self.buffer_size:\n",
        "            # print(f\"Warning: Eigenvector feature dimension ({actual_feature_dim}) for block {block_idx} \"\n",
        "            #       f\"exceeds buffer_size ({self.buffer_size}). Truncating features.\")\n",
        "            new_eigenvectors = new_eigenvectors[:self.buffer_size, :]\n",
        "            actual_feature_dim = self.buffer_size\n",
        "\n",
        "        slots_to_fill = min(num_new_vecs, self.buffer_size)\n",
        "\n",
        "        # Clear old values and set new ones\n",
        "        current_buffer_entry.eigen_vectors.fill_(0.)\n",
        "        current_buffer_entry.projection_mask.fill_(0.)\n",
        "\n",
        "        if slots_to_fill > 0:\n",
        "            current_buffer_entry.eigen_vectors[:actual_feature_dim, :slots_to_fill] = new_eigenvectors[:, :slots_to_fill]\n",
        "            current_buffer_entry.projection_mask[:slots_to_fill] = 1.0 # Mark as active\n",
        "\n",
        "    def get_subspace(self, block_idx: int) -> torch.Tensor | None:\n",
        "        \"\"\"\n",
        "        Retrieves the active eigenvectors for a given block.\n",
        "        Returns:\n",
        "            Tensor of shape (feature_dim_of_stored_vecs, num_active_vecs), or None.\n",
        "            Feature_dim_of_stored_vecs could be up to buffer_size.\n",
        "        \"\"\"\n",
        "        if not (0 <= block_idx < self.num_blocks):\n",
        "            raise IndexError(f\"block_idx {block_idx} out of range for {self.num_blocks} buffers.\")\n",
        "\n",
        "        current_buffer_entry = self.subspace_module_list[block_idx] # MODIFIED HERE\n",
        "        active_indices = current_buffer_entry.projection_mask.bool() # Convert 0/1 to boolean mask\n",
        "\n",
        "        if not active_indices.any():\n",
        "            return None # No active eigenvectors\n",
        "\n",
        "        # Retrieve only up to the point where features might have been stored.\n",
        "        # This assumes eigenvectors were stored contiguously from feature dim 0.\n",
        "        # A more robust way would be to also store the actual_feature_dim, but this matches the buffer_size logic.\n",
        "        active_vectors = current_buffer_entry.eigen_vectors[:, active_indices]\n",
        "        return active_vectors"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-28T12:32:53.355840Z",
          "iopub.execute_input": "2025-05-28T12:32:53.356095Z",
          "iopub.status.idle": "2025-05-28T12:32:53.375381Z",
          "shell.execute_reply.started": "2025-05-28T12:32:53.356072Z",
          "shell.execute_reply": "2025-05-28T12:32:53.374905Z"
        },
        "id": "XxkM3R2v5wIZ"
      },
      "outputs": [],
      "execution_count": 11
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stage 2: Training Pipeline"
      ],
      "metadata": {
        "id": "jIKsk6bB5wIa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-28T12:32:53.376040Z",
          "iopub.execute_input": "2025-05-28T12:32:53.376455Z",
          "iopub.status.idle": "2025-05-28T12:32:53.393331Z",
          "shell.execute_reply.started": "2025-05-28T12:32:53.376439Z",
          "shell.execute_reply": "2025-05-28T12:32:53.392799Z"
        },
        "id": "l3HA4bK15wIa"
      },
      "outputs": [],
      "execution_count": 12
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Natural Gradient Calculation\n"
      ],
      "metadata": {
        "id": "57Wtq7Mw5wIa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _compute_lora_curvature_matrices(lora_layer: FlexLoRA, damping: float = 1e-6) -> tuple[torch.Tensor | None, torch.Tensor | None]:\n",
        "    \"\"\"\n",
        "    Computes the smaller (rank x rank) curvature matrices for LoRA A and B parameters.\n",
        "    M_A = B @ B.T\n",
        "    M_B = A.T @ A\n",
        "    Returns (M_A + damping * I, M_B + damping * I), or (None, None) if issues arise.\n",
        "    \"\"\"\n",
        "    if not hasattr(lora_layer, 'lora_A') or not hasattr(lora_layer, 'lora_B') or lora_layer.lora_A is None or lora_layer.lora_B is None:\n",
        "        return None, None\n",
        "\n",
        "    A = lora_layer.lora_A\n",
        "    B = lora_layer.lora_B\n",
        "\n",
        "    # Removed gradient check as these matrices are based on parameter values, not grads for curvature.\n",
        "\n",
        "    if A.ndim < 2 or B.ndim < 2 or A.shape[1] == 0 or B.shape[0] == 0: # rank is A.shape[1] or B.shape[0]\n",
        "        return None, None\n",
        "\n",
        "    rank_A = A.shape[1]\n",
        "    rank_B = B.shape[0]\n",
        "\n",
        "    if rank_A != rank_B:\n",
        "        return None, None\n",
        "    if rank_A == 0: # current_rank can be 0\n",
        "        return None, None\n",
        "\n",
        "    try:\n",
        "        A_data = A #.detach()\n",
        "        B_data = B #.detach()\n",
        "\n",
        "        # M_A = B @ B.T (curvature related to A's parameters)\n",
        "        # Fisher for A parameters is kron(B @ B.T, I_in), we only need B @ B.T\n",
        "        M_A = B_data @ B_data.T  # Shape: (rank_A, rank_A)\n",
        "        M_A_damped = M_A + damping * torch.eye(M_A.shape[0], device=A_data.device, dtype=A_data.dtype)\n",
        "\n",
        "        # M_B = A.T @ A (curvature related to B's parameters)\n",
        "        # Fisher for B parameters is kron(I_out, A.T @ A), we only need A.T @ A\n",
        "        M_B = A_data.T @ A_data # Shape: (rank_A, rank_A)\n",
        "        M_B_damped = M_B + damping * torch.eye(M_B.shape[0], device=B_data.device, dtype=B_data.dtype)\n",
        "\n",
        "        return M_A_damped, M_B_damped\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in _compute_lora_curvature_matrices for {type(lora_layer).__name__}: {e}\")\n",
        "        return None, None"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-28T12:32:53.394088Z",
          "iopub.execute_input": "2025-05-28T12:32:53.394245Z",
          "iopub.status.idle": "2025-05-28T12:32:53.409721Z",
          "shell.execute_reply.started": "2025-05-28T12:32:53.394232Z",
          "shell.execute_reply": "2025-05-28T12:32:53.409052Z"
        },
        "id": "3Z0zqo6K5wIa"
      },
      "outputs": [],
      "execution_count": 13
    },
    {
      "cell_type": "code",
      "source": [
        "def natural_gradient_step(model: nn.Module, lr: float = 0.001, damping_lora_fisher: float = 1e-6):\n",
        "    \"\"\"\n",
        "    Performs a natural gradient update step for LoRA parameters using LoRA-specific curvature.\n",
        "    It computes LoRA-specific curvature matrices (M_A_lora, M_B_lora), inverts them,\n",
        "    and applies them to the gradients for the update.\n",
        "    If issues arise, it falls back to an SGD update.\n",
        "    \"\"\"\n",
        "    for module_name, module in model.named_modules():\n",
        "        if isinstance(module, FlexLoRA):\n",
        "            if module.current_rank == 0:\n",
        "                continue\n",
        "            if not hasattr(module, 'lora_A') or not hasattr(module, 'lora_B') or \\\n",
        "               module.lora_A.grad is None or module.lora_B.grad is None:\n",
        "                continue\n",
        "\n",
        "            grad_A = module.lora_A.grad\n",
        "            grad_B = module.lora_B.grad\n",
        "\n",
        "            # Compute LoRA-specific curvature matrices\n",
        "            M_A_lora_damped, M_B_lora_damped = _compute_lora_curvature_matrices(module, damping=damping_lora_fisher)\n",
        "\n",
        "            if M_A_lora_damped is None or M_B_lora_damped is None or \\\n",
        "               M_A_lora_damped.numel() == 0 or M_B_lora_damped.numel() == 0:\n",
        "                # Fallback to SGD if LoRA curvature matrices cannot be computed\n",
        "                print(f\"Warning: LoRA curvature matrix computation failed for {module_name}. Falling back to SGD.\")\n",
        "                with torch.no_grad():\n",
        "                    if module.lora_A.grad is not None:\n",
        "                        module.lora_A.data -= lr * module.lora_A.grad\n",
        "                    if module.lora_B.grad is not None:\n",
        "                        module.lora_B.data -= lr * module.lora_B.grad\n",
        "                # Gradients are zeroed at the end of the common path or here for SGD\n",
        "                if module.lora_A.grad is not None: module.lora_A.grad.zero_()\n",
        "                if module.lora_B.grad is not None: module.lora_B.grad.zero_()\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                M_A_lora_inv = torch.linalg.pinv(M_A_lora_damped)\n",
        "                M_B_lora_inv = torch.linalg.pinv(M_B_lora_damped)\n",
        "\n",
        "                # LoRA-only Natural Gradient update\n",
        "                ng_update_A = grad_A @ M_A_lora_inv\n",
        "                ng_update_B = M_B_lora_inv @ grad_B\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    module.lora_A.data -= lr * ng_update_A.to(module.lora_A.data.device)\n",
        "                    module.lora_B.data -= lr * ng_update_B.to(module.lora_B.data.device)\n",
        "\n",
        "            except RuntimeError as e:\n",
        "                print(f\"RuntimeError during natural gradient update for {module_name}: {e}. Falling back to SGD.\")\n",
        "                # Fallback to SGD\n",
        "                with torch.no_grad():\n",
        "                    if module.lora_A.grad is not None: module.lora_A.data -= lr * module.lora_A.grad\n",
        "                    if module.lora_B.grad is not None: module.lora_B.data -= lr * module.lora_B.grad\n",
        "            finally: # Ensure gradients are zeroed after update or in case of SGD fallback from try block\n",
        "                 if module.lora_A.grad is not None: module.lora_A.grad.zero_()\n",
        "                 if module.lora_B.grad is not None: module.lora_B.grad.zero_()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-28T12:32:53.410469Z",
          "iopub.execute_input": "2025-05-28T12:32:53.410905Z",
          "iopub.status.idle": "2025-05-28T12:32:53.430368Z",
          "shell.execute_reply.started": "2025-05-28T12:32:53.410860Z",
          "shell.execute_reply": "2025-05-28T12:32:53.429668Z"
        },
        "id": "ru2O60Uv5wIa"
      },
      "outputs": [],
      "execution_count": 14
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Neural Reprojection (adapted for FlexLoRA)\n"
      ],
      "metadata": {
        "id": "HrsuOxbV5wIa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def neural_reprojection(\n",
        "    lora_layer: FlexLoRA,\n",
        "    rho_eigen_sum: float = 0.9,\n",
        "    damping_lora_fisher: float = 1e-6\n",
        "    ):\n",
        "    \"\"\"\n",
        "    Performs neural reprojection on LoRA parameters.\n",
        "    Computes LoRA-specific curvature matrices, performs SVD on these,\n",
        "    identifies principal subspaces, and projects LoRA matrices A and B onto these subspaces.\n",
        "    \"\"\"\n",
        "    if not isinstance(lora_layer, FlexLoRA):\n",
        "        return\n",
        "    if lora_layer.current_rank == 0:\n",
        "        return\n",
        "\n",
        "    M_A_damped, M_B_damped = _compute_lora_curvature_matrices(lora_layer, damping=damping_lora_fisher)\n",
        "\n",
        "    if M_A_damped is None or M_B_damped is None or M_A_damped.numel() == 0 or M_B_damped.numel() == 0:\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        # SVD on the (rank x rank) curvature matrices\n",
        "        # U will contain eigenvectors for the *rank* dimension\n",
        "        U_A, S_A, Vh_A = torch.linalg.svd(M_A_damped)\n",
        "        U_B, S_B, Vh_B = torch.linalg.svd(M_B_damped)\n",
        "    except Exception as e:\n",
        "        return\n",
        "\n",
        "    if S_A.numel() == 0 and S_B.numel() == 0:\n",
        "        return\n",
        "\n",
        "    k_A = 0\n",
        "    if S_A.numel() > 0:\n",
        "        total_eigenvalue_sum_A = torch.sum(S_A)\n",
        "        if total_eigenvalue_sum_A > 1e-9:\n",
        "            cumulative_eigenvalue_sum_A = torch.cumsum(S_A, dim=0)\n",
        "            k_candidates_A = torch.where(cumulative_eigenvalue_sum_A >= rho_eigen_sum * total_eigenvalue_sum_A)[0]\n",
        "            if len(k_candidates_A) > 0: k_A = k_candidates_A[0].item() + 1\n",
        "            else: k_A = S_A.shape[0]\n",
        "            k_A = max(1, min(k_A, S_A.shape[0]))\n",
        "        else:\n",
        "            k_A = S_A.shape[0]\n",
        "\n",
        "    k_B = 0\n",
        "    if S_B.numel() > 0:\n",
        "        total_eigenvalue_sum_B = torch.sum(S_B)\n",
        "        if total_eigenvalue_sum_B > 1e-9:\n",
        "            cumulative_eigenvalue_sum_B = torch.cumsum(S_B, dim=0)\n",
        "            k_candidates_B = torch.where(cumulative_eigenvalue_sum_B >= rho_eigen_sum * total_eigenvalue_sum_B)[0]\n",
        "            if len(k_candidates_B) > 0: k_B = k_candidates_B[0].item() + 1\n",
        "            else: k_B = S_B.shape[0]\n",
        "            k_B = max(1, min(k_B, S_B.shape[0]))\n",
        "        else:\n",
        "            k_B = S_B.shape[0]\n",
        "\n",
        "    P_A = torch.eye(M_A_damped.shape[0], device=U_A.device, dtype=U_A.dtype)\n",
        "    if k_A > 0 and k_A < U_A.shape[0]:\n",
        "        U_Ak = U_A[:, :k_A]\n",
        "        P_A = U_Ak @ U_Ak.T\n",
        "\n",
        "    P_B = torch.eye(M_B_damped.shape[0], device=U_B.device, dtype=U_B.dtype)\n",
        "    if k_B > 0 and k_B < U_B.shape[0]:\n",
        "        U_Bk = U_B[:, :k_B]\n",
        "        P_B = U_Bk @ U_Bk.T\n",
        "\n",
        "    with torch.no_grad():\n",
        "        projected_lora_A = lora_layer.lora_A.data @ P_A\n",
        "        lora_layer.lora_A.data.copy_(projected_lora_A)\n",
        "\n",
        "        projected_lora_B = P_B @ lora_layer.lora_B.data\n",
        "        lora_layer.lora_B.data.copy_(projected_lora_B)\n",
        "    pass"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-28T12:32:53.431127Z",
          "iopub.execute_input": "2025-05-28T12:32:53.431307Z",
          "iopub.status.idle": "2025-05-28T12:32:53.449660Z",
          "shell.execute_reply.started": "2025-05-28T12:32:53.431293Z",
          "shell.execute_reply": "2025-05-28T12:32:53.449176Z"
        },
        "id": "mUg3x_Lj5wIb"
      },
      "outputs": [],
      "execution_count": 15
    },
    {
      "cell_type": "code",
      "source": [
        "class SVTracker(nn.Module):\n",
        "    \"\"\"\n",
        "    Tracks the history of singular values for a FlexLoRA layer's effective matrix (lora_A @ lora_B).\n",
        "    Assumes the associated FlexLoRA layer has `lora_A`, `lora_B`, and `max_rank` attributes.\n",
        "    \"\"\"\n",
        "    def __init__(self, flex_lora_layer, window_size: int = 100):\n",
        "        super().__init__()\n",
        "        if not hasattr(flex_lora_layer, 'lora_A') or not hasattr(flex_lora_layer, 'lora_B'):\n",
        "            raise ValueError(\"Provided layer must have 'lora_A' and 'lora_B' parameters.\")\n",
        "        if not hasattr(flex_lora_layer, 'max_rank'):\n",
        "            raise ValueError(\"Provided FlexLoRA layer must have a 'max_rank' attribute.\")\n",
        "\n",
        "        self.flex_lora_layer = flex_lora_layer\n",
        "        self.window_size = window_size\n",
        "\n",
        "        self.register_buffer('sv_history',\n",
        "                             torch.zeros(window_size, flex_lora_layer.max_rank,\n",
        "                                         device=flex_lora_layer.lora_A.device))\n",
        "\n",
        "    def forward(self):\n",
        "        \"\"\"Calculates SVD of lora_A @ lora_B and updates history.\"\"\"\n",
        "        with torch.no_grad():\n",
        "            lora_A_data = self.flex_lora_layer.lora_A.data\n",
        "            lora_B_data = self.flex_lora_layer.lora_B.data\n",
        "            effective_matrix = lora_A_data @ lora_B_data\n",
        "\n",
        "            try:\n",
        "                # SVD: U, S, Vh = svd(A). S are singular values.\n",
        "                S = torch.linalg.svdvals(effective_matrix)\n",
        "            except Exception as e:\n",
        "                # print(f\"SVD failed in SVTracker: {e}. Skipping history update.\")\n",
        "                return\n",
        "\n",
        "            # Pad or truncate S to match self.flex_lora_layer.max_rank for consistent history storage\n",
        "            num_singular_values = S.shape[0]\n",
        "            max_r = self.flex_lora_layer.max_rank\n",
        "\n",
        "            s_padded = torch.zeros(max_r, device=S.device)\n",
        "            if num_singular_values > 0:\n",
        "                s_padded[:min(num_singular_values, max_r)] = S[:min(num_singular_values, max_r)]\n",
        "\n",
        "            self.sv_history = torch.roll(self.sv_history, shifts=1, dims=0)\n",
        "            self.sv_history[0, :] = s_padded.detach()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-28T12:32:53.450270Z",
          "iopub.execute_input": "2025-05-28T12:32:53.450501Z",
          "iopub.status.idle": "2025-05-28T12:32:53.468742Z",
          "shell.execute_reply.started": "2025-05-28T12:32:53.450482Z",
          "shell.execute_reply": "2025-05-28T12:32:53.468103Z"
        },
        "id": "HU1DwQRZ5wIb"
      },
      "outputs": [],
      "execution_count": 16
    },
    {
      "cell_type": "code",
      "source": [
        "class RankScheduler:\n",
        "    def __init__(self, initial_rank: int, min_rank: int, max_rank: int, # Added min/max rank from FlexLoRA\n",
        "                 warmup_steps: int = 1000, ema_alpha: float = 0.1): # Added ema_alpha for sv_ema\n",
        "        self.current_rank = initial_rank\n",
        "        self.min_rank = min_rank # Store min_rank\n",
        "        self.max_rank = max_rank # Store max_rank\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.step_counter = 0\n",
        "        self.ema_alpha = ema_alpha # For smoothing sv_ratio\n",
        "        self.sv_ema = None # EMA of singular value ratio\n",
        "\n",
        "    def update(self, sv_ratio: float): # sv_ratio is sigma_1/sigma_r from rank_adjustment's perspective\n",
        "        self.step_counter += 1\n",
        "\n",
        "        if self.sv_ema is None:\n",
        "            self.sv_ema = sv_ratio\n",
        "        else:\n",
        "            self.sv_ema = self.ema_alpha * sv_ratio + (1 - self.ema_alpha) * self.sv_ema\n",
        "\n",
        "        if self.step_counter < self.warmup_steps:\n",
        "            return self.current_rank # Keep current rank during warmup\n",
        "\n",
        "        # Logic:\n",
        "        # if self.step_counter > 0.8: # Typo: self.step_counter > 0.8 ? Should be sv_ema\n",
        "        # Assuming the condition is based on self.sv_ema\n",
        "        if self.sv_ema is not None: # Check if sv_ema is initialized\n",
        "            if self.sv_ema < 0.5: # If ratio is small, indicates potential for rank reduction\n",
        "                # Decrease rank, but not below min_rank\n",
        "                self.current_rank = max(self.min_rank, self.current_rank - 2)\n",
        "            elif self.sv_ema > 0.8: # If ratio is large, potential for rank increase\n",
        "                # Increase rank, but not above max_rank\n",
        "                self.current_rank = min(self.max_rank, self.current_rank + 2)\n",
        "\n",
        "        return self.current_rank"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-28T12:32:53.469511Z",
          "iopub.execute_input": "2025-05-28T12:32:53.469829Z",
          "iopub.status.idle": "2025-05-28T12:32:53.487346Z",
          "shell.execute_reply.started": "2025-05-28T12:32:53.469787Z",
          "shell.execute_reply": "2025-05-28T12:32:53.486682Z"
        },
        "id": "WQkOAgwV5wIb"
      },
      "outputs": [],
      "execution_count": 17
    },
    {
      "cell_type": "code",
      "source": [
        "def rank_adjustment(\n",
        "    layer: FlexLoRA,\n",
        "    sv_tracker: SVTracker,\n",
        "    rank_scheduler: RankScheduler, # Pass the scheduler instance for this layer\n",
        "    # rank_eta, rank_tau are removed as scheduler now handles logic\n",
        "    sv_history_min_samples: int = 5\n",
        "    ):\n",
        "    \"\"\"\n",
        "    Adjusts the rank of a FlexLoRA layer using an external RankScheduler\n",
        "    based on its singular value history (sigma_1/sigma_r ratio from mean_sv).\n",
        "    \"\"\"\n",
        "    if not all(hasattr(layer, attr) for attr in\n",
        "               ['current_rank', 'min_rank', 'max_rank', 'lora_A', 'lora_B']):\n",
        "        raise ValueError(\"FlexLoRA layer is missing required attributes for rank adjustment.\")\n",
        "\n",
        "    if sv_tracker.sv_history[:, 0].count_nonzero() < sv_history_min_samples :\n",
        "        # print(f\"Warning: Not enough singular value history for layer. Skipping rank adjustment. Need {sv_history_min_samples} samples.\")\n",
        "        return\n",
        "\n",
        "    with torch.no_grad():\n",
        "        mean_sv = sv_tracker.sv_history.mean(dim=0)\n",
        "\n",
        "        current_rank_val = layer.current_rank # Use a different name to avoid conflict with scheduler's current_rank\n",
        "        if current_rank_val == 0 or current_rank_val > layer.max_rank: # current_rank_val == 0 means LoRA is inactive or uninitialized for adjustment\n",
        "            return\n",
        "\n",
        "        if mean_sv.numel() == 0 or current_rank_val > mean_sv.shape[0]:\n",
        "            # print(\"Warning: Mean singular values not available or current rank exceeds available SVs. Skipping rank adjustment.\")\n",
        "            return\n",
        "\n",
        "        sigma_1 = mean_sv[0].item()\n",
        "        sigma_r = mean_sv[current_rank_val - 1].item() if current_rank_val > 0 else 0.0\n",
        "\n",
        "        if sigma_1 < 1e-9 or sigma_r < 1e-9: # Avoid division by zero or unstable ratios\n",
        "            # print(f\"Warning: sigma_1 ({sigma_1}) or sigma_r ({sigma_r}) is near zero. Skipping rank update based on sv_ratio.\")\n",
        "            return\n",
        "\n",
        "        sv_ratio_metric = sigma_1 / sigma_r\n",
        "\n",
        "        # Get the new rank from the scheduler\n",
        "        new_rank_from_scheduler = rank_scheduler.update(sv_ratio_metric)\n",
        "        # Ensure new_rank is within the layer's capabilities and min/max bounds\n",
        "        new_rank = int(max(layer.min_rank, min(new_rank_from_scheduler, layer.max_rank)))\n",
        "\n",
        "\n",
        "        if new_rank == current_rank_val:\n",
        "            return\n",
        "\n",
        "        old_lora_A = layer.lora_A.data\n",
        "        old_lora_B = layer.lora_B.data\n",
        "        device = old_lora_A.device\n",
        "\n",
        "        # Preserve weights using SVD of W_LoRA = old_lora_A @ old_lora_B\n",
        "        # This aligns with \"recomputes W_LoRA = A B, takes its SVD, and reconstructs new A, B\"\n",
        "        W_LoRA = old_lora_A @ old_lora_B\n",
        "        try:\n",
        "            U, S, Vh = torch.linalg.svd(W_LoRA)\n",
        "        except Exception as e:\n",
        "            print(f\"SVD failed during rank adjustment for layer {type(layer).__name__}: {e}. Rank not changed.\")\n",
        "            return\n",
        "\n",
        "        # Determine the effective rank for SVD reconstruction.\n",
        "        # This rank cannot exceed the number of singular values, S.shape[0] (min of W_LoRA dimensions).\n",
        "        # layer.max_rank should ideally be configured to be <= S.shape[0].\n",
        "        effective_rank_for_svd = new_rank\n",
        "        if new_rank > S.shape[0]:\n",
        "            print(f\"Warning: Target new_rank {new_rank} exceeds available singular values {S.shape[0]} for layer {type(layer).__name__}. \"\n",
        "                  f\"Capping rank to {S.shape[0]}. Consider adjusting layer.max_rank.\")\n",
        "            effective_rank_for_svd = S.shape[0]\n",
        "            new_rank = effective_rank_for_svd # Update new_rank to the capped value for consistency\n",
        "\n",
        "        if new_rank == current_rank_val and effective_rank_for_svd == new_rank : # Check again if capping resulted in no change\n",
        "             return\n",
        "\n",
        "\n",
        "        # Reconstruct LoRA matrices A' and B' such that A'B' approximates W_LoRA.\n",
        "        # A_new = U[:, :k] @ diag(sqrt(S[:k]))\n",
        "        # B_new = diag(sqrt(S[:k])) @ Vh[:k, :]\n",
        "        # This distributes singular values (sqrt) to both A and B.\n",
        "\n",
        "        # Handle case where effective_rank_for_svd is 0 (e.g. if min_rank is 0 and it's chosen)\n",
        "        if effective_rank_for_svd == 0:\n",
        "            new_lora_A_data = torch.empty((old_lora_A.shape[0], 0), device=device, dtype=old_lora_A.dtype)\n",
        "            new_lora_B_data = torch.empty((0, old_lora_B.shape[1]), device=device, dtype=old_lora_B.dtype)\n",
        "        else:\n",
        "            U_k = U[:, :effective_rank_for_svd]\n",
        "            S_k = S[:effective_rank_for_svd]\n",
        "            Vh_k = Vh[:effective_rank_for_svd, :]\n",
        "\n",
        "            # Singular values (S_k) are non-negative. torch.sqrt(0) is 0.\n",
        "            sqrt_S_k = torch.sqrt(S_k)\n",
        "            diag_sqrt_S_k = torch.diag(sqrt_S_k)\n",
        "\n",
        "            new_lora_A_data = U_k @ diag_sqrt_S_k\n",
        "            new_lora_B_data = diag_sqrt_S_k @ Vh_k\n",
        "\n",
        "        layer.lora_A = nn.Parameter(new_lora_A_data.to(device))\n",
        "        layer.lora_B = nn.Parameter(new_lora_B_data.to(device))\n",
        "        layer.current_rank = new_rank # Update layer's current rank\n",
        "        # rank_scheduler.current_rank is updated internally by rank_scheduler.update()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-28T12:32:53.488025Z",
          "iopub.execute_input": "2025-05-28T12:32:53.488277Z",
          "iopub.status.idle": "2025-05-28T12:32:53.504169Z",
          "shell.execute_reply.started": "2025-05-28T12:32:53.488253Z",
          "shell.execute_reply": "2025-05-28T12:32:53.503558Z"
        },
        "id": "Q20SWQgK5wIb"
      },
      "outputs": [],
      "execution_count": 18
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Parameter Fusion"
      ],
      "metadata": {
        "id": "QgrqPoaY5wIc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5.1 Momentum Smoothing (Using existing robust version from Training.py)\n",
        "class MomentumFusion:\n",
        "    def __init__(self, beta: float = 0.9):\n",
        "        self.beta = beta\n",
        "        self.momentum_buffer = {} # Using dict for FlexLoRA parameters by name\n",
        "\n",
        "    def __call__(self, model: nn.Module):\n",
        "        with torch.no_grad():\n",
        "            for name, module in model.named_modules():\n",
        "                if isinstance(module, FlexLoRA):\n",
        "                    if module.current_rank == 0: continue # Skip if no LoRA adaptation\n",
        "\n",
        "                    params_to_fuse = {}\n",
        "                    if hasattr(module, 'lora_A') and module.lora_A is not None:\n",
        "                        params_to_fuse[\"lora_A\"] = module.lora_A\n",
        "                    if hasattr(module, 'lora_B') and module.lora_B is not None:\n",
        "                        params_to_fuse[\"lora_B\"] = module.lora_B\n",
        "\n",
        "                    for p_name, param in params_to_fuse.items():\n",
        "                        if param.requires_grad: # Only fuse trainable LoRA params\n",
        "                            full_param_name = f\"{name}.{p_name}\"\n",
        "                            if full_param_name not in self.momentum_buffer:\n",
        "                                self.momentum_buffer[full_param_name] = torch.zeros_like(param.data)\n",
        "\n",
        "                            buf = self.momentum_buffer[full_param_name]\n",
        "                            buf.mul_(self.beta).add_(param.data, alpha=1 - self.beta)\n",
        "                            param.data.copy_(buf)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-28T12:32:53.504960Z",
          "iopub.execute_input": "2025-05-28T12:32:53.505253Z",
          "iopub.status.idle": "2025-05-28T12:32:53.522552Z",
          "shell.execute_reply.started": "2025-05-28T12:32:53.505233Z",
          "shell.execute_reply": "2025-05-28T12:32:53.522065Z"
        },
        "id": "Kf-SzQ1w5wIc"
      },
      "outputs": [],
      "execution_count": 19
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "# Assume FlexLoRA is available from Grit.Infrastructure\n",
        "# from Grit.Infrastructure import FlexLoRA"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-28T12:32:53.523359Z",
          "iopub.execute_input": "2025-05-28T12:32:53.523595Z",
          "iopub.status.idle": "2025-05-28T12:32:53.541195Z",
          "shell.execute_reply.started": "2025-05-28T12:32:53.523577Z",
          "shell.execute_reply": "2025-05-28T12:32:53.540608Z"
        },
        "id": "ZKQ-VO0F5wIc"
      },
      "outputs": [],
      "execution_count": 20
    },
    {
      "cell_type": "code",
      "source": [
        "def fisher_clip(\n",
        "    gradient: torch.Tensor,\n",
        "    inv_fisher_factor_A: torch.Tensor, # Inverse of activation covariance (A_inv)\n",
        "    inv_fisher_factor_B: torch.Tensor, # Inverse of output gradient covariance (B_inv)\n",
        "    clip_norm: float = 1.0\n",
        "    ) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Clips a gradient in the Fisher-approximated natural gradient space.\n",
        "    Conceptually, natural_grad = F_inv @ grad.flatten(), where F_inv = kron(inv_B, inv_A).\n",
        "    This function computes it efficiently as vec(inv_B @ grad @ inv_A.T).\n",
        "    \"\"\"\n",
        "    if gradient is None:\n",
        "        return None\n",
        "\n",
        "    with torch.no_grad():\n",
        "        natural_grad_matrix = inv_fisher_factor_B @ gradient @ inv_fisher_factor_A\n",
        "        natural_grad_flat = natural_grad_matrix.flatten()\n",
        "\n",
        "        grad_norm_fisher_space = torch.norm(natural_grad_flat)\n",
        "\n",
        "        if grad_norm_fisher_space > clip_norm:\n",
        "            scale = clip_norm / (grad_norm_fisher_space + 1e-7) # Add epsilon for stability\n",
        "            return gradient * scale\n",
        "        return gradient"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-28T12:32:53.541961Z",
          "iopub.execute_input": "2025-05-28T12:32:53.542217Z",
          "iopub.status.idle": "2025-05-28T12:32:53.555290Z",
          "shell.execute_reply.started": "2025-05-28T12:32:53.542196Z",
          "shell.execute_reply": "2025-05-28T12:32:53.554723Z"
        },
        "id": "Xoo7K7z55wIc"
      },
      "outputs": [],
      "execution_count": 21
    },
    {
      "cell_type": "code",
      "source": [
        "# NaturalGradientClipper\n",
        "class NaturalGradientClipper:\n",
        "    def __init__(self, max_norm: float = 1.0):\n",
        "        self.max_norm = max_norm\n",
        "\n",
        "    def __call__(self, natural_grad: torch.Tensor) -> torch.Tensor:\n",
        "        norm = torch.norm(natural_grad)\n",
        "        if norm > self.max_norm:\n",
        "            return natural_grad * (self.max_norm / (norm + 1e-7)) # Added epsilon for stability\n",
        "        return natural_grad"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-28T12:32:53.555882Z",
          "iopub.execute_input": "2025-05-28T12:32:53.556111Z",
          "iopub.status.idle": "2025-05-28T12:32:53.571848Z",
          "shell.execute_reply.started": "2025-05-28T12:32:53.556091Z",
          "shell.execute_reply": "2025-05-28T12:32:53.571312Z"
        },
        "id": "1K95NL_Y5wIc"
      },
      "outputs": [],
      "execution_count": 22
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Memory Optimization Framework\n"
      ],
      "metadata": {
        "id": "O_5g0uh75wId"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def block_svd(matrix: torch.Tensor, block_size: int = 256) -> tuple[list, list, list]:\n",
        "    \"\"\"\n",
        "    Performs SVD on blocks of a matrix. Assumes square blocks.\n",
        "    Returns lists of U_blocks, S_value_blocks, Vh_blocks (transposed V).\n",
        "    \"\"\"\n",
        "    U_blocks, S_blocks, Vh_blocks = [], [], []\n",
        "    block_size_rows, block_size_cols = block_size, block_size # Assuming square blocks from signature\n",
        "\n",
        "    for i in range(0, matrix.size(0), block_size_rows):\n",
        "        for j in range(0, matrix.size(1), block_size_cols):\n",
        "            block = matrix[i:i+block_size_rows, j:j+block_size_cols].clone()\n",
        "            if block.numel() == 0: continue\n",
        "            try:\n",
        "                U, S, Vh = torch.linalg.svd(block) # linalg.svd returns Vh (V.T)\n",
        "                U_blocks.append(U)\n",
        "                S_blocks.append(S)\n",
        "                Vh_blocks.append(Vh) # Storing Vh as is consistent with linalg.svd\n",
        "            except Exception as e:\n",
        "                # print(f\"SVD failed for block ({i},{j}) of size {block.shape}: {e}\")\n",
        "                U_blocks.append(torch.empty(0,0, device=matrix.device))\n",
        "                S_blocks.append(torch.empty(0, device=matrix.device))\n",
        "                Vh_blocks.append(torch.empty(0,0, device=matrix.device))\n",
        "    return U_blocks, S_blocks, Vh_blocks\n",
        "\n",
        "class HalfPrecisionFisher(nn.Module):\n",
        "    \"\"\"\n",
        "    Manages KFAC A and B factors stored in half-precision (float16).\n",
        "    EMA updates are performed with care for precision.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_features: int, out_features: int, ema_decay: float = 0.95):\n",
        "        super().__init__()\n",
        "        self.ema_decay = ema_decay # 0.95 decay\n",
        "\n",
        "        self.register_buffer('A_half', torch.zeros(in_features, in_features, dtype=torch.float16))\n",
        "        self.register_buffer('B_half', torch.zeros(out_features, out_features, dtype=torch.float16))\n",
        "\n",
        "    def update(self, current_A_cov: torch.Tensor, current_B_cov: torch.Tensor):\n",
        "        \"\"\"Update with raw (float32) covariances from current batch.\"\"\"\n",
        "        with torch.no_grad():\n",
        "            new_A_f32 = self.A_half.float() * self.ema_decay + \\\n",
        "                        current_A_cov.half().float() * (1 - self.ema_decay)\n",
        "            self.A_half = new_A_f32.half()\n",
        "\n",
        "            new_B_f32 = self.B_half.float() * self.ema_decay + \\\n",
        "                        current_B_cov.half().float() * (1 - self.ema_decay)\n",
        "            self.B_half = new_B_f32.half()\n",
        "\n",
        "    def get_factors_f32(self) -> tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"Returns factors cast to float32 for use.\"\"\"\n",
        "        return self.A_half.float(), self.B_half.float()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-28T12:32:53.572542Z",
          "iopub.execute_input": "2025-05-28T12:32:53.572694Z",
          "iopub.status.idle": "2025-05-28T12:32:53.587764Z",
          "shell.execute_reply.started": "2025-05-28T12:32:53.572682Z",
          "shell.execute_reply": "2025-05-28T12:32:53.587211Z"
        },
        "id": "UCl7rSeA5wId"
      },
      "outputs": [],
      "execution_count": 23
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GRIT Adapter"
      ],
      "metadata": {
        "id": "ZG_DnXM65wIe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GRITAdapter(nn.Module):\n",
        "    def __init__(self, model: nn.Module,\n",
        "                 layers_to_adapt_substrings: list[str],\n",
        "                 rank_range: tuple[int, int] = (8, 64),\n",
        "                 kfac_ema_decay: float = 0.95,\n",
        "                 kfac_damping: tuple[float, float] = (1e-7, 1e-5),\n",
        "                 fisher_k: float = 0.1, # Fraction for eigen decomposition\n",
        "                 subspace_buffer_size: int = 1024,\n",
        "                 # Rank scheduling params\n",
        "                 rank_scheduler_warmup_steps: int = 1000,\n",
        "                 rank_scheduler_ema_alpha: float = 0.1,\n",
        "                 sv_tracker_window_size: int = 100,\n",
        "                 sv_history_min_samples: int = 5,\n",
        "                 # Neural reprojection params\n",
        "                 rho_eigen_sum: float = 0.9,\n",
        "                 # Parameter fusion params\n",
        "                 momentum_fusion_beta: float = 0.9,\n",
        "                 # Update params\n",
        "                 learning_rate: float = 0.001,\n",
        "                 damping_lora_fisher: float = 1e-6\n",
        "                 ):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.layers_to_adapt_substrings = layers_to_adapt_substrings\n",
        "        self.rank_range = rank_range\n",
        "        self.kfac_ema_decay = kfac_ema_decay\n",
        "        self.kfac_damping = kfac_damping\n",
        "        self.fisher_k = fisher_k\n",
        "        self.subspace_buffer_size = subspace_buffer_size\n",
        "        self.rank_scheduler_warmup_steps = rank_scheduler_warmup_steps\n",
        "        self.rank_scheduler_ema_alpha = rank_scheduler_ema_alpha\n",
        "        self.sv_tracker_window_size = sv_tracker_window_size\n",
        "        self.sv_history_min_samples = sv_history_min_samples\n",
        "        self.rho_eigen_sum = rho_eigen_sum\n",
        "        self.momentum_fusion_beta = momentum_fusion_beta\n",
        "        self.learning_rate = learning_rate\n",
        "        self.damping_lora_fisher = damping_lora_fisher\n",
        "\n",
        "        self.current_model_logits_for_kfac = None # ADDED for True Fisher\n",
        "        self.current_sampled_labels_for_kfac = None # ADDED for True Fisher\n",
        "\n",
        "        self.all_kfac_handlers = []\n",
        "        self.fisher_engines = {} # Store FisherEigen instances per layer/block\n",
        "        self.subspace_manager = None # Will be initialized after KFAC instrumentation\n",
        "        self.sv_trackers = {} # For FlexLoRA layers\n",
        "        self.rank_schedulers = {} # For FlexLoRA layers\n",
        "        self.momentum_fuser = MomentumFusion(beta=self.momentum_fusion_beta)\n",
        "\n",
        "        self._initialize_grit()\n",
        "\n",
        "    def _initialize_grit(self):\n",
        "        # 1. Inject LoRA layers\n",
        "        lora_inject(self.model, self.layers_to_adapt_substrings, self.rank_range)\n",
        "\n",
        "        # 2. Instrument KFAC\n",
        "        # KFAC instrumentation might depend on the structure of the base model.\n",
        "        # This part might need to be more generic or configurable.\n",
        "\n",
        "        model_blocks_attr_names = ['trf_blocks', 'blocks'] # Common names for transformer blocks\n",
        "        actual_blocks_attr_name = None\n",
        "        for attr_name in model_blocks_attr_names:\n",
        "            if hasattr(self.model, attr_name) and isinstance(getattr(self.model, attr_name), (nn.ModuleList, nn.Sequential)):\n",
        "                actual_blocks_attr_name = attr_name\n",
        "                break\n",
        "\n",
        "        if actual_blocks_attr_name:\n",
        "            model_blocks = getattr(self.model, actual_blocks_attr_name)\n",
        "            for i, block in enumerate(model_blocks):\n",
        "                # Ensure instrument_layer can find layers like 'attn.q_proj' or 'mlp' within the block\n",
        "                # This might require block to have conventional sub-module names or instrument_layer to be more flexible\n",
        "                block_kfac_handlers = instrument_layer(block, grit_adapter_ref=self)\n",
        "                self.all_kfac_handlers.extend(block_kfac_handlers)\n",
        "        else:\n",
        "            print(f\"Warning: GRITAdapter KFAC instrumentation could not find a suitable blocks attribute (tried {model_blocks_attr_names}). KFAC might not be fully set up.\")\n",
        "\n",
        "        # Create a map from KFAC handler layer_type to its index in all_kfac_handlers\n",
        "        self.kfac_handler_to_idx_map = {\n",
        "            handler.layer_type: i for i, handler in enumerate(self.all_kfac_handlers)\n",
        "        }\n",
        "\n",
        "        # Initialize FisherEigen and SubspaceBuffer based on KFAC handlers\n",
        "        # This assumes KFAC handlers are for nn.Linear layers that were also LoRA adapted.\n",
        "        # The number of \"blocks\" for SubspaceBuffer should match num KFAC-instrumented entities.\n",
        "        num_kfac_instrumented_entities = len(self.all_kfac_handlers)\n",
        "        if num_kfac_instrumented_entities > 0:\n",
        "            # device = next(self.model.parameters()).device # Get model device\n",
        "            # Assuming all KFAC handlers and the model are on the same device\n",
        "            # Get device from the first KFAC handler's A_factor if available, else model param\n",
        "            if self.all_kfac_handlers and self.all_kfac_handlers[0].A_factor is not None:\n",
        "                 device = self.all_kfac_handlers[0].A_factor.device\n",
        "            else:\n",
        "                 try:\n",
        "                     device = next(self.model.parameters()).device\n",
        "                 except StopIteration:\n",
        "                     device = torch.device(\"cpu\") # Default if model has no parameters\n",
        "\n",
        "            self.subspace_manager = SubspaceBuffer(num_blocks=num_kfac_instrumented_entities,\n",
        "                                                   buffer_size=self.subspace_buffer_size,\n",
        "                                                   device=device) # Pass device\n",
        "            for i, handler in enumerate(self.all_kfac_handlers):\n",
        "                # Pass layer_type to FisherEigen for better logging\n",
        "                self.fisher_engines[handler.layer_type] = FisherEigen(layer_type=handler.layer_type).to(device)\n",
        "        else:\n",
        "            print(\"Warning: No KFAC handlers found. FisherEigen and SubspaceBuffer not initialized.\")\n",
        "\n",
        "\n",
        "        # Initialize SVTrackers and RankSchedulers for FlexLoRA layers\n",
        "        min_r, max_r = self.rank_range\n",
        "        initial_r = min_r # As per lora_inject\n",
        "        for name, module in self.model.named_modules():\n",
        "            if isinstance(module, FlexLoRA):\n",
        "                self.sv_trackers[name] = SVTracker(module, window_size=self.sv_tracker_window_size)\n",
        "                self.rank_schedulers[name] = RankScheduler(\n",
        "                    initial_rank=module.initial_rank, # Use FlexLoRA's actual initial_rank\n",
        "                    min_rank=module.min_rank,\n",
        "                    max_rank=module.max_rank,\n",
        "                    warmup_steps=self.rank_scheduler_warmup_steps,\n",
        "                    ema_alpha=self.rank_scheduler_ema_alpha\n",
        "                )\n",
        "\n",
        "    def _update_kfac_factors_and_fisher(self):\n",
        "        \"\"\"\n",
        "        Updates KFAC factors (implicitly via hooks during forward/backward),\n",
        "        then updates Fisher blocks and decomposes them for FIM estimation.\n",
        "        The resulting FIM eigenspaces are used for neural reprojection.\n",
        "        This method assumes a forward and backward pass has just occurred.\n",
        "        \"\"\"\n",
        "        if not self.all_kfac_handlers or self.subspace_manager is None:\n",
        "            # print(\"KFAC or SubspaceManager not initialized. Skipping Fisher updates.\")\n",
        "            return\n",
        "\n",
        "        for i, handler in enumerate(self.all_kfac_handlers):\n",
        "            kfac_A = handler.A_factor\n",
        "            kfac_B = handler.B_factor\n",
        "            fisher_engine = self.fisher_engines.get(handler.layer_type)\n",
        "\n",
        "            if fisher_engine is None:\n",
        "                # print(f\"No FisherEngine for {handler.layer_type}. Skipping.\")\n",
        "                continue\n",
        "\n",
        "            if kfac_A is None or kfac_B is None or kfac_A.numel() == 0 or kfac_B.numel() == 0:\n",
        "                # print(f\"Warning: KFAC factors for {handler.layer_type} are None or empty. Skipping Fisher update for this handler.\")\n",
        "                continue\n",
        "\n",
        "            fisher_engine.update_fisher(A=kfac_A, B=kfac_B)\n",
        "            if fisher_engine.A_s is not None and fisher_engine.B_s is not None and fisher_engine.A_s.numel() > 0 and fisher_engine.B_s.numel() > 0:\n",
        "                # Calculate the absolute number of eigenvectors to find\n",
        "                num_vectors_for_LOBPCG = max(1, int(self.subspace_buffer_size * self.fisher_k))\n",
        "                eig_vals, eig_vecs = fisher_engine.decompose(num_requested_vectors=num_vectors_for_LOBPCG)\n",
        "                if eig_vecs is not None:\n",
        "                    self.subspace_manager.update_buffer(block_idx=i, new_eigenvectors=eig_vecs)\n",
        "                # else:\n",
        "                #     print(f\"Fisher decomposition failed for {handler.layer_type}. Subspace buffer not updated.\")\n",
        "            # else:\n",
        "            #     print(f\"Fisher block for {handler.layer_type} is empty. Subspace buffer not updated.\")\n",
        "\n",
        "\n",
        "    def _apply_neural_reprojection(self):\n",
        "        \"\"\"\n",
        "        Applies neural reprojection to LoRA parameters using FIM eigenvectors\n",
        "        obtained from K-FAC. The LoRA parameters A and B are modified such that\n",
        "        their product A@B is projected onto the dominant FIM eigenspace.\n",
        "        This method assumes LoRA parameters have already been updated by natural gradient.\n",
        "        \"\"\"\n",
        "        if not self.subspace_manager or not hasattr(self.model, 'named_modules'):\n",
        "            # print(\"SubspaceManager not initialized or model has no named_modules. Skipping neural reprojection.\")\n",
        "            return\n",
        "\n",
        "        with torch.no_grad(): # All operations here are on .data or analytical\n",
        "            for name, lora_layer_candidate in self.model.named_modules():\n",
        "                if isinstance(lora_layer_candidate, FlexLoRA):\n",
        "                    flex_lora_module = lora_layer_candidate\n",
        "\n",
        "                    base_linear_layer = flex_lora_module.base_layer\n",
        "                    if not hasattr(base_linear_layer, '_kfac_instrumented_by_grit_') or \\\n",
        "                       not hasattr(base_linear_layer, 'kfac_handler'):\n",
        "                        continue # This LoRA layer's base is not KFAC instrumented\n",
        "\n",
        "                    kfac_handler_on_base = base_linear_layer.kfac_handler\n",
        "                    handler_layer_type = kfac_handler_on_base.layer_type\n",
        "\n",
        "                    block_idx = self.kfac_handler_to_idx_map.get(handler_layer_type)\n",
        "                    if block_idx is None:\n",
        "                        # print(f\"Warning: Could not find block_idx for KFAC handler {handler_layer_type}. Skipping reprojection for {name}.\")\n",
        "                        continue\n",
        "\n",
        "                    # Retrieve the precomputed FIM eigenbasis U_k from the SubspaceBuffer\n",
        "                    U_k = self.subspace_manager.get_subspace(block_idx)\n",
        "\n",
        "                    if U_k is None or U_k.numel() == 0:\n",
        "                        # print(f\"Warning: No FIM eigenvectors found in SubspaceBuffer for {handler_layer_type} (block {block_idx}). Skipping reprojection for {name}.\")\n",
        "                        continue\n",
        "\n",
        "                    current_rank = flex_lora_module.current_rank\n",
        "                    if current_rank == 0:\n",
        "                        continue\n",
        "\n",
        "                    lora_A_data = flex_lora_module.lora_A.data\n",
        "                    lora_B_data = flex_lora_module.lora_B.data\n",
        "\n",
        "                    if lora_A_data.numel() == 0 or lora_B_data.numel() == 0:\n",
        "                        continue # Nothing to project if A or B is empty\n",
        "\n",
        "                    # Compute the current LoRA weight effect: W_LoRA = A @ B\n",
        "                    W_LoRA = lora_A_data @ lora_B_data\n",
        "                    if W_LoRA.numel() == 0:\n",
        "                        continue\n",
        "\n",
        "                    original_shape = W_LoRA.shape\n",
        "                    vec_W_LoRA = W_LoRA.flatten()\n",
        "\n",
        "                    # Ensure U_k has features matching vec_W_LoRA's length (dimension match)\n",
        "                    if U_k.shape[0] != vec_W_LoRA.shape[0]:\n",
        "                        # This indicates a mismatch between KFAC FIM dimension (for base layer)\n",
        "                        # and LoRA effective matrix dimension. This should ideally not happen if KFAC\n",
        "                        # is on the base layer that LoRA adapts.\n",
        "                        # print(f\"Critical Dimension Mismatch for {name}: FIM eigenvectors {U_k.shape} for dim {U_k.shape[0]}, W_LoRA {original_shape} for dim {vec_W_LoRA.shape[0]}. Skipping layer.\")\n",
        "                        continue\n",
        "\n",
        "                    # Project vec(W_LoRA) onto the FIM eigen-subspace: vec_tilde_W = U_k @ (U_k.T @ vec(W_LoRA))\n",
        "                    vec_tilde_W = U_k @ (U_k.T @ vec_W_LoRA)\n",
        "                    tilde_W = vec_tilde_W.reshape(original_shape)\n",
        "\n",
        "                    try:\n",
        "                        # Factorize the projected result tilde_W back into new LoRA matrices A and B via SVD\n",
        "                        U_svd, S_svd, Vh_svd = torch.linalg.svd(tilde_W)\n",
        "\n",
        "                        # Ensure current_rank does not exceed available singular values from SVD\n",
        "                        effective_rank_for_reconstruction = min(current_rank, S_svd.shape[0])\n",
        "                        if effective_rank_for_reconstruction == 0 : # if target rank is 0 or no singular values\n",
        "                             A_reproj = torch.empty((lora_A_data.shape[0], 0), device=lora_A_data.device, dtype=lora_A_data.dtype)\n",
        "                             B_reproj = torch.empty((0, lora_B_data.shape[1]), device=lora_B_data.device, dtype=lora_B_data.dtype)\n",
        "                        else:\n",
        "                            U_svd_k = U_svd[:, :effective_rank_for_reconstruction]\n",
        "                            S_svd_k = S_svd[:effective_rank_for_reconstruction]\n",
        "                            Vh_svd_k = Vh_svd[:effective_rank_for_reconstruction, :]\n",
        "\n",
        "                            sqrt_S_svd_k = torch.sqrt(S_svd_k)\n",
        "\n",
        "                            A_reproj = U_svd_k @ torch.diag(sqrt_S_svd_k)\n",
        "                            B_reproj = torch.diag(sqrt_S_svd_k) @ Vh_svd_k\n",
        "\n",
        "                        flex_lora_module.lora_A.data.copy_(A_reproj)\n",
        "                        flex_lora_module.lora_B.data.copy_(B_reproj)\n",
        "\n",
        "                    except Exception as e:\n",
        "                        # print(f\"Error during SVD reconstruction in neural reprojection for {name}: {e}. Skipping update for this layer.\")\n",
        "                        pass # Continue to the next layer\n",
        "\n",
        "    def _perform_rank_adjustment(self):\n",
        "        for name, module in self.model.named_modules():\n",
        "            if isinstance(module, FlexLoRA):\n",
        "                sv_tracker = self.sv_trackers.get(name)\n",
        "                rank_sched = self.rank_schedulers.get(name)\n",
        "                if sv_tracker and rank_sched:\n",
        "                    sv_tracker.forward() # Update SV history\n",
        "                    rank_adjustment(\n",
        "                        layer=module,\n",
        "                        sv_tracker=sv_tracker,\n",
        "                        rank_scheduler=rank_sched,\n",
        "                        sv_history_min_samples=self.sv_history_min_samples\n",
        "                    )\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        \"\"\"\n",
        "        Performs a single GRIT optimization step.\n",
        "        This function fully replaces the standard optimizer step for LoRA parameters\n",
        "        when GRIT is solely responsible for their updates.\n",
        "        It should be called after model.backward() (which computes gradients for LoRA parameters)\n",
        "        and before an external optimizer might zero gradients if such an optimizer is used for non-LoRA parameters.\n",
        "\n",
        "        The KFAC factors (A and B) are accumulated via hooks during the forward and backward passes.\n",
        "        These factors are used to estimate the Fisher Information Matrix (FIM) block-wise.\n",
        "        The dominant eigenspace of this FIM is then used for neural reprojection of LoRA updates.\n",
        "\n",
        "        Args:\n",
        "            closure: An optional closure that re-evaluates the model and returns the loss.\n",
        "        \"\"\"\n",
        "        # 1. Update KFAC factors (usually done by hooks) and then Fisher information\n",
        "        # This assumes forward/backward has just happened.\n",
        "        self._update_kfac_factors_and_fisher() # KFAC hooks are expected to have been populated by a preceding backward pass\n",
        "\n",
        "        # 2. Natural Gradient Step for LoRA parameters\n",
        "        # This replaces the standard optimizer step for LoRA parameters.\n",
        "        natural_gradient_step(self.model, lr=self.learning_rate, damping_lora_fisher=self.damping_lora_fisher)\n",
        "\n",
        "        # 3. Neural Reprojection\n",
        "        self._apply_neural_reprojection()\n",
        "\n",
        "        # 4. Rank Adjustment (includes SV tracking)\n",
        "        self._perform_rank_adjustment()\n",
        "\n",
        "        # 5. Parameter Fusion (Momentum Smoothing)\n",
        "        self.momentum_fuser(self.model) # Apply momentum to LoRA parameters\n",
        "\n",
        "    def forward(self, *args, **kwargs):\n",
        "        \"\"\"Forward pass through the underlying model.\"\"\"\n",
        "        logits = self.model(*args, **kwargs)\n",
        "        if self.training: # Only store logits during training for KFAC True Fisher\n",
        "            self.current_model_logits_for_kfac = logits\n",
        "        return logits\n",
        "\n",
        "    def parameters(self, recurse: bool = True):\n",
        "        \"\"\"Returns parameters of the adapted model that are trainable (LoRA params).\"\"\"\n",
        "        return filter(lambda p: p.requires_grad, self.model.parameters(recurse))\n",
        "\n",
        "    def named_parameters(self, prefix: str = '', recurse: bool = True):\n",
        "        \"\"\"Returns named parameters of the adapted model that are trainable.\"\"\"\n",
        "        return filter(lambda kv: kv[1].requires_grad, self.model.named_parameters(prefix, recurse))\n",
        "\n",
        "    def state_dict(self, destination=None, prefix='', keep_vars=False):\n",
        "        \"\"\"Returns a state dict containing only the LoRA parameters.\"\"\"\n",
        "        # It's often better to save the state_dict of the original model,\n",
        "        # and then separately save LoRA parameters if needed, or re-inject on load.\n",
        "        # However, for a pure adapter, one might want to save only LoRA parts.\n",
        "        lora_state_dict = {}\n",
        "        for name, param in self.model.named_parameters():\n",
        "            if param.requires_grad: # Assuming only LoRA parameters are trainable\n",
        "                 if any(sub in name for sub in self.layers_to_adapt_substrings): # Further check if it's a LoRA param\n",
        "                    lora_state_dict[name] = param.data\n",
        "        return lora_state_dict # This might need refinement based on FlexLoRA naming\n",
        "\n",
        "    def load_state_dict(self, state_dict, strict=True):\n",
        "        \"\"\"Loads LoRA parameters into the model.\"\"\"\n",
        "        # This needs to carefully map state_dict keys to the FlexLoRA parameters.\n",
        "        # It's safer to load the full model and then re-apply GRIT,\n",
        "        # or to have a dedicated save/load for LoRA parameters.\n",
        "        # For now, this is a placeholder. A robust implementation would iterate\n",
        "        # through FlexLoRA modules and load 'lora_A' and 'lora_B'.\n",
        "        # self.model.load_state_dict(state_dict, strict=False) # Be careful with strict=False\n",
        "\n",
        "        # A more targeted way for FlexLoRA:\n",
        "        current_model_state_dict = self.model.state_dict()\n",
        "        for name, param_data in state_dict.items():\n",
        "            if name in current_model_state_dict:\n",
        "                # Check if this parameter belongs to a FlexLoRA module and should be loaded\n",
        "                # This assumes keys in state_dict directly match model param names (e.g., \"blocks.0.attn.q_proj.lora_A\")\n",
        "                module_path = name.split('.')[:-1] # e.g., ['blocks', '0', 'attn', 'q_proj']\n",
        "                param_short_name = name.split('.')[-1] # e.g., 'lora_A'\n",
        "\n",
        "                try:\n",
        "                    parent_module_path = \".\".join(module_path)\n",
        "                    parent_module = self.model.get_submodule(parent_module_path)\n",
        "                    if isinstance(parent_module, FlexLoRA) and (param_short_name == 'lora_A' or param_short_name == 'lora_B'):\n",
        "                         current_model_state_dict[name].copy_(param_data)\n",
        "                    # else if not a FlexLoRA or not lora_A/B, it might be other trainable param if any\n",
        "                except AttributeError:\n",
        "                    if strict:\n",
        "                        raise RuntimeError(f\"Error loading {name} into GRITAdapter: module path not found.\")\n",
        "            elif strict:\n",
        "                raise RuntimeError(f\"Error loading {name} into GRITAdapter: parameter not found in model.\")\n",
        "\n",
        "        # After potentially updating parts of the state dict, load it into the model.\n",
        "        # This approach is still a bit risky if state_dict contains non-LoRA params.\n",
        "        # A truly clean way is to ensure state_dict ONLY contains LoRA weights\n",
        "        # and load them specifically.\n",
        "        # For simplicity here, if state_dict is purely LoRA, this would be fine:\n",
        "        self.model.load_state_dict(current_model_state_dict, strict=strict)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-28T12:32:53.588557Z",
          "iopub.execute_input": "2025-05-28T12:32:53.588770Z",
          "iopub.status.idle": "2025-05-28T12:32:53.617830Z",
          "shell.execute_reply.started": "2025-05-28T12:32:53.588748Z",
          "shell.execute_reply": "2025-05-28T12:32:53.617266Z"
        },
        "id": "2OTH0_ue5wIe"
      },
      "outputs": [],
      "execution_count": 24
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 1: PREPARING DATASET\n"
      ],
      "metadata": {
        "id": "jtStB4xL5wIf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def download_and_load_file(file_path, url):\n",
        "    ssl_context = ssl.create_default_context()\n",
        "    ssl_context.check_hostname = False\n",
        "    ssl_context.verify_mode = ssl.CERT_NONE\n",
        "\n",
        "    if not os.path.exists(file_path):\n",
        "        with urllib.request.urlopen(url, context=ssl_context) as response:\n",
        "            text_data = response.read().decode(\"utf-8\")\n",
        "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
        "            file.write(text_data)\n",
        "    else:\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "            text_data = file.read()\n",
        "\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-28T12:32:53.618451Z",
          "iopub.execute_input": "2025-05-28T12:32:53.618647Z",
          "iopub.status.idle": "2025-05-28T12:32:53.636327Z",
          "shell.execute_reply.started": "2025-05-28T12:32:53.618634Z",
          "shell.execute_reply": "2025-05-28T12:32:53.635679Z"
        },
        "id": "uqJ8ZkVX5wIf"
      },
      "outputs": [],
      "execution_count": 25
    },
    {
      "cell_type": "code",
      "source": [
        "# Download instruction dataset\n",
        "file_path = \"instruction-data.json\"\n",
        "url = (\n",
        "    \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch\"\n",
        "    \"/main/ch07/01_main-chapter-code/instruction-data.json\"\n",
        ")\n",
        "\n",
        "data = download_and_load_file(file_path, url)\n",
        "print(\"Number of entries:\", len(data))\n",
        "\n",
        "# ALPACA FORMAT for instructions\n",
        "def format_input(entry):\n",
        "    instruction_text = (\n",
        "        f\"Below is an instruction that describes a task. \"\n",
        "        f\"Write a response that appropriately completes the request.\"\n",
        "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
        "    )\n",
        "\n",
        "    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
        "\n",
        "    return instruction_text + input_text\n",
        "\n",
        "# SPLITTING DATASET INTO TRAIN-TEST-VALIDATION\n",
        "train_portion = int(len(data) * 0.85)  # 85% for training\n",
        "test_portion = int(len(data) * 0.1)    # 10% for testing\n",
        "val_portion = len(data) - train_portion - test_portion  # Remaining 5% for validation\n",
        "\n",
        "train_data = data[:train_portion]\n",
        "test_data = data[train_portion:train_portion + test_portion]\n",
        "val_data = data[train_portion + test_portion:]\n",
        "print(\"Training set length:\", len(train_data))\n",
        "print(\"Validation set length:\", len(val_data))\n",
        "print(\"Test set length:\", len(test_data))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-28T12:32:53.636953Z",
          "iopub.execute_input": "2025-05-28T12:32:53.637114Z",
          "iopub.status.idle": "2025-05-28T12:32:53.855336Z",
          "shell.execute_reply.started": "2025-05-28T12:32:53.637102Z",
          "shell.execute_reply": "2025-05-28T12:32:53.854467Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lES85Z785wIf",
        "outputId": "3c87e36e-3042-42b5-f4fd-014d0f5856c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of entries: 1100\n",
            "Training set length: 935\n",
            "Validation set length: 55\n",
            "Test set length: 110\n"
          ]
        }
      ],
      "execution_count": 26
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 2: ORGANIZING DATA INTO TRAINING BATCHES\n"
      ],
      "metadata": {
        "id": "Q-Ips5mR5wIf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize tokenizer\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "# Create InstructionDataset class for preprocessing\n",
        "class InstructionDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer):\n",
        "        self.data = data\n",
        "\n",
        "        # Pre-tokenize texts\n",
        "        self.encoded_texts = []\n",
        "        for entry in data:\n",
        "            instruction_plus_input = format_input(entry)\n",
        "            response_text = f\"\\n\\n### Response:\\n{entry['output']}\"\n",
        "            full_text = instruction_plus_input + response_text\n",
        "            self.encoded_texts.append(\n",
        "                tokenizer.encode(full_text)\n",
        "            )\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.encoded_texts[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "# Custom collate function for batching\n",
        "def custom_collate_fn(\n",
        "    batch,\n",
        "    pad_token_id=50256,\n",
        "    ignore_index=-100,\n",
        "    allowed_max_length=None,\n",
        "    device=\"cpu\"\n",
        "):\n",
        "    # Find the longest sequence in the batch\n",
        "    batch_max_length = max(len(item)+1 for item in batch)\n",
        "\n",
        "    # Pad and prepare inputs and targets\n",
        "    inputs_lst, targets_lst = [], []\n",
        "\n",
        "    for item in batch:\n",
        "        new_item = item.copy()\n",
        "        # Add an <|endoftext|> token\n",
        "        new_item += [pad_token_id]\n",
        "        # Pad sequences to max_length\n",
        "        padded = (\n",
        "            new_item + [pad_token_id] *\n",
        "            (batch_max_length - len(new_item))\n",
        "        )\n",
        "        inputs = torch.tensor(padded[:-1])  # Truncate the last token for inputs\n",
        "        targets = torch.tensor(padded[1:])  # Shift +1 to the right for targets\n",
        "\n",
        "        # Replace all but the first padding tokens in targets by ignore_index\n",
        "        mask = targets == pad_token_id\n",
        "        indices = torch.nonzero(mask).squeeze()\n",
        "        if indices.numel() > 1:\n",
        "            targets[indices[1:]] = ignore_index\n",
        "\n",
        "        # Optionally truncate to maximum sequence length\n",
        "        if allowed_max_length is not None:\n",
        "            inputs = inputs[:allowed_max_length]\n",
        "            targets = targets[:allowed_max_length]\n",
        "\n",
        "        inputs_lst.append(inputs)\n",
        "        targets_lst.append(targets)\n",
        "\n",
        "    # Convert list of inputs and targets to tensors and transfer to target device\n",
        "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
        "    targets_tensor = torch.stack(targets_lst).to(device)\n",
        "\n",
        "    return inputs_tensor, targets_tensor"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-28T12:32:53.856344Z",
          "iopub.execute_input": "2025-05-28T12:32:53.857050Z",
          "iopub.status.idle": "2025-05-28T12:32:55.181275Z",
          "shell.execute_reply.started": "2025-05-28T12:32:53.857013Z",
          "shell.execute_reply": "2025-05-28T12:32:55.180704Z"
        },
        "id": "GeLdLFaT5wIl"
      },
      "outputs": [],
      "execution_count": 27
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 3: CREATING DATALOADERS FOR AN INSTRUCTION DATASET\n"
      ],
      "metadata": {
        "id": "re2chKnj5wIl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Uncomment for Apple Silicon support\n",
        "# if torch.cuda.is_available():\n",
        "#     device = torch.device(\"cuda\")\n",
        "# elif torch.backends.mps.is_available():\n",
        "#     device = torch.device(\"mps\")\n",
        "# else:\n",
        "#     device = torch.device(\"cpu\")\n",
        "\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# Customize collate function for specific device\n",
        "customized_collate_fn = partial(custom_collate_fn, device=device, allowed_max_length=1024)\n",
        "\n",
        "# Create DataLoaders for training and evaluation\n",
        "num_workers = 0\n",
        "batch_size = 8\n",
        "torch.manual_seed(123)\n",
        "\n",
        "train_dataset = InstructionDataset(train_data, tokenizer)\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=customized_collate_fn,\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        "    num_workers=num_workers\n",
        ")\n",
        "\n",
        "val_dataset = InstructionDataset(val_data, tokenizer)\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=customized_collate_fn,\n",
        "    shuffle=False,\n",
        "    drop_last=False,\n",
        "    num_workers=num_workers\n",
        ")\n",
        "\n",
        "test_dataset = InstructionDataset(test_data, tokenizer)\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=customized_collate_fn,\n",
        "    shuffle=False,\n",
        "    drop_last=False,\n",
        "    num_workers=num_workers\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-28T12:32:55.184348Z",
          "iopub.execute_input": "2025-05-28T12:32:55.184554Z",
          "iopub.status.idle": "2025-05-28T12:32:55.232168Z",
          "shell.execute_reply.started": "2025-05-28T12:32:55.184539Z",
          "shell.execute_reply": "2025-05-28T12:32:55.231600Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lSOC4Hqs5wIm",
        "outputId": "306dbf09-d190-44bb-84ac-3d6278799b3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ],
      "execution_count": 28
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 4: LOADING A PRETRAINED LLM\n"
      ],
      "metadata": {
        "id": "g572WoZS5wIm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def assign(left, right):\n",
        "    if left.shape != right.shape:\n",
        "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
        "    return torch.nn.Parameter(torch.tensor(right))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-28T12:32:55.232733Z",
          "iopub.execute_input": "2025-05-28T12:32:55.232928Z",
          "iopub.status.idle": "2025-05-28T12:32:55.236650Z",
          "shell.execute_reply.started": "2025-05-28T12:32:55.232912Z",
          "shell.execute_reply": "2025-05-28T12:32:55.235990Z"
        },
        "id": "6UhBJAv35wIm"
      },
      "outputs": [],
      "execution_count": 29
    },
    {
      "cell_type": "code",
      "source": [
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\": 50257,   # Vocabulary size\n",
        "    \"context_length\": 1024, # Shortened context length (orig: 1024)\n",
        "    \"emb_dim\": 768,        # Embedding dimension\n",
        "    \"n_heads\": 12,         # Number of attention heads\n",
        "    \"n_layers\": 12,        # Number of layers\n",
        "    \"drop_rate\": 0.1,      # Dropout rate\n",
        "    \"qkv_bias\": True      # Query-key-value bias\n",
        "}"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-28T12:32:55.237357Z",
          "iopub.execute_input": "2025-05-28T12:32:55.238024Z",
          "iopub.status.idle": "2025-05-28T12:32:55.251113Z",
          "shell.execute_reply.started": "2025-05-28T12:32:55.238003Z",
          "shell.execute_reply": "2025-05-28T12:32:55.250433Z"
        },
        "id": "1wwBzRGb5wIn"
      },
      "outputs": [],
      "execution_count": 30
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.att = MultiHeadAttention(\n",
        "            d_in=cfg[\"emb_dim\"],\n",
        "            d_out=cfg[\"emb_dim\"],\n",
        "            context_length=cfg[\"context_length\"],\n",
        "            num_heads=cfg[\"n_heads\"],\n",
        "            dropout=cfg[\"drop_rate\"],\n",
        "            qkv_bias=cfg[\"qkv_bias\"])\n",
        "        self.ff = FeedForward(cfg)\n",
        "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Shortcut connection for attention block\n",
        "        shortcut = x\n",
        "        x = self.norm1(x)\n",
        "        x = self.att(x)  # Shape [batch_size, num_tokens, emb_size]\n",
        "        x = self.drop_shortcut(x)\n",
        "        x = x + shortcut  # Add the original input back\n",
        "\n",
        "        # Shortcut connection for feed forward block\n",
        "        shortcut = x\n",
        "        x = self.norm2(x)\n",
        "        x = self.ff(x)\n",
        "        # 2*4*768\n",
        "        x = self.drop_shortcut(x)\n",
        "        x = x + shortcut  # Add the original input back\n",
        "\n",
        "        return x\n",
        "        # 2*4*768"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-28T12:32:55.251812Z",
          "iopub.execute_input": "2025-05-28T12:32:55.252101Z",
          "iopub.status.idle": "2025-05-28T12:32:55.271692Z",
          "shell.execute_reply.started": "2025-05-28T12:32:55.252084Z",
          "shell.execute_reply": "2025-05-28T12:32:55.271046Z"
        },
        "id": "sJMTNV4h5wIn"
      },
      "outputs": [],
      "execution_count": 31
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        assert (d_out % num_heads == 0), \\\n",
        "            \"d_out must be divisible by num_heads\"\n",
        "\n",
        "        self.d_out = d_out\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
        "\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer(\n",
        "            \"mask\",\n",
        "            torch.triu(torch.ones(context_length, context_length),\n",
        "                       diagonal=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, d_in = x.shape\n",
        "\n",
        "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
        "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
        "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
        "        keys = keys.transpose(1, 2)\n",
        "        queries = queries.transpose(1, 2)\n",
        "        values = values.transpose(1, 2)\n",
        "\n",
        "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
        "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
        "\n",
        "        # Original mask truncated to the number of tokens and converted to boolean\n",
        "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
        "\n",
        "        # Use the mask to fill attention scores\n",
        "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
        "\n",
        "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
        "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
        "\n",
        "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
        "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
        "        context_vec = self.out_proj(context_vec) # optional projection\n",
        "\n",
        "        return context_vec"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-28T12:32:55.272489Z",
          "iopub.execute_input": "2025-05-28T12:32:55.272695Z",
          "iopub.status.idle": "2025-05-28T12:32:55.286387Z",
          "shell.execute_reply.started": "2025-05-28T12:32:55.272677Z",
          "shell.execute_reply": "2025-05-28T12:32:55.285830Z"
        },
        "id": "F6hTOo0N5wIn"
      },
      "outputs": [],
      "execution_count": 32
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, emb_dim):\n",
        "        super().__init__()\n",
        "        self.eps = 1e-5\n",
        "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
        "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
        "        return self.scale * norm_x + self.shift\n",
        "\n",
        "class GELU(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1 + torch.tanh(\n",
        "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
        "            (x + 0.044715 * torch.pow(x, 3))\n",
        "        ))\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]), ## Expansion\n",
        "            GELU(), ## Activation\n",
        "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]), ## Contraction\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-28T12:32:55.287318Z",
          "iopub.execute_input": "2025-05-28T12:32:55.287600Z",
          "iopub.status.idle": "2025-05-28T12:32:55.305993Z",
          "shell.execute_reply.started": "2025-05-28T12:32:55.287583Z",
          "shell.execute_reply": "2025-05-28T12:32:55.305340Z"
        },
        "id": "aS0Kq5_Y5wIo"
      },
      "outputs": [],
      "execution_count": 33
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTModel(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
        "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
        "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "        self.trf_blocks = nn.Sequential(\n",
        "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
        "\n",
        "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.out_head = nn.Linear(\n",
        "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
        "        )\n",
        "\n",
        "    def forward(self, in_idx):\n",
        "        batch_size, seq_len = in_idx.shape\n",
        "        tok_embeds = self.tok_emb(in_idx)\n",
        "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
        "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
        "        x = self.drop_emb(x)\n",
        "        x = self.trf_blocks(x)\n",
        "        x = self.final_norm(x)\n",
        "        logits = self.out_head(x)\n",
        "        return logits\n",
        "\n",
        "torch.manual_seed(123)\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "model.eval();  # Disable dropout during inference"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-28T12:32:55.306589Z",
          "iopub.execute_input": "2025-05-28T12:32:55.306815Z",
          "iopub.status.idle": "2025-05-28T12:32:56.763065Z",
          "shell.execute_reply.started": "2025-05-28T12:32:55.306794Z",
          "shell.execute_reply": "2025-05-28T12:32:56.762418Z"
        },
        "id": "DVVqcagp5wIo"
      },
      "outputs": [],
      "execution_count": 34
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def load_weights_into_gpt(gpt, params):\n",
        "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])\n",
        "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n",
        "\n",
        "    for b in range(len(params[\"blocks\"])):\n",
        "        q_w, k_w, v_w = np.split(\n",
        "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
        "        gpt.trf_blocks[b].att.W_query.weight = assign(\n",
        "            gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n",
        "        gpt.trf_blocks[b].att.W_key.weight = assign(\n",
        "            gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n",
        "        gpt.trf_blocks[b].att.W_value.weight = assign(\n",
        "            gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n",
        "\n",
        "        q_b, k_b, v_b = np.split(\n",
        "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
        "        gpt.trf_blocks[b].att.W_query.bias = assign(\n",
        "            gpt.trf_blocks[b].att.W_query.bias, q_b)\n",
        "        gpt.trf_blocks[b].att.W_key.bias = assign(\n",
        "            gpt.trf_blocks[b].att.W_key.bias, k_b)\n",
        "        gpt.trf_blocks[b].att.W_value.bias = assign(\n",
        "            gpt.trf_blocks[b].att.W_value.bias, v_b)\n",
        "\n",
        "        gpt.trf_blocks[b].att.out_proj.weight = assign(\n",
        "            gpt.trf_blocks[b].att.out_proj.weight,\n",
        "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
        "        gpt.trf_blocks[b].att.out_proj.bias = assign(\n",
        "            gpt.trf_blocks[b].att.out_proj.bias,\n",
        "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
        "\n",
        "        gpt.trf_blocks[b].ff.layers[0].weight = assign(\n",
        "            gpt.trf_blocks[b].ff.layers[0].weight,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
        "        gpt.trf_blocks[b].ff.layers[0].bias = assign(\n",
        "            gpt.trf_blocks[b].ff.layers[0].bias,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
        "        gpt.trf_blocks[b].ff.layers[2].weight = assign(\n",
        "            gpt.trf_blocks[b].ff.layers[2].weight,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
        "        gpt.trf_blocks[b].ff.layers[2].bias = assign(\n",
        "            gpt.trf_blocks[b].ff.layers[2].bias,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
        "\n",
        "        gpt.trf_blocks[b].norm1.scale = assign(\n",
        "            gpt.trf_blocks[b].norm1.scale,\n",
        "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
        "        gpt.trf_blocks[b].norm1.shift = assign(\n",
        "            gpt.trf_blocks[b].norm1.shift,\n",
        "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
        "        gpt.trf_blocks[b].norm2.scale = assign(\n",
        "            gpt.trf_blocks[b].norm2.scale,\n",
        "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
        "        gpt.trf_blocks[b].norm2.shift = assign(\n",
        "            gpt.trf_blocks[b].norm2.shift,\n",
        "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
        "\n",
        "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
        "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
        "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-28T12:32:56.763810Z",
          "iopub.execute_input": "2025-05-28T12:32:56.764067Z",
          "iopub.status.idle": "2025-05-28T12:32:56.775227Z",
          "shell.execute_reply.started": "2025-05-28T12:32:56.764038Z",
          "shell.execute_reply": "2025-05-28T12:32:56.774532Z"
        },
        "id": "2Tb6cBsg5wIp"
      },
      "outputs": [],
      "execution_count": 35
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
        "\n",
        "    # For-loop is the same as before: Get logits, and only focus on last time step\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "        logits = logits[:, -1, :]\n",
        "\n",
        "        # New: Filter logits with top_k sampling\n",
        "        if top_k is not None:\n",
        "            # Keep only top_k values\n",
        "            top_logits, _ = torch.topk(logits, top_k)\n",
        "            min_val = top_logits[:, -1]\n",
        "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
        "\n",
        "        # New: Apply temperature scaling\n",
        "        if temperature > 0.0:\n",
        "            logits = logits / temperature\n",
        "\n",
        "            # Apply softmax to get probabilities\n",
        "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
        "\n",
        "            # Sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
        "\n",
        "        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
        "        else:\n",
        "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
        "\n",
        "        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
        "            break\n",
        "\n",
        "        # Same as before: append sampled index to the running sequence\n",
        "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
        "\n",
        "    return idx"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-28T12:32:56.775994Z",
          "iopub.execute_input": "2025-05-28T12:32:56.776203Z",
          "iopub.status.idle": "2025-05-28T12:32:56.802952Z",
          "shell.execute_reply.started": "2025-05-28T12:32:56.776189Z",
          "shell.execute_reply": "2025-05-28T12:32:56.802316Z"
        },
        "id": "ra7a85Jv5wIp"
      },
      "outputs": [],
      "execution_count": 36
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "\n",
        "def text_to_token_ids(text, tokenizer):\n",
        "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
        "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
        "    return encoded_tensor\n",
        "\n",
        "def token_ids_to_text(token_ids, tokenizer):\n",
        "    flat = token_ids.squeeze(0) # remove batch dimension\n",
        "    return tokenizer.decode(flat.tolist())"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-28T12:32:56.803565Z",
          "iopub.execute_input": "2025-05-28T12:32:56.803758Z",
          "iopub.status.idle": "2025-05-28T12:32:56.821959Z",
          "shell.execute_reply.started": "2025-05-28T12:32:56.803743Z",
          "shell.execute_reply": "2025-05-28T12:32:56.821284Z"
        },
        "id": "7DElxhX15wIq"
      },
      "outputs": [],
      "execution_count": 37
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure GPT model\n",
        "BASE_CONFIG = {\n",
        "    \"vocab_size\": 50257,     # Vocabulary size\n",
        "    \"context_length\": 1024,  # Context length\n",
        "    \"drop_rate\": 0.0,        # Dropout rate\n",
        "    \"qkv_bias\": True         # Query-key-value bias\n",
        "}\n",
        "\n",
        "model_configs = {\n",
        "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
        "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
        "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
        "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
        "}\n",
        "\n",
        "CHOOSE_MODEL = \"gpt2-medium (355M)\"\n",
        "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
        "\n",
        "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
        "settings, params = download_and_load_gpt2(\n",
        "    model_size=model_size,\n",
        "    models_dir=\"gpt2\"\n",
        ")\n",
        "\n",
        "model = GPTModel(BASE_CONFIG)\n",
        "load_weights_into_gpt(model, params)\n",
        "model.eval()\n",
        "\n",
        "# Instantiate GRITAdapter\n",
        "# Target layers for LoRA based on typical GPT-2 structure within TransformerBlock:\n",
        "# att.W_query, att.W_key, att.W_value, att.W_out (output projection)\n",
        "# ff.layers[0] (first FFN linear layer), ff.layers[2] (second FFN linear layer)\n",
        "layers_to_adapt = [\n",
        "    '.att.W_query', '.att.W_key', '.att.W_value', '.att.out_proj',\n",
        "    '.ff.layers.0', '.ff.layers.2'\n",
        "]\n",
        "\n",
        "grit_adapter = GRITAdapter(\n",
        "    model=model,\n",
        "    layers_to_adapt_substrings=layers_to_adapt, # Adjusted for GPTModel structure\n",
        "    rank_range=(8, 16), # Example rank range (min_rank, max_rank)\n",
        "    learning_rate=5e-4, # GRIT-specific learning rate for LoRA params\n",
        "    kfac_ema_decay=0.95,\n",
        "    kfac_damping=(1e-7, 1e-5),\n",
        "    fisher_k=0.1,\n",
        "    rho_eigen_sum=0.9,\n",
        "    # Other GRIT params can be added here if defaults are not suitable\n",
        ")\n",
        "grit_adapter.to(device) # Move GRITAdapter and its managed model to device"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-28T12:32:56.822758Z",
          "iopub.execute_input": "2025-05-28T12:32:56.823534Z",
          "iopub.status.idle": "2025-05-28T12:34:54.510111Z",
          "shell.execute_reply.started": "2025-05-28T12:32:56.823513Z",
          "shell.execute_reply": "2025-05-28T12:34:54.509358Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ot6njd4C5wIq",
        "outputId": "28e474ba-0812-4857-ad32-7179b6fbee77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "checkpoint: 100%|| 77.0/77.0 [00:00<00:00, 188kiB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "encoder.json: 100%|| 1.04M/1.04M [00:00<00:00, 2.46MiB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "hparams.json: 100%|| 91.0/91.0 [00:00<00:00, 231kiB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "model.ckpt.data-00000-of-00001: 100%|| 1.42G/1.42G [02:31<00:00, 9.39MiB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "model.ckpt.index: 100%|| 10.4k/10.4k [00:00<00:00, 13.1MiB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "model.ckpt.meta: 100%|| 927k/927k [00:00<00:00, 2.16MiB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "vocab.bpe: 100%|| 456k/456k [00:00<00:00, 1.59MiB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GRITAdapter(\n",
              "  (model): GPTModel(\n",
              "    (tok_emb): Embedding(50257, 1024)\n",
              "    (pos_emb): Embedding(1024, 1024)\n",
              "    (drop_emb): Dropout(p=0.0, inplace=False)\n",
              "    (trf_blocks): Sequential(\n",
              "      (0): TransformerBlock(\n",
              "        (att): MultiHeadAttention(\n",
              "          (W_query): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x1024))\n",
              "          (W_key): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x1024))\n",
              "          (W_value): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x1024))\n",
              "          (out_proj): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x1024))\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ff): FeedForward(\n",
              "          (layers): Sequential(\n",
              "            (0): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x4096))\n",
              "            (1): GELU()\n",
              "            (2): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(4096x1024))\n",
              "          )\n",
              "        )\n",
              "        (norm1): LayerNorm()\n",
              "        (norm2): LayerNorm()\n",
              "        (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (1): TransformerBlock(\n",
              "        (att): MultiHeadAttention(\n",
              "          (W_query): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x1024))\n",
              "          (W_key): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x1024))\n",
              "          (W_value): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x1024))\n",
              "          (out_proj): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x1024))\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ff): FeedForward(\n",
              "          (layers): Sequential(\n",
              "            (0): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x4096))\n",
              "            (1): GELU()\n",
              "            (2): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(4096x1024))\n",
              "          )\n",
              "        )\n",
              "        (norm1): LayerNorm()\n",
              "        (norm2): LayerNorm()\n",
              "        (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (2): TransformerBlock(\n",
              "        (att): MultiHeadAttention(\n",
              "          (W_query): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x1024))\n",
              "          (W_key): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x1024))\n",
              "          (W_value): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x1024))\n",
              "          (out_proj): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x1024))\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ff): FeedForward(\n",
              "          (layers): Sequential(\n",
              "            (0): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x4096))\n",
              "            (1): GELU()\n",
              "            (2): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(4096x1024))\n",
              "          )\n",
              "        )\n",
              "        (norm1): LayerNorm()\n",
              "        (norm2): LayerNorm()\n",
              "        (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (3): TransformerBlock(\n",
              "        (att): MultiHeadAttention(\n",
              "          (W_query): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x1024))\n",
              "          (W_key): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x1024))\n",
              "          (W_value): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x1024))\n",
              "          (out_proj): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x1024))\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ff): FeedForward(\n",
              "          (layers): Sequential(\n",
              "            (0): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x4096))\n",
              "            (1): GELU()\n",
              "            (2): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(4096x1024))\n",
              "          )\n",
              "        )\n",
              "        (norm1): LayerNorm()\n",
              "        (norm2): LayerNorm()\n",
              "        (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (4): TransformerBlock(\n",
              "        (att): MultiHeadAttention(\n",
              "          (W_query): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x1024))\n",
              "          (W_key): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x1024))\n",
              "          (W_value): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x1024))\n",
              "          (out_proj): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x1024))\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ff): FeedForward(\n",
              "          (layers): Sequential(\n",
              "            (0): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x4096))\n",
              "            (1): GELU()\n",
              "            (2): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(4096x1024))\n",
              "          )\n",
              "        )\n",
              "        (norm1): LayerNorm()\n",
              "        (norm2): LayerNorm()\n",
              "        (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (5): TransformerBlock(\n",
              "        (att): MultiHeadAttention(\n",
              "          (W_query): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x1024))\n",
              "          (W_key): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x1024))\n",
              "          (W_value): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x1024))\n",
              "          (out_proj): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x1024))\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ff): FeedForward(\n",
              "          (layers): Sequential(\n",
              "            (0): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x4096))\n",
              "            (1): GELU()\n",
              "            (2): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(4096x1024))\n",
              "          )\n",
              "        )\n",
              "        (norm1): LayerNorm()\n",
              "        (norm2): LayerNorm()\n",
              "        (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (6): TransformerBlock(\n",
              "        (att): MultiHeadAttention(\n",
              "          (W_query): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x1024))\n",
              "          (W_key): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x1024))\n",
              "          (W_value): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x1024))\n",
              "          (out_proj): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x1024))\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ff): FeedForward(\n",
              "          (layers): Sequential(\n",
              "            (0): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x4096))\n",
              "            (1): GELU()\n",
              "            (2): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(4096x1024))\n",
              "          )\n",
              "        )\n",
              "        (norm1): LayerNorm()\n",
              "        (norm2): LayerNorm()\n",
              "        (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (7): TransformerBlock(\n",
              "        (att): MultiHeadAttention(\n",
              "          (W_query): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x1024))\n",
              "          (W_key): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x1024))\n",
              "          (W_value): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x1024))\n",
              "          (out_proj): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x1024))\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ff): FeedForward(\n",
              "          (layers): Sequential(\n",
              "            (0): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x4096))\n",
              "            (1): GELU()\n",
              "            (2): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(4096x1024))\n",
              "          )\n",
              "        )\n",
              "        (norm1): LayerNorm()\n",
              "        (norm2): LayerNorm()\n",
              "        (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (8): TransformerBlock(\n",
              "        (att): MultiHeadAttention(\n",
              "          (W_query): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x1024))\n",
              "          (W_key): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x1024))\n",
              "          (W_value): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x1024))\n",
              "          (out_proj): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x1024))\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ff): FeedForward(\n",
              "          (layers): Sequential(\n",
              "            (0): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x4096))\n",
              "            (1): GELU()\n",
              "            (2): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(4096x1024))\n",
              "          )\n",
              "        )\n",
              "        (norm1): LayerNorm()\n",
              "        (norm2): LayerNorm()\n",
              "        (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (9): TransformerBlock(\n",
              "        (att): MultiHeadAttention(\n",
              "          (W_query): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x1024))\n",
              "          (W_key): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x1024))\n",
              "          (W_value): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x1024))\n",
              "          (out_proj): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x1024))\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ff): FeedForward(\n",
              "          (layers): Sequential(\n",
              "            (0): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x4096))\n",
              "            (1): GELU()\n",
              "            (2): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(4096x1024))\n",
              "          )\n",
              "        )\n",
              "        (norm1): LayerNorm()\n",
              "        (norm2): LayerNorm()\n",
              "        (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (10): TransformerBlock(\n",
              "        (att): MultiHeadAttention(\n",
              "          (W_query): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x1024))\n",
              "          (W_key): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x1024))\n",
              "          (W_value): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x1024))\n",
              "          (out_proj): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x1024))\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ff): FeedForward(\n",
              "          (layers): Sequential(\n",
              "            (0): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x4096))\n",
              "            (1): GELU()\n",
              "            (2): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(4096x1024))\n",
              "          )\n",
              "        )\n",
              "        (norm1): LayerNorm()\n",
              "        (norm2): LayerNorm()\n",
              "        (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (11): TransformerBlock(\n",
              "        (att): MultiHeadAttention(\n",
              "          (W_query): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x1024))\n",
              "          (W_key): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x1024))\n",
              "          (W_value): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x1024))\n",
              "          (out_proj): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x1024))\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ff): FeedForward(\n",
              "          (layers): Sequential(\n",
              "            (0): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x4096))\n",
              "            (1): GELU()\n",
              "            (2): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(4096x1024))\n",
              "          )\n",
              "        )\n",
              "        (norm1): LayerNorm()\n",
              "        (norm2): LayerNorm()\n",
              "        (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (12): TransformerBlock(\n",
              "        (att): MultiHeadAttention(\n",
              "          (W_query): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x1024))\n",
              "          (W_key): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x1024))\n",
              "          (W_value): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x1024))\n",
              "          (out_proj): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x1024))\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ff): FeedForward(\n",
              "          (layers): Sequential(\n",
              "            (0): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x4096))\n",
              "            (1): GELU()\n",
              "            (2): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(4096x1024))\n",
              "          )\n",
              "        )\n",
              "        (norm1): LayerNorm()\n",
              "        (norm2): LayerNorm()\n",
              "        (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (13): TransformerBlock(\n",
              "        (att): MultiHeadAttention(\n",
              "          (W_query): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x1024))\n",
              "          (W_key): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x1024))\n",
              "          (W_value): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x1024))\n",
              "          (out_proj): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x1024))\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ff): FeedForward(\n",
              "          (layers): Sequential(\n",
              "            (0): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x4096))\n",
              "            (1): GELU()\n",
              "            (2): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(4096x1024))\n",
              "          )\n",
              "        )\n",
              "        (norm1): LayerNorm()\n",
              "        (norm2): LayerNorm()\n",
              "        (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (14): TransformerBlock(\n",
              "        (att): MultiHeadAttention(\n",
              "          (W_query): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x1024))\n",
              "          (W_key): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x1024))\n",
              "          (W_value): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x1024))\n",
              "          (out_proj): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x1024))\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ff): FeedForward(\n",
              "          (layers): Sequential(\n",
              "            (0): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x4096))\n",
              "            (1): GELU()\n",
              "            (2): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(4096x1024))\n",
              "          )\n",
              "        )\n",
              "        (norm1): LayerNorm()\n",
              "        (norm2): LayerNorm()\n",
              "        (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (15): TransformerBlock(\n",
              "        (att): MultiHeadAttention(\n",
              "          (W_query): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x1024))\n",
              "          (W_key): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x1024))\n",
              "          (W_value): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x1024))\n",
              "          (out_proj): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x1024))\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ff): FeedForward(\n",
              "          (layers): Sequential(\n",
              "            (0): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x4096))\n",
              "            (1): GELU()\n",
              "            (2): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(4096x1024))\n",
              "          )\n",
              "        )\n",
              "        (norm1): LayerNorm()\n",
              "        (norm2): LayerNorm()\n",
              "        (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (16): TransformerBlock(\n",
              "        (att): MultiHeadAttention(\n",
              "          (W_query): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x1024))\n",
              "          (W_key): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x1024))\n",
              "          (W_value): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x1024))\n",
              "          (out_proj): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x1024))\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ff): FeedForward(\n",
              "          (layers): Sequential(\n",
              "            (0): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x4096))\n",
              "            (1): GELU()\n",
              "            (2): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(4096x1024))\n",
              "          )\n",
              "        )\n",
              "        (norm1): LayerNorm()\n",
              "        (norm2): LayerNorm()\n",
              "        (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (17): TransformerBlock(\n",
              "        (att): MultiHeadAttention(\n",
              "          (W_query): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x1024))\n",
              "          (W_key): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x1024))\n",
              "          (W_value): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x1024))\n",
              "          (out_proj): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x1024))\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ff): FeedForward(\n",
              "          (layers): Sequential(\n",
              "            (0): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x4096))\n",
              "            (1): GELU()\n",
              "            (2): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(4096x1024))\n",
              "          )\n",
              "        )\n",
              "        (norm1): LayerNorm()\n",
              "        (norm2): LayerNorm()\n",
              "        (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (18): TransformerBlock(\n",
              "        (att): MultiHeadAttention(\n",
              "          (W_query): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x1024))\n",
              "          (W_key): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x1024))\n",
              "          (W_value): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x1024))\n",
              "          (out_proj): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x1024))\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ff): FeedForward(\n",
              "          (layers): Sequential(\n",
              "            (0): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x4096))\n",
              "            (1): GELU()\n",
              "            (2): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(4096x1024))\n",
              "          )\n",
              "        )\n",
              "        (norm1): LayerNorm()\n",
              "        (norm2): LayerNorm()\n",
              "        (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (19): TransformerBlock(\n",
              "        (att): MultiHeadAttention(\n",
              "          (W_query): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x1024))\n",
              "          (W_key): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x1024))\n",
              "          (W_value): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x1024))\n",
              "          (out_proj): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x1024))\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ff): FeedForward(\n",
              "          (layers): Sequential(\n",
              "            (0): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x4096))\n",
              "            (1): GELU()\n",
              "            (2): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(4096x1024))\n",
              "          )\n",
              "        )\n",
              "        (norm1): LayerNorm()\n",
              "        (norm2): LayerNorm()\n",
              "        (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (20): TransformerBlock(\n",
              "        (att): MultiHeadAttention(\n",
              "          (W_query): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x1024))\n",
              "          (W_key): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x1024))\n",
              "          (W_value): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x1024))\n",
              "          (out_proj): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x1024))\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ff): FeedForward(\n",
              "          (layers): Sequential(\n",
              "            (0): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x4096))\n",
              "            (1): GELU()\n",
              "            (2): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(4096x1024))\n",
              "          )\n",
              "        )\n",
              "        (norm1): LayerNorm()\n",
              "        (norm2): LayerNorm()\n",
              "        (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (21): TransformerBlock(\n",
              "        (att): MultiHeadAttention(\n",
              "          (W_query): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x1024))\n",
              "          (W_key): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x1024))\n",
              "          (W_value): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x1024))\n",
              "          (out_proj): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x1024))\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ff): FeedForward(\n",
              "          (layers): Sequential(\n",
              "            (0): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x4096))\n",
              "            (1): GELU()\n",
              "            (2): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(4096x1024))\n",
              "          )\n",
              "        )\n",
              "        (norm1): LayerNorm()\n",
              "        (norm2): LayerNorm()\n",
              "        (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (22): TransformerBlock(\n",
              "        (att): MultiHeadAttention(\n",
              "          (W_query): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x1024))\n",
              "          (W_key): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x1024))\n",
              "          (W_value): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x1024))\n",
              "          (out_proj): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x1024))\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ff): FeedForward(\n",
              "          (layers): Sequential(\n",
              "            (0): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x4096))\n",
              "            (1): GELU()\n",
              "            (2): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(4096x1024))\n",
              "          )\n",
              "        )\n",
              "        (norm1): LayerNorm()\n",
              "        (norm2): LayerNorm()\n",
              "        (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (23): TransformerBlock(\n",
              "        (att): MultiHeadAttention(\n",
              "          (W_query): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x1024))\n",
              "          (W_key): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x1024))\n",
              "          (W_value): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x1024))\n",
              "          (out_proj): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x1024))\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ff): FeedForward(\n",
              "          (layers): Sequential(\n",
              "            (0): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(1024x4096))\n",
              "            (1): GELU()\n",
              "            (2): FlexLoRA(initial_rank=8, current_rank=8, min_rank=8, max_rank=16, base_layer=Linear(4096x1024))\n",
              "          )\n",
              "        )\n",
              "        (norm1): LayerNorm()\n",
              "        (norm2): LayerNorm()\n",
              "        (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (final_norm): LayerNorm()\n",
              "    (out_head): Linear(in_features=1024, out_features=50257, bias=False)\n",
              "  )\n",
              "  (subspace_manager): SubspaceBuffer(\n",
              "    (subspace_module_list): ModuleList(\n",
              "      (0-143): 144 x Module()\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "execution_count": 38
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 5: FINETUNING THE LLM ON INSTRUCTION DATA\n"
      ],
      "metadata": {
        "id": "q99gityH5wIr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss calculation and training utilities\n",
        "def calc_loss_batch(input_batch, target_batch, model, device):\n",
        "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
        "    logits = model(input_batch)\n",
        "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
        "    return loss\n",
        "\n",
        "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
        "    total_loss = 0.\n",
        "    if len(data_loader) == 0:\n",
        "        return float(\"nan\")\n",
        "    elif num_batches is None:\n",
        "        num_batches = len(data_loader)\n",
        "    else:\n",
        "        # Reduce the number of batches to match the total number of batches in the data loader\n",
        "        # if num_batches exceeds the number of batches in the data loader\n",
        "        num_batches = min(num_batches, len(data_loader))\n",
        "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "        if i < num_batches:\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            total_loss += loss.item()\n",
        "        else:\n",
        "            break\n",
        "    return total_loss / num_batches\n",
        "\n",
        "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
        "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
        "    model.train()\n",
        "    return train_loss, val_loss\n",
        "\n",
        "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
        "    # If model is GRITAdapter, use model.model for generation if hooks interfere\n",
        "    # Or ensure GRITAdapter can be set to eval mode correctly for generation\n",
        "    eval_model = model.model if isinstance(model, GRITAdapter) else model\n",
        "    eval_model.eval()\n",
        "    with torch.no_grad():\n",
        "        token_ids = generate(\n",
        "            model=eval_model, # Use eval_model\n",
        "            idx=text_to_token_ids(start_context, tokenizer).to(device),\n",
        "            max_new_tokens=35,\n",
        "            context_size=BASE_CONFIG[\"context_length\"],\n",
        "            eos_id=50256,\n",
        "        )\n",
        "        generated_text = token_ids_to_text(token_ids, tokenizer)\n",
        "        response_text = generated_text[len(start_context):]\n",
        "        print(f\"Generated response: {response_text.strip()}\")\n",
        "    # model.train() # GRITAdapter's mode or its underlying model's mode will be set in the loop\n",
        "    if isinstance(model, GRITAdapter):\n",
        "        model.model.train() # Set underlying model to train\n",
        "        model.train() # Also set adapter to train if it has specific train mode behavior\n",
        "    else:\n",
        "        model.train()\n",
        "\n",
        "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
        "                      eval_freq, eval_iter, start_context, tokenizer, kfac_update_freq=100):\n",
        "    # Initialize lists to track losses and tokens seen\n",
        "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
        "    tokens_seen, global_step = 0, -1\n",
        "\n",
        "    # Main training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        if isinstance(model, GRITAdapter):\n",
        "            model.model.train() # Set base model to train mode\n",
        "            model.train()\n",
        "        else:\n",
        "            model.train()  # Set model to training mode for non-GRIT usage\n",
        "\n",
        "        for i, (input_batch, target_batch) in enumerate(train_loader):\n",
        "\n",
        "            # User Suggestion Point 1: Forward pass through GRITAdapter.\n",
        "            # The GRITAdapter.forward() method is called here, which in turn calls the base model.\n",
        "            # If model is in training, GRITAdapter.forward() saves the output logits\n",
        "            # in model.current_model_logits_for_kfac.\n",
        "            main_loss_logits = model(input_batch)\n",
        "\n",
        "            # User Suggestion Point 2 (Context): Compute the main cross-entropy loss.\n",
        "            # This loss is used for empirical Fisher updates and potentially for main parameter updates.\n",
        "            main_loss = torch.nn.functional.cross_entropy(main_loss_logits.flatten(0, 1), target_batch.flatten())\n",
        "\n",
        "            if isinstance(model, GRITAdapter):\n",
        "                # Check if it's time for a True Fisher update step\n",
        "                if global_step > 0 and global_step % kfac_update_freq == 0 and model.current_model_logits_for_kfac is not None:\n",
        "                    # User Suggestion Point 3: True Fisher step.\n",
        "                    # LoRA gradients are zeroed before the KFAC-specific backward pass\n",
        "                    # (aligning with User Suggestion Point 2's principle: \"Before that, LoRA gradients are zeroed...\").\n",
        "                    for p in model.parameters(): # model.parameters() in GRITAdapter yields only trainable (LoRA) ones\n",
        "                        if p.grad is not None:\n",
        "                            p.grad.detach_()\n",
        "                            p.grad.zero_()\n",
        "\n",
        "                    # Detach the saved logits and sample new \"labels\" from the model's predictive distribution.\n",
        "                    with torch.no_grad():\n",
        "                        logits_for_sampling = model.current_model_logits_for_kfac.detach()\n",
        "                        probs = torch.softmax(logits_for_sampling / 1.0, dim=-1) # Temperature = 1.0\n",
        "                        sampled_labels = torch.multinomial(probs.view(-1, probs.shape[-1]), num_samples=1).view(logits_for_sampling.shape[:-1])\n",
        "                        model.current_sampled_labels_for_kfac = sampled_labels\n",
        "\n",
        "                    # Compute the secondary cross-entropy (kfac_loss) using the original (graph-attached) logits\n",
        "                    # and the newly sampled (detached) labels.\n",
        "                    kfac_loss_logits = model.current_model_logits_for_kfac\n",
        "\n",
        "                    kfac_loss = torch.nn.functional.cross_entropy(\n",
        "                        kfac_loss_logits.flatten(0, 1),\n",
        "                        model.current_sampled_labels_for_kfac.flatten()\n",
        "                    )\n",
        "                    # The backward pass of kfac_loss populates KFAC gradient covariance (B_factor)\n",
        "                    # using gradients from the model's own predictive distribution (True Fisher).\n",
        "                    # It also populates .grad for LoRA parameters from this kfac_loss.\n",
        "                    kfac_loss.backward()\n",
        "\n",
        "                    # User Suggestion Point 5 (invoked from True Fisher path):\n",
        "                    # GRITAdapter.step() applies KFAC updates, natural gradient (using kfac_loss grads),\n",
        "                    # reprojection, rank adjustment, and momentum fusion, utilizing the True Fisher information.\n",
        "                    model.step()\n",
        "\n",
        "                    # Clear stored logits and labels after use.\n",
        "                    model.current_sampled_labels_for_kfac = None\n",
        "                    model.current_model_logits_for_kfac = None\n",
        "\n",
        "                else:\n",
        "                    # User Suggestion Point 4: Empirical Fisher step.\n",
        "                    # User Suggestion Point 2 (Backprop part): LoRA gradients are zeroed before the main loss backward pass\n",
        "                    # to isolate empirical Fisher effects for KFAC and for the natural gradient step.\n",
        "                    if hasattr(model, 'parameters'):\n",
        "                        for p in model.parameters(): # model.parameters() in GRITAdapter yields only trainable (LoRA) ones\n",
        "                            if p.grad is not None:\n",
        "                                p.grad.detach_()\n",
        "                                p.grad.zero_()\n",
        "\n",
        "                    # Backprop main loss: This populates KFAC B_factor with empirical Fisher gradients\n",
        "                    # (from main_loss gradients) and populates .grad for LoRA parameters from main_loss.\n",
        "                    main_loss.backward()\n",
        "\n",
        "                    # User Suggestion Point 5 (invoked from Empirical Fisher path):\n",
        "                    # GRITAdapter.step() applies KFAC updates (using empirical Fisher B_factor),\n",
        "                    # natural gradient (using main_loss grads), reprojection, rank adjustment, and momentum.\n",
        "                    model.step()\n",
        "\n",
        "            elif optimizer: # Standard optimizer step for non-GRIT models or non-LoRA parameters\n",
        "                # For non-GRIT models or if an optimizer manages non-LoRA params alongside GRIT\n",
        "                optimizer.zero_grad()\n",
        "                main_loss.backward() # Standard backward pass for the main loss\n",
        "                optimizer.step()\n",
        "\n",
        "            # For GRIT, optimizer.zero_grad() is implicitly handled for LoRA parameters by model.step()'s natural_gradient_step.\n",
        "\n",
        "            tokens_seen += input_batch.numel()\n",
        "            global_step += 1\n",
        "\n",
        "            # Optional evaluation step\n",
        "            if global_step % eval_freq == 0:\n",
        "                train_loss, val_loss = evaluate_model(\n",
        "                    model, train_loader, val_loader, device, eval_iter)\n",
        "                train_losses.append(train_loss)\n",
        "                val_losses.append(val_loss)\n",
        "                track_tokens_seen.append(tokens_seen)\n",
        "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
        "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
        "\n",
        "        # Print a sample text after each epoch\n",
        "        generate_and_print_sample(\n",
        "            model, tokenizer, device, start_context\n",
        "        )\n",
        "\n",
        "    return train_losses, val_losses, track_tokens_seen\n",
        "\n",
        "# Initial evaluation\n",
        "# model.to(device) # GRITAdapter handles moving the model to device\n",
        "torch.manual_seed(123)\n",
        "\n",
        "with torch.no_grad():\n",
        "    train_loss = calc_loss_loader(train_loader, grit_adapter, device, num_batches=5) # Use grit_adapter\n",
        "    val_loss = calc_loss_loader(val_loader, grit_adapter, device, num_batches=5)   # Use grit_adapter\n",
        "\n",
        "print(\"Initial training loss:\", train_loss)\n",
        "print(\"Initial validation loss:\", val_loss)\n",
        "\n",
        "# Train the model\n",
        "start_time = time.time()\n",
        "torch.manual_seed(123)\n",
        "\n",
        "# optimizer = torch.optim.AdamW(model.parameters(), lr=0.00005, weight_decay=0.1)\n",
        "# The optimizer for GRIT will be handled by the natural_gradient_step within grit_adapter.step()\n",
        "# If there were non-LoRA parameters to train with a standard optimizer, you would define it here for those.\n",
        "# For pure LoRA fine-tuning with GRIT, an external optimizer for LoRA params is not used in the same way.\n",
        "\n",
        "num_epochs = 1  # Increase this for better results\n",
        "\n",
        "train_losses, val_losses, tokens_seen = train_model_simple(\n",
        "    grit_adapter, train_loader, val_loader, None, device, # Pass grit_adapter, optimizer is None\n",
        "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
        "    start_context=format_input(val_data[0]), tokenizer=tokenizer,\n",
        "    kfac_update_freq=50 # Example: Update KFAC with True Fisher every 50 steps\n",
        ")\n",
        "\n",
        "end_time = time.time()\n",
        "execution_time_minutes = (end_time - start_time) / 60\n",
        "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")\n",
        "\n",
        "# Visualize training progress\n",
        "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
        "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
        "\n",
        "    # Plot training and validation loss against epochs\n",
        "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
        "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
        "    ax1.set_xlabel(\"Epochs\")\n",
        "    ax1.set_ylabel(\"Loss\")\n",
        "    ax1.legend(loc=\"upper right\")\n",
        "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n",
        "\n",
        "    # Create a second x-axis for tokens seen\n",
        "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
        "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
        "    ax2.set_xlabel(\"Tokens seen\")\n",
        "\n",
        "    fig.tight_layout()  # Adjust layout to make room\n",
        "    plt.savefig(\"loss-plot.pdf\")\n",
        "    plt.show()\n",
        "\n",
        "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
        "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-28T12:34:54.511025Z",
          "iopub.execute_input": "2025-05-28T12:34:54.511296Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Ghy42gR5wIr",
        "outputId": "cfde08b9-59dd-49ec-9adc-aa645f0249fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial training loss: 3.825909471511841\n",
            "Initial validation loss: 3.761934232711792\n",
            "Ep 1 (Step 000000): Train loss 3.865, Val loss 3.762\n",
            "Ep 1 (Step 000005): Train loss 3.822, Val loss 3.762\n",
            "Ep 1 (Step 000010): Train loss 3.754, Val loss 3.762\n",
            "Ep 1 (Step 000015): Train loss 3.795, Val loss 3.762\n",
            "Ep 1 (Step 000020): Train loss 3.758, Val loss 3.761\n",
            "Ep 1 (Step 000025): Train loss 3.733, Val loss 3.761\n",
            "Ep 1 (Step 000030): Train loss 3.867, Val loss 3.761\n",
            "Ep 1 (Step 000035): Train loss 3.740, Val loss 3.761\n",
            "Ep 1 (Step 000040): Train loss 3.838, Val loss 3.761\n",
            "Ep 1 (Step 000045): Train loss 3.787, Val loss 3.761\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 6: EXTRACTING AND SAVING RESPONSES\n"
      ],
      "metadata": {
        "id": "bI-00pTm5wIs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "# Generate responses for test examples\n",
        "for entry in test_data[:3]:\n",
        "    input_text = format_input(entry)\n",
        "    token_ids = generate(\n",
        "        model=model,\n",
        "        idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
        "        max_new_tokens=256,\n",
        "        context_size=BASE_CONFIG[\"context_length\"],\n",
        "        eos_id=50256\n",
        "    )\n",
        "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
        "    response_text = (\n",
        "        generated_text[len(input_text):]\n",
        "        .replace(\"### Response:\", \"\")\n",
        "        .strip()\n",
        "    )\n",
        "\n",
        "    print(\"\\nInput:\", input_text)\n",
        "    print(f\"\\nCorrect response: {entry['output']}\")\n",
        "    print(f\"\\nModel response: {response_text.strip()}\")\n",
        "    print(\"-------------------------------------\")\n",
        "\n",
        "# Process all test examples and save results\n",
        "for i, entry in tqdm(enumerate(test_data), total=len(test_data)):\n",
        "    input_text = format_input(entry)\n",
        "    token_ids = generate(\n",
        "        model=model,\n",
        "        idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
        "        max_new_tokens=256,\n",
        "        context_size=BASE_CONFIG[\"context_length\"],\n",
        "        eos_id=50256\n",
        "    )\n",
        "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
        "    response_text = generated_text[len(input_text):].replace(\"### Response:\", \"\").strip()\n",
        "    test_data[i][\"model_response\"] = response_text\n",
        "\n",
        "# Save results to file\n",
        "with open(\"instruction-data-with-response.json\", \"w\") as file:\n",
        "    json.dump(test_data, file, indent=4)  # \"indent\" for pretty-printing\n",
        "\n",
        "# Save the fine-tuned model\n",
        "file_name = f\"{re.sub(r'[ ()]', '', CHOOSE_MODEL)}-grit-sft.pth\"\n",
        "# torch.save(model.state_dict(), file_name) # Save original model state\n",
        "torch.save(grit_adapter.state_dict(), file_name) # Save only LoRA params via GRITAdapter\n",
        "print(f\"GRIT Adapter LoRA parameters saved as {file_name}\")\n",
        "\n",
        "# To load the model with GRIT LoRA weights:\n",
        "# 1. Load the base GPTModel: model = GPTModel(BASE_CONFIG); load_weights_into_gpt(model, params);\n",
        "# 2. Instantiate GRITAdapter: grit_adapter = GRITAdapter(model, layers_to_adapt_substrings=layers_to_adapt, ...)\n",
        "# 3. Load the LoRA weights: grit_adapter.load_state_dict(torch.load(file_name))\n",
        "# 4. grit_adapter.to(device)\n",
        "# 5. model.eval() (or grit_adapter.model.eval() / grit_adapter.eval())"
      ],
      "metadata": {
        "trusted": true,
        "id": "HybhJZQk5wIs"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "pdxvP-XN5wIt"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}