{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 1: Infrastructure Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-21T12:55:10.781078Z",
     "iopub.status.busy": "2025-05-21T12:55:10.780851Z",
     "iopub.status.idle": "2025-05-21T12:55:15.335844Z",
     "shell.execute_reply": "2025-05-21T12:55:15.335260Z",
     "shell.execute_reply.started": "2025-05-21T12:55:10.781061Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 1: Infrastructure Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T12:55:15.337492Z",
     "iopub.status.busy": "2025-05-21T12:55:15.337116Z",
     "iopub.status.idle": "2025-05-21T12:55:15.348020Z",
     "shell.execute_reply": "2025-05-21T12:55:15.347318Z",
     "shell.execute_reply.started": "2025-05-21T12:55:15.337467Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DampedKFAC(nn.Module):\n",
    "    # Assuming 'd,d' were placeholders for feature dimensions\n",
    "    def __init__(self, in_features: int, out_features: int, ema_decay: float = 0.95, damping: tuple[float, float] = (1e-7, 1e-5)):\n",
    "        super().__init__()\n",
    "        self.ema_decay = ema_decay\n",
    "        self.damping = damping # Tuple (damping_A, damping_B)\n",
    "        \n",
    "        # Using register_buffer for A_accum and B_accum\n",
    "        self.register_buffer('A_accum', torch.zeros(in_features, in_features))\n",
    "        self.register_buffer('B_accum', torch.zeros(out_features, out_features))\n",
    "        self.A_initialized = False\n",
    "        self.B_initialized = False\n",
    "\n",
    "    def update(self, current_A_cov: torch.Tensor, current_B_cov: torch.Tensor = None):\n",
    "        \"\"\"\n",
    "        Update A_accum or B_accum with new covariance.\n",
    "        If current_B_cov is None, only A_accum is updated.\n",
    "        If current_A_cov is None, only B_accum is updated.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            if current_A_cov is not None:\n",
    "                if not self.A_initialized:\n",
    "                    self.A_accum.copy_(current_A_cov)\n",
    "                    self.A_initialized = True\n",
    "                else:\n",
    "                    self.A_accum.mul_(self.ema_decay).add_(current_A_cov, alpha=(1 - self.ema_decay))\n",
    "            \n",
    "            if current_B_cov is not None:\n",
    "                if not self.B_initialized:\n",
    "                    self.B_accum.copy_(current_B_cov)\n",
    "                    self.B_initialized = True\n",
    "                else:\n",
    "                    self.B_accum.mul_(self.ema_decay).add_(current_B_cov, alpha=(1 - self.ema_decay))\n",
    "\n",
    "    def get_factors(self) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Returns the current A_accum and B_accum factors.\"\"\"\n",
    "        return self.A_accum, self.B_accum\n",
    "\n",
    "    def get_inverse_factors(self) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Computes and returns damped inverses (A_inv, B_inv).\"\"\"\n",
    "        with torch.no_grad():\n",
    "            device = self.A_accum.device\n",
    "            \n",
    "            A_damped = self.A_accum + self.damping[0] * torch.eye(self.A_accum.size(0), device=device)\n",
    "            B_damped = self.B_accum + self.damping[1] * torch.eye(self.B_accum.size(0), device=device)\n",
    "            \n",
    "            try:\n",
    "                A_inv = torch.linalg.pinv(A_damped)\n",
    "            except Exception as e:\n",
    "                # print(f\"Error inverting A_damped: {e}. Returning identity.\")\n",
    "                A_inv = torch.eye(self.A_accum.size(0), device=device)\n",
    "            \n",
    "            try:\n",
    "                B_inv = torch.linalg.pinv(B_damped)\n",
    "            except Exception as e:\n",
    "                # print(f\"Error inverting B_damped: {e}. Returning identity.\")\n",
    "                B_inv = torch.eye(self.B_accum.size(0), device=device)\n",
    "                \n",
    "            return A_inv, B_inv\n",
    "\n",
    "    def __repr__(self):\n",
    "        return (f\"DampedKFAC(in={self.A_accum.shape[0]}, out={self.B_accum.shape[0]}, \"\n",
    "                f\"ema_decay={self.ema_decay}, damping={self.damping}, \"\n",
    "                f\"A_init={self.A_initialized}, B_init={self.B_initialized})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T12:55:15.349626Z",
     "iopub.status.busy": "2025-05-21T12:55:15.348910Z",
     "iopub.status.idle": "2025-05-21T12:55:15.371809Z",
     "shell.execute_reply": "2025-05-21T12:55:15.371240Z",
     "shell.execute_reply.started": "2025-05-21T12:55:15.349599Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 1. Curvature Estimation Framework Implementation\n",
    "# 1.1 K-FAC Layer Specialization\n",
    "\n",
    "# Stage 1: Infrastructure Setup\n",
    "\n",
    "class KFACLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Handles K-FAC's activation and gradient covariance estimation for a given module.\n",
    "    Uses an instance of DampedKFAC to manage EMA updates and factor inversions.\n",
    "    \"\"\"\n",
    "    def __init__(self, module: nn.Linear, layer_type: str, ema_decay: float = 0.95, kfac_damping: tuple[float, float] = (1e-7, 1e-5)):\n",
    "        super().__init__()\n",
    "        self.layer_type = layer_type\n",
    "        # ema_decay and kfac_damping are now for the DampedKFAC instance\n",
    "        \n",
    "        try:\n",
    "            device = next(module.parameters()).device\n",
    "        except StopIteration: \n",
    "            device = torch.device(\"cpu\")\n",
    "\n",
    "        in_features = module.in_features\n",
    "        out_features = module.out_features\n",
    "\n",
    "        # Instantiate DampedKFAC\n",
    "        # Make sure DampedKFAC is defined before this class is used.\n",
    "        self.kfac_manager = DampedKFAC(in_features, out_features, ema_decay, kfac_damping)\n",
    "        self.kfac_manager.to(device)\n",
    "\n",
    "\n",
    "    def forward_hook(self, module: nn.Module, input_data: tuple[torch.Tensor], output_data: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Hook to compute activation covariance (A_factor) using E[aa^T] estimator.\n",
    "        input_data is a tuple, input_data[0] is the actual input tensor.\n",
    "        Uses DampedKFAC to update A_accum.\n",
    "        Incorporates suggested reshaping for 3D/4D inputs.\n",
    "        \"\"\"\n",
    "        x = input_data[0].detach()\n",
    "        \n",
    "        # Suggested reshaping\n",
    "        if x.ndim == 3:  # (batch_size, seq_len, dim)\n",
    "            act_reshaped = x.reshape(-1, x.shape[-1])  # (batch_size*seq_len, dim)\n",
    "        elif x.ndim == 4:  # Conv2d inputs (batch_size, channels, H, W)\n",
    "            # Reshape to (batch_size*H*W, channels) if module is Conv2d-like\n",
    "            # For nn.Linear, this path shouldn't be hit if input is already (B, Cin)\n",
    "            # This part needs care depending on whether KFACLayer is attached to Conv2d\n",
    "            # Original KFACLayer was for nn.Linear. If KFACLayer is generalized,\n",
    "            # input features for Conv2d (e.g. unfolded patches) would be different.\n",
    "            # For now, assuming if it's Linear, ndim won't be 4 unless an error.\n",
    "            # If it IS a Conv2D's activations flattened for a subsequent Linear, this is complex.\n",
    "            act_reshaped = x.reshape(-1, x.shape[1]) # (batch_size*H*W, channels) \n",
    "                                                   # This assumes x.shape[1] is 'in_features'\n",
    "        elif x.ndim > 2: # General case for >2D not explicitly handled by 3D/4D\n",
    "            act_reshaped = x.reshape(-1, x.shape[-1])\n",
    "        else: # ndim <= 2\n",
    "            act_reshaped = x \n",
    "        \n",
    "        if act_reshaped.shape[0] == 0: return\n",
    "        \n",
    "        current_A_factor_dim = self.kfac_manager.A_accum.shape[0]\n",
    "        if act_reshaped.shape[1] != current_A_factor_dim:\n",
    "            print(f\"Warning: Activation dimension mismatch for KFACLayer {self.layer_type}. Expected {current_A_factor_dim}, got {act_reshaped.shape[1]}. Skipping A_factor update.\")\n",
    "            return\n",
    "\n",
    "        # Covariance of activations: E[aa^T] estimated by (act.T @ act) / N\n",
    "        # Snippet: cov = (x.T @ x) / x.size(0)\n",
    "        # Using act_reshaped for x\n",
    "        cov_a = (act_reshaped.T @ act_reshaped) / act_reshaped.shape[0]\n",
    "        \n",
    "        # Update using DampedKFAC. Current_B_cov is None as this is forward pass.\n",
    "        self.kfac_manager.update(current_A_cov=cov_a, current_B_cov=None)\n",
    "\n",
    "    def backward_hook(self, module: nn.Module, grad_input: tuple[torch.Tensor], grad_output_data: tuple[torch.Tensor]):\n",
    "        \"\"\"\n",
    "        Hook to compute gradient covariance (B_factor) using E[gg^T] estimator.\n",
    "        grad_output_data is a tuple, grad_output_data[0] is the gradient w.r.t. module's output.\n",
    "        Uses DampedKFAC to update B_accum.\n",
    "        \"\"\"\n",
    "        g = grad_output_data[0].detach()\n",
    "        \n",
    "        # Reshaping for gradients (similar to activations)\n",
    "        if g.ndim > 2:\n",
    "            grad_reshaped = g.reshape(-1, g.shape[-1])\n",
    "        else:\n",
    "            grad_reshaped = g\n",
    "\n",
    "        if grad_reshaped.shape[0] == 0: return\n",
    "        \n",
    "        current_B_factor_dim = self.kfac_manager.B_accum.shape[0]\n",
    "        if grad_reshaped.shape[1] != current_B_factor_dim:\n",
    "            print(f\"Warning: Gradient dimension mismatch for KFACLayer {self.layer_type}. Expected {current_B_factor_dim}, got {grad_reshaped.shape[1]}. Skipping B_factor update.\")\n",
    "            return\n",
    "\n",
    "        # Covariance of gradients: E[gg^T] estimated by (grad.T @ grad) / N\n",
    "        cov_g = (grad_reshaped.T @ grad_reshaped) / grad_reshaped.shape[0]\n",
    "\n",
    "        # Update using DampedKFAC. Current_A_cov is None as this is backward pass.\n",
    "        self.kfac_manager.update(current_A_cov=None, current_B_cov=cov_g)\n",
    "\n",
    "    # Expose factors for external use if needed (e.g. by FisherEigen or other parts)\n",
    "    @property\n",
    "    def A_factor(self):\n",
    "        return self.kfac_manager.A_accum\n",
    "\n",
    "    @property\n",
    "    def B_factor(self):\n",
    "        return self.kfac_manager.B_accum\n",
    "\n",
    "    def get_inverse_factors(self):\n",
    "        return self.kfac_manager.get_inverse_factors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T12:55:15.373472Z",
     "iopub.status.busy": "2025-05-21T12:55:15.373271Z",
     "iopub.status.idle": "2025-05-21T12:55:15.390623Z",
     "shell.execute_reply": "2025-05-21T12:55:15.389921Z",
     "shell.execute_reply.started": "2025-05-21T12:55:15.373456Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 1.2 Transformer Layer Instrumentation\n",
    "\n",
    "def instrument_layer(transformer_block: nn.Module) -> list[KFACLayer]:\n",
    "    \"\"\"\n",
    "    Instruments specific layers in a transformer_block with KFACLayer handlers.\n",
    "    Assumes transformer_block has 'attn.q_proj', 'attn.k_proj', 'attn.v_proj', and 'mlp' attributes\n",
    "    which are nn.Linear layers.\n",
    "    Returns a list of created KFACLayer handlers.\n",
    "    \"\"\"\n",
    "    kfac_handlers = []\n",
    "    \n",
    "    # Helper to instrument a single linear layer\n",
    "    def _instrument_linear(module: nn.Linear, name: str, handlers_list: list):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            if not hasattr(module, 'kfac_handler'): # Avoid double instrumentation\n",
    "                # print(f\"Instrumenting layer {name} for KFAC.\")\n",
    "                # Pass ema_decay and potentially kfac_damping if KFACLayer expects it\n",
    "                # For this example, assume KFACLayer default ema_decay (0.95) is fine,\n",
    "                # and default kfac_damping in KFACLayer is also fine.\n",
    "                # If specific values are needed here, they should be passed.\n",
    "                kfac_layer_instance = KFACLayer(module, name) # Uses KFACLayer defaults\n",
    "                module.register_forward_hook(kfac_layer_instance.forward_hook)\n",
    "                module.register_full_backward_hook(kfac_layer_instance.backward_hook)\n",
    "                module.kfac_handler = kfac_layer_instance \n",
    "                handlers_list.append(kfac_layer_instance)\n",
    "        else:\n",
    "            print(f\"Warning: Expected nn.Linear for {name}, got {type(module)}. Skipping KFAC instrumentation.\")\n",
    "\n",
    "    # Self-attention projections\n",
    "    if hasattr(transformer_block, 'attn'):\n",
    "        attn_module = transformer_block.attn\n",
    "        if hasattr(attn_module, 'q_proj'):\n",
    "            _instrument_linear(attn_module.q_proj, f\"{transformer_block.__class__.__name__}.attn.q_proj\", kfac_handlers)\n",
    "        if hasattr(attn_module, 'k_proj'):\n",
    "            _instrument_linear(attn_module.k_proj, f\"{transformer_block.__class__.__name__}.attn.k_proj\", kfac_handlers)\n",
    "        if hasattr(attn_module, 'v_proj'):\n",
    "            _instrument_linear(attn_module.v_proj, f\"{transformer_block.__class__.__name__}.attn.v_proj\", kfac_handlers)\n",
    "        # Optional: Output projection (often included in KFAC for attention)\n",
    "        # if hasattr(attn_module, 'o_proj'):\n",
    "        #     _instrument_linear(attn_module.o_proj, f\"{transformer_block.__class__.__name__}.attn.o_proj\", kfac_handlers)\n",
    "    else:\n",
    "        print(f\"Warning: Transformer block {transformer_block.__class__.__name__} has no 'attn' attribute.\")\n",
    "        \n",
    "    # FFN matrix (assuming transformer_block.mlp is an nn.Linear layer)\n",
    "    if hasattr(transformer_block, 'mlp'):\n",
    "        _instrument_linear(transformer_block.mlp, f\"{transformer_block.__class__.__name__}.mlp\", kfac_handlers)\n",
    "    else:\n",
    "         print(f\"Warning: Transformer block {transformer_block.__class__.__name__} has no 'mlp' attribute for FFN KFAC.\")\n",
    "         \n",
    "    return kfac_handlers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Low-Rank Adaptation Core\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T12:55:15.391505Z",
     "iopub.status.busy": "2025-05-21T12:55:15.391292Z",
     "iopub.status.idle": "2025-05-21T12:55:15.405324Z",
     "shell.execute_reply": "2025-05-21T12:55:15.404742Z",
     "shell.execute_reply.started": "2025-05-21T12:55:15.391489Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 2.1 Rank-Flexible LoRA Module \n",
    "\n",
    "class FlexLoRA(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements Low-Rank Adaptation (LoRA) for an nn.Linear layer.\n",
    "    The base layer's weights are frozen, and LoRA matrices (A and B) are trained.\n",
    "    Includes attributes for dynamic rank adjustment.\n",
    "    \"\"\"\n",
    "    def __init__(self, base_layer: nn.Linear, initial_rank: int = 8, min_rank: int = 4, max_rank: int = 16):\n",
    "        super().__init__()\n",
    "        if not isinstance(base_layer, nn.Linear):\n",
    "            raise ValueError(f\"FlexLoRA currently only supports nn.Linear, got {type(base_layer)}\")\n",
    "\n",
    "        self.base_layer = base_layer\n",
    "        self.initial_rank = initial_rank\n",
    "        self.current_rank = initial_rank # Initialize current_rank with initial_rank\n",
    "        self.min_rank = min_rank\n",
    "        self.max_rank = max_rank\n",
    "        \n",
    "        self.base_layer.weight.requires_grad = False\n",
    "        if self.base_layer.bias is not None:\n",
    "            self.base_layer.bias.requires_grad = False\n",
    "\n",
    "        device = base_layer.weight.device\n",
    "        # LoRA parameters dimensions are based on current_rank (which is initial_rank at creation)\n",
    "        self.lora_A = nn.Parameter(torch.randn(base_layer.in_features, self.current_rank, device=device))\n",
    "        self.lora_B = nn.Parameter(torch.zeros(self.current_rank, base_layer.out_features, device=device))\n",
    "        \n",
    "        self.bypass = nn.Identity()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        base_out = self.base_layer(x)\n",
    "        \n",
    "        if self.current_rank > 0: # Check current_rank for LoRA path\n",
    "            lora_adapt = (x @ self.lora_A) @ self.lora_B\n",
    "            return self.bypass(base_out) + lora_adapt\n",
    "        return base_out\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return (f\"{self.__class__.__name__}(initial_rank={self.initial_rank}, current_rank={self.current_rank}, \"\n",
    "                f\"min_rank={self.min_rank}, max_rank={self.max_rank}, \"\n",
    "                f\"base_layer={self.base_layer.__class__.__name__}({self.base_layer.in_features}x{self.base_layer.out_features}))\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T12:55:15.406249Z",
     "iopub.status.busy": "2025-05-21T12:55:15.406007Z",
     "iopub.status.idle": "2025-05-21T12:55:15.420156Z",
     "shell.execute_reply": "2025-05-21T12:55:15.419488Z",
     "shell.execute_reply.started": "2025-05-21T12:55:15.406229Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 2.2 Layer Modification Protocol\n",
    "\n",
    "def lora_inject(model: nn.Module, layers_to_adapt_substrings: list[str], \n",
    "                rank_range: tuple[int, int] = (8, 64)) -> nn.Module:\n",
    "    \"\"\"\n",
    "    Replaces specified nn.Linear layers in the model with FlexLoRA wrappers.\n",
    "    Args:\n",
    "        model: The model to modify.\n",
    "        layers_to_adapt_substrings: List of substrings. If a module's name contains\n",
    "                                    any of these, it will be adapted.\n",
    "        rank_range: Tuple of (min_rank, max_rank). min_rank is used for initial_rank.\n",
    "    Returns:\n",
    "        The modified model.\n",
    "    \"\"\"\n",
    "    min_r, max_r = rank_range\n",
    "    # Use the lower bound of the rank range as the initial rank for LoRA\n",
    "    # and pass min_r, max_r to FlexLoRA for its own min_rank, max_rank settings.\n",
    "    initial_r = min_r \n",
    "\n",
    "    for name, module in model.named_modules():\n",
    "        parent_name_parts = name.split('.')[:-1]\n",
    "        child_name = name.split('.')[-1]\n",
    "        \n",
    "        parent_module = model\n",
    "        if parent_name_parts:\n",
    "            try:\n",
    "                parent_module = model.get_submodule(\".\".join(parent_name_parts))\n",
    "            except AttributeError:\n",
    "                continue\n",
    "\n",
    "        actual_module_to_check = getattr(parent_module, child_name, None)\n",
    "\n",
    "        if isinstance(actual_module_to_check, nn.Linear):\n",
    "            if any(sub in name for sub in layers_to_adapt_substrings):\n",
    "                if isinstance(actual_module_to_check, FlexLoRA):\n",
    "                    continue\n",
    "\n",
    "                original_layer = actual_module_to_check\n",
    "                \n",
    "                lora_layer = FlexLoRA(original_layer, \n",
    "                                        initial_rank=initial_r, \n",
    "                                        min_rank=min_r, \n",
    "                                        max_rank=max_r)\n",
    "                setattr(parent_module, child_name, lora_layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Neural Reprojection Engine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T12:55:15.421158Z",
     "iopub.status.busy": "2025-05-21T12:55:15.420919Z",
     "iopub.status.idle": "2025-05-21T12:55:15.438690Z",
     "shell.execute_reply": "2025-05-21T12:55:15.438096Z",
     "shell.execute_reply.started": "2025-05-21T12:55:15.421137Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 3.1 Eigen Decomposition Module\n",
    "\n",
    "class FisherEigen(nn.Module):\n",
    "    \"\"\"\n",
    "    Handles Fisher block construction (via Kronecker product of KFAC's A and B factors)\n",
    "    and its eigendecomposition using LOBPCG.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # The Fisher block buffer will be created dynamically in update_fisher\n",
    "        self.register_buffer('fisher_block', torch.empty(0), persistent=False)\n",
    "\n",
    "    def update_fisher(self, A: torch.Tensor, B: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Updates the Fisher block using A and B factors.\n",
    "        Fisher ~ kron(B, A)\n",
    "        Assumes A and B are symmetric, or uses (X + X.T)/2 for safety.\n",
    "        \"\"\"\n",
    "        # Ensure factors are symmetric (KFAC factors should be, but enforce for safety)\n",
    "        A_sym = (A + A.T) / 2 if A.numel() > 0 else A\n",
    "        B_sym = (B + B.T) / 2 if B.numel() > 0 else B\n",
    "        \n",
    "        if A_sym.numel() == 0 or B_sym.numel() == 0:\n",
    "            # print(\"Warning: A or B factor is empty. Fisher block will be empty.\")\n",
    "            self.fisher_block = torch.empty(0, device=A.device if A.numel() > 0 else (B.device if B.numel() > 0 else 'cpu'))\n",
    "            return\n",
    "\n",
    "        # Explicit Kronecker product. This can be very memory intensive.\n",
    "        self.fisher_block = torch.kron(B_sym, A_sym)\n",
    "\n",
    "    def decompose(self, k: float = 0.2) -> tuple[torch.Tensor | None, torch.Tensor | None]:\n",
    "        \"\"\"\n",
    "        Decomposes the computed Fisher block to find top-k eigenvectors using LOBPCG.\n",
    "        Args:\n",
    "            k: Fraction of total eigenvectors to return (corresponding to largest eigenvalues).\n",
    "        Returns:\n",
    "            Tuple of (eigenvalues, eigenvectors). Eigenvectors are column vectors.\n",
    "            Returns (None, None) if decomposition is not possible or fails.\n",
    "        \"\"\"\n",
    "        if self.fisher_block is None or self.fisher_block.numel() == 0:\n",
    "            # print(\"Fisher block not computed or is empty. Cannot decompose.\")\n",
    "            return None, None\n",
    "\n",
    "        dim = self.fisher_block.shape[0]\n",
    "        if dim == 0:\n",
    "            return None, None\n",
    "\n",
    "        # k for lobpcg is the number of eigenpairs to find\n",
    "        num_eigenvectors_to_find = int(dim * k)\n",
    "        # Ensure k_lobpcg is valid: 0 < k_lobpcg < dim for non-trivial cases, k_lobpcg <= dim\n",
    "        num_eigenvectors_to_find = max(1, min(num_eigenvectors_to_find, dim -1 if dim > 1 else 1))\n",
    "        if dim == 1 and num_eigenvectors_to_find > 1 : num_eigenvectors_to_find = 1 # Max 1 for 1x1 matrix\n",
    "\n",
    "        if num_eigenvectors_to_find <= 0:\n",
    "             return None, None\n",
    "        \n",
    "        try:\n",
    "            # LOBPCG for largest eigenvalues.\n",
    "            # It requires the matrix to be symmetric (which self.fisher_block should be by construction).\n",
    "            eigenvalues, eigenvectors = torch.lobpcg(\n",
    "                A=self.fisher_block, # The matrix A for which to solve A*X = lambda*X*B (B is identity here)\n",
    "                k=num_eigenvectors_to_find,\n",
    "                largest=True, # Get largest eigenvalues\n",
    "                method=\"ortho\" # Ensures eigenvectors are orthogonalized\n",
    "            )\n",
    "            # lobpcg returns eigenvalues in ascending order if largest=False, \n",
    "            # and typically in no specific order or descending for largest=True.\n",
    "            # We want largest, so let's sort to be sure.\n",
    "            sorted_indices = torch.argsort(eigenvalues, descending=True)\n",
    "            eigenvalues = eigenvalues[sorted_indices]\n",
    "            eigenvectors = eigenvectors[:, sorted_indices]\n",
    "            \n",
    "            return eigenvalues, eigenvectors\n",
    "\n",
    "        except Exception as e:\n",
    "            # Fallback to eigh if LOBPCG fails (e.g. due to matrix properties or k)\n",
    "            # print(f\"LOBPCG decomposition failed for a {dim}x{dim} matrix (k={num_eigenvectors_to_find}): {e}. Trying eigh.\")\n",
    "            try:\n",
    "                eigenvalues_all, eigenvectors_all = torch.linalg.eigh(self.fisher_block)\n",
    "                top_eigenvalues = eigenvalues_all[-num_eigenvectors_to_find:]\n",
    "                top_eigenvectors = eigenvectors_all[:, -num_eigenvectors_to_find:]\n",
    "                # Return in descending order of eigenvalue magnitude\n",
    "                return top_eigenvalues.flip(dims=[0]), top_eigenvectors.fliplr()\n",
    "            except Exception as e_eigh:\n",
    "                print(f\"LOBPCG and eigh decomposition failed for a {dim}x{dim} matrix (k={num_eigenvectors_to_find}): {e_eigh}\")\n",
    "                return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T12:55:15.439674Z",
     "iopub.status.busy": "2025-05-21T12:55:15.439382Z",
     "iopub.status.idle": "2025-05-21T12:55:15.460600Z",
     "shell.execute_reply": "2025-05-21T12:55:15.459935Z",
     "shell.execute_reply.started": "2025-05-21T12:55:15.439624Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 3.2 Subspace Preservation System\n",
    "\n",
    "class SubspaceBuffer(nn.Module):\n",
    "    \"\"\"\n",
    "    Stores and manages projection masks and eigenvectors for multiple blocks/layers.\n",
    "    Uses a fixed buffer_size for eigenvector storage dimensions.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_blocks: int, buffer_size: int = 1024, device: str ='cpu'):\n",
    "        super().__init__()\n",
    "        self.num_blocks = num_blocks\n",
    "        self.buffer_size = buffer_size # Acts as max dimension and max number of vectors\n",
    "        self.device = torch.device(device)\n",
    "\n",
    "        self.subspace_module_list = nn.ModuleList() # MODIFIED HERE\n",
    "        for _ in range(num_blocks):\n",
    "            # Each buffer entry is a BufferDict\n",
    "            buffer_entry = nn.Module() # Using nn.Module to host buffers for easy device moving\n",
    "            buffer_entry.register_buffer(\n",
    "                'eigen_vectors', \n",
    "                torch.zeros(buffer_size, buffer_size, device=self.device) # (max_feature_dim, max_num_eigenvectors)\n",
    "            )\n",
    "            buffer_entry.register_buffer(\n",
    "                'projection_mask', # Indicates how many of the 'buffer_size' eigenvector slots are active\n",
    "                torch.zeros(buffer_size, device=self.device, dtype=torch.float32) # Using float for 0/1 values\n",
    "            )\n",
    "            self.subspace_module_list.append(buffer_entry) # MODIFIED HERE\n",
    "        \n",
    "    def update_buffer(self, block_idx: int, new_eigenvectors: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Updates the buffer for a specific block with new eigenvectors.\n",
    "        Args:\n",
    "            block_idx: Index of the layer/block.\n",
    "            new_eigenvectors: Tensor of shape (full_vector_dim, num_new_vecs).\n",
    "                              Assumes full_vector_dim <= buffer_size and num_new_vecs <= buffer_size.\n",
    "        \"\"\"\n",
    "        if not (0 <= block_idx < self.num_blocks):\n",
    "            raise IndexError(f\"block_idx {block_idx} out of range for {self.num_blocks} buffers.\")\n",
    "\n",
    "        current_buffer_entry = self.subspace_module_list[block_idx] # MODIFIED HERE\n",
    "        \n",
    "        if new_eigenvectors is None or new_eigenvectors.numel() == 0:\n",
    "            # print(f\"Warning: No new eigenvectors provided for block {block_idx}. Buffer not updated.\")\n",
    "            current_buffer_entry.projection_mask.fill_(0.) # Clear mask if no new vectors\n",
    "            return\n",
    "\n",
    "        actual_feature_dim, num_new_vecs = new_eigenvectors.shape\n",
    "\n",
    "        if actual_feature_dim > self.buffer_size:\n",
    "            # print(f\"Warning: Eigenvector feature dimension ({actual_feature_dim}) for block {block_idx} \"\n",
    "            #       f\"exceeds buffer_size ({self.buffer_size}). Truncating features.\")\n",
    "            new_eigenvectors = new_eigenvectors[:self.buffer_size, :]\n",
    "            actual_feature_dim = self.buffer_size\n",
    "        \n",
    "        slots_to_fill = min(num_new_vecs, self.buffer_size)\n",
    "        \n",
    "        # Clear old values and set new ones\n",
    "        current_buffer_entry.eigen_vectors.fill_(0.)\n",
    "        current_buffer_entry.projection_mask.fill_(0.)\n",
    "\n",
    "        if slots_to_fill > 0:\n",
    "            current_buffer_entry.eigen_vectors[:actual_feature_dim, :slots_to_fill] = new_eigenvectors[:, :slots_to_fill]\n",
    "            current_buffer_entry.projection_mask[:slots_to_fill] = 1.0 # Mark as active\n",
    "\n",
    "    def get_subspace(self, block_idx: int) -> torch.Tensor | None:\n",
    "        \"\"\"\n",
    "        Retrieves the active eigenvectors for a given block.\n",
    "        Returns:\n",
    "            Tensor of shape (feature_dim_of_stored_vecs, num_active_vecs), or None.\n",
    "            Feature_dim_of_stored_vecs could be up to buffer_size.\n",
    "        \"\"\"\n",
    "        if not (0 <= block_idx < self.num_blocks):\n",
    "            raise IndexError(f\"block_idx {block_idx} out of range for {self.num_blocks} buffers.\")\n",
    "        \n",
    "        current_buffer_entry = self.subspace_module_list[block_idx] # MODIFIED HERE\n",
    "        active_indices = current_buffer_entry.projection_mask.bool() # Convert 0/1 to boolean mask\n",
    "        \n",
    "        if not active_indices.any():\n",
    "            return None # No active eigenvectors\n",
    "\n",
    "        # Retrieve only up to the point where features might have been stored.\n",
    "        # This assumes eigenvectors were stored contiguously from feature dim 0.\n",
    "        # A more robust way would be to also store the actual_feature_dim, but this matches the buffer_size logic.\n",
    "        active_vectors = current_buffer_entry.eigen_vectors[:, active_indices]\n",
    "        return active_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T12:55:15.461628Z",
     "iopub.status.busy": "2025-05-21T12:55:15.461413Z",
     "iopub.status.idle": "2025-05-21T12:55:15.482441Z",
     "shell.execute_reply": "2025-05-21T12:55:15.481850Z",
     "shell.execute_reply.started": "2025-05-21T12:55:15.461606Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- Dummy Model for Illustration (adapted for instrument_layer) ---\n",
    "class AttentionModule(nn.Module): # Helper module for attention projections\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.q_proj = nn.Linear(dim, dim)\n",
    "        self.k_proj = nn.Linear(dim, dim)\n",
    "        self.v_proj = nn.Linear(dim, dim)\n",
    "        self.o_proj = nn.Linear(dim, dim) # Output projection\n",
    "\n",
    "    def forward(self, q, k, v): # Simplified forward\n",
    "        return self.o_proj(self.q_proj(q) + self.k_proj(k) + self.v_proj(v))\n",
    "\n",
    "\n",
    "class SimpleTransformerBlock(nn.Module):\n",
    "    def __init__(self, dim, n_heads, mlp_ratio=4): # n_heads not used in this simplified version\n",
    "        super().__init__()\n",
    "        self.attn = AttentionModule(dim) # Contains q_proj, k_proj, v_proj\n",
    "        \n",
    "        # FFN layers\n",
    "        self.mlp_fc1 = nn.Linear(dim, dim * mlp_ratio)\n",
    "        self.mlp_gelu = nn.GELU()\n",
    "        self.mlp_fc2 = nn.Linear(dim * mlp_ratio, dim)\n",
    "        \n",
    "        # Expose one FFN layer as 'self.mlp' for instrument_layer as per its current design\n",
    "        # This assumes KFAC is applied to the first linear layer of the MLP.\n",
    "        self.mlp = self.mlp_fc1 \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Simplified forward pass\n",
    "        # Dummy attention: uses x for q, k, v.\n",
    "        attn_out = self.attn(x, x, x) \n",
    "        \n",
    "        # MLP path\n",
    "        mlp_hidden = self.mlp_gelu(self.mlp(attn_out)) # self.mlp is mlp_fc1\n",
    "        mlp_out = self.mlp_fc2(mlp_hidden)\n",
    "        return attn_out + mlp_out # Residual connection\n",
    "\n",
    "class DummyModel(nn.Module):\n",
    "    def __init__(self, num_blocks=2, dim=64, n_heads=4):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList([SimpleTransformerBlock(dim, n_heads) for _ in range(num_blocks)])\n",
    "        self.out_head = nn.Linear(dim, 10) # Example output\n",
    "\n",
    "    def forward(self, x):\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return self.out_head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T12:55:15.485315Z",
     "iopub.status.busy": "2025-05-21T12:55:15.485111Z",
     "iopub.status.idle": "2025-05-21T12:55:18.387965Z",
     "shell.execute_reply": "2025-05-21T12:55:18.387194Z",
     "shell.execute_reply.started": "2025-05-21T12:55:15.485293Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n--- KFAC Example ---\n",
      "Instrumenting KFAC for block 0...\n",
      "Instrumented 4 layers in total for KFAC.\n",
      "KFAC A_factor for 'SimpleTransformerBlock.attn.q_proj' (sample):\\n tensor([[ 0.4425, -0.1333,  0.1536, -0.0331,  0.4627],\n",
      "        [-0.1333,  1.3624, -0.3087, -0.0966, -0.4073],\n",
      "        [ 0.1536, -0.3087,  1.2780, -0.1461, -0.0248],\n",
      "        [-0.0331, -0.0966, -0.1461,  0.6619, -0.2659],\n",
      "        [ 0.4627, -0.4073, -0.0248, -0.2659,  1.8390]])\n",
      "KFAC B_factor for 'SimpleTransformerBlock.attn.q_proj' (sample):\\n tensor([[ 4.8930e-07,  5.4784e-07, -1.1847e-06,  1.3906e-06, -4.3782e-07],\n",
      "        [ 5.4784e-07,  6.2973e-07, -1.3418e-06,  1.5643e-06, -4.9425e-07],\n",
      "        [-1.1847e-06, -1.3418e-06,  2.9071e-06, -3.3941e-06,  1.0758e-06],\n",
      "        [ 1.3906e-06,  1.5643e-06, -3.3941e-06,  3.9753e-06, -1.2569e-06],\n",
      "        [-4.3782e-07, -4.9425e-07,  1.0758e-06, -1.2569e-06,  4.0789e-07]])\n"
     ]
    }
   ],
   "source": [
    "# --- KFAC Example (using new KFACLayer and instrument_layer) ---\n",
    "print(\"\\\\n--- KFAC Example ---\")\n",
    "model_kfac = DummyModel(num_blocks=1, dim=32) # Using 1 block for simplicity\n",
    "\n",
    "all_kfac_handlers = []\n",
    "for i, block in enumerate(model_kfac.blocks):\n",
    "    print(f\"Instrumenting KFAC for block {i}...\")\n",
    "    block_kfac_handlers = instrument_layer(block) # instrument_layer targets one block\n",
    "    all_kfac_handlers.extend(block_kfac_handlers)\n",
    "\n",
    "print(f\"Instrumented {len(all_kfac_handlers)} layers in total for KFAC.\")\n",
    "\n",
    "# Simulate a forward/backward pass\n",
    "dummy_data = torch.randn(2, 5, 32) # Batch, Seq, Dim\n",
    "optimizer_kfac = torch.optim.Adam(model_kfac.parameters(), lr=1e-3)\n",
    "\n",
    "# First pass (initializes factors if using mean_vector outer product)\n",
    "output_kfac = model_kfac(dummy_data)\n",
    "loss_kfac = output_kfac.mean()\n",
    "loss_kfac.backward()\n",
    "optimizer_kfac.step()\n",
    "optimizer_kfac.zero_grad()\n",
    "\n",
    "# Second pass to see updated factors\n",
    "output_kfac_2 = model_kfac(dummy_data)\n",
    "loss_kfac_2 = output_kfac_2.mean()\n",
    "loss_kfac_2.backward()\n",
    "optimizer_kfac.step()\n",
    "\n",
    "\n",
    "if all_kfac_handlers:\n",
    "    first_handler = all_kfac_handlers[0]\n",
    "    print(f\"KFAC A_factor for '{first_handler.layer_type}' (sample):\\\\n\", first_handler.A_factor[:min(5,first_handler.A_factor.shape[0]),:min(5,first_handler.A_factor.shape[1])])\n",
    "    print(f\"KFAC B_factor for '{first_handler.layer_type}' (sample):\\\\n\", first_handler.B_factor[:min(5,first_handler.B_factor.shape[0]),:min(5,first_handler.B_factor.shape[1])])\n",
    "else:\n",
    "    print(\"No KFAC handlers created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T12:55:18.389242Z",
     "iopub.status.busy": "2025-05-21T12:55:18.388901Z",
     "iopub.status.idle": "2025-05-21T12:55:18.404449Z",
     "shell.execute_reply": "2025-05-21T12:55:18.403761Z",
     "shell.execute_reply.started": "2025-05-21T12:55:18.389213Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n--- LoRA Example ---\n",
      "Model with LoRA injected:\n",
      "Trainable params with LoRA: 3434, Total params: 14954\n",
      "LoRA model training step completed.\n"
     ]
    }
   ],
   "source": [
    "# --- LoRA Example (using new FlexLoRA and lora_inject) ---\n",
    "print(\"\\\\n--- LoRA Example ---\")\n",
    "model_lora = DummyModel(num_blocks=1, dim=32)\n",
    "# Adapt layers containing 'attn' or 'mlp' in their name.\n",
    "# Note: 'mlp' in SimpleTransformerBlock is self.mlp_fc1. If we want to adapt both mlp_fc1 and mlp_fc2,\n",
    "# the substring matching needs to be more specific or lora_inject needs to traverse deeper.\n",
    "# For this example, 'blocks.0.mlp' will match the first FFN layer if 'mlp' is in substrings.\n",
    "# 'blocks.0.attn.q_proj' will match if 'attn' or 'q_proj' are in substrings.\n",
    "lora_inject(model_lora, layers_to_adapt_substrings=['attn.q_proj', 'attn.k_proj', 'attn.v_proj', 'mlp'], rank_range=(4, 16))\n",
    "print(\"Model with LoRA injected:\")\n",
    "# print(model_lora) # To see FlexLoRA modules, can be verbose\n",
    "\n",
    "trainable_params = sum(p.numel() for p in model_lora.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model_lora.parameters())\n",
    "print(f\"Trainable params with LoRA: {trainable_params}, Total params: {total_params}\")\n",
    "\n",
    "output_lora = model_lora(dummy_data) # dummy_data from KFAC example\n",
    "loss_lora = output_lora.mean()\n",
    "optimizer_lora = torch.optim.Adam(filter(lambda p: p.requires_grad, model_lora.parameters()), lr=1e-3)\n",
    "optimizer_lora.zero_grad()\n",
    "loss_lora.backward()\n",
    "optimizer_lora.step()\n",
    "print(\"LoRA model training step completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T12:55:18.405852Z",
     "iopub.status.busy": "2025-05-21T12:55:18.405260Z",
     "iopub.status.idle": "2025-05-21T12:55:18.543352Z",
     "shell.execute_reply": "2025-05-21T12:55:18.542537Z",
     "shell.execute_reply.started": "2025-05-21T12:55:18.405827Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n--- FisherEigen & SubspaceBuffer Example ---\n",
      "Fisher block computed, shape: torch.Size([1024, 1024])\n",
      "Decomposed Fisher: Got 102 eigenvectors with shape torch.Size([1024, 102])\n",
      "Top eigenvalues (sample):\\n tensor([0.0005, 0.0004, 0.0004, 0.0003, 0.0003])\n",
      "Retrieved 102 vectors (shape torch.Size([1024, 102])) from SubspaceBuffer for block 0.\n",
      "\\n--- Infrastructure Script End ---\n"
     ]
    }
   ],
   "source": [
    "# --- FisherEigen & SubspaceBuffer Example ---\n",
    "print(\"\\\\n--- FisherEigen & SubspaceBuffer Example ---\")\n",
    "if all_kfac_handlers: # Requires KFAC to have run and populated factors\n",
    "    # Example: use the first KFAC handler's factors\n",
    "    kfac_A = all_kfac_handlers[0].A_factor\n",
    "    kfac_B = all_kfac_handlers[0].B_factor\n",
    "    \n",
    "    if torch.all(kfac_A == 0) or torch.all(kfac_B == 0):\n",
    "        print(\"Warning: KFAC factors from the first handler are zero. Synthesizing dummy factors for FisherEigen test.\")\n",
    "        # Synthesize dummy factors if they are zero (e.g., if only one pass done or specific KFAC math leads to it)\n",
    "        dim_A = kfac_A.shape[0] if kfac_A.numel() > 0 else 32\n",
    "        dim_B = kfac_B.shape[0] if kfac_B.numel() > 0 else 32\n",
    "        kfac_A = torch.eye(dim_A, device=kfac_A.device) * 0.1 + torch.rand(dim_A,dim_A, device=kfac_A.device)*0.01\n",
    "        kfac_B = torch.eye(dim_B, device=kfac_B.device) * 0.1 + torch.rand(dim_B,dim_B, device=kfac_B.device)*0.01\n",
    "        kfac_A = (kfac_A + kfac_A.T)/2 # ensure symmetric\n",
    "        kfac_B = (kfac_B + kfac_B.T)/2\n",
    "\n",
    "\n",
    "    fisher_engine = FisherEigen()\n",
    "    fisher_engine.update_fisher(A=kfac_A, B=kfac_B)\n",
    "    print(f\"Fisher block computed, shape: {fisher_engine.fisher_block.shape if fisher_engine.fisher_block is not None else 'None'}\")\n",
    "\n",
    "    if fisher_engine.fisher_block is not None and fisher_engine.fisher_block.numel() > 0:\n",
    "        eig_vals, eig_vecs = fisher_engine.decompose(k=0.1) # Request top 10%\n",
    "        \n",
    "        if eig_vecs is not None:\n",
    "            print(f\"Decomposed Fisher: Got {eig_vecs.shape[1]} eigenvectors with shape {eig_vecs.shape}\")\n",
    "            print(\"Top eigenvalues (sample):\\\\n\", eig_vals[:min(5, len(eig_vals) if eig_vals is not None else 0)])\n",
    "\n",
    "            # Subspace Buffer example\n",
    "            # Suppose we track subspaces for a few \"blocks\" (e.g. layers from KFAC)\n",
    "            num_tracked_fisher_blocks = 1 # For this example, just one from the first KFAC handler\n",
    "            subspace_buffer_size = 1024 # Max dim / max num vectors for this buffer example\n",
    "            \n",
    "            # Ensure eigenvector dimensions match subspace buffer expectations\n",
    "            # Fisher block dim = dim_A * dim_B. Eigenvector dim is also this.\n",
    "            # For SubspaceBuffer, this full_vector_dim must be <= buffer_size.\n",
    "            \n",
    "            current_fisher_block_dim = fisher_engine.fisher_block.shape[0]\n",
    "            if current_fisher_block_dim > subspace_buffer_size:\n",
    "                 print(f\"Warning: Fisher block dim ({current_fisher_block_dim}) > SubspaceBuffer buffer_size ({subspace_buffer_size}). \"\n",
    "                       \"Eigenvectors will be truncated by SubspaceBuffer.\")\n",
    "\n",
    "            subspace_manager = SubspaceBuffer(num_blocks=num_tracked_fisher_blocks, \n",
    "                                              buffer_size=subspace_buffer_size,\n",
    "                                              device=eig_vecs.device)\n",
    "            subspace_manager.update_buffer(block_idx=0, new_eigenvectors=eig_vecs)\n",
    "            retrieved_vecs = subspace_manager.get_subspace(block_idx=0)\n",
    "            if retrieved_vecs is not None:\n",
    "                print(f\"Retrieved {retrieved_vecs.shape[1]} vectors (shape {retrieved_vecs.shape}) from SubspaceBuffer for block 0.\")\n",
    "            else:\n",
    "                print(\"No vectors retrieved from SubspaceBuffer for block 0 (likely empty or issue).\")\n",
    "        else:\n",
    "            print(\"FisherEigen decomposition failed to produce eigenvectors.\")\n",
    "    else:\n",
    "        print(\"Skipping FisherEigen decomposition as Fisher block is empty or not computed.\")\n",
    "else:\n",
    "    print(\"Skipping FisherEigen & SubspaceBuffer example as KFAC handlers are not available.\")\n",
    "\n",
    "print(\"\\\\n--- Infrastructure Script End ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 2: Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T12:55:18.544423Z",
     "iopub.status.busy": "2025-05-21T12:55:18.544117Z",
     "iopub.status.idle": "2025-05-21T12:55:18.548083Z",
     "shell.execute_reply": "2025-05-21T12:55:18.547386Z",
     "shell.execute_reply.started": "2025-05-21T12:55:18.544399Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Forward Pass Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T12:55:18.548897Z",
     "iopub.status.busy": "2025-05-21T12:55:18.548714Z",
     "iopub.status.idle": "2025-05-21T12:55:18.567723Z",
     "shell.execute_reply": "2025-05-21T12:55:18.567119Z",
     "shell.execute_reply.started": "2025-05-21T12:55:18.548882Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def forward_hook(layer, inputs, outputs):\n",
    "    \"\"\"\n",
    "    Hook to track activation statistics for K-FAC and record covariances.\n",
    "    Attaches A_factor directly to the layer.\n",
    "    \"\"\"\n",
    "    # Track activation statistics for K-FAC\n",
    "    x = inputs[0].detach()\n",
    "    # Ensure x is 2D: (batch_size_effective, in_features)\n",
    "    if x.ndim > 2:\n",
    "        x_reshaped = x.reshape(-1, x.shape[-1])\n",
    "    else:\n",
    "        x_reshaped = x\n",
    "    \n",
    "    if x_reshaped.shape[0] == 0: return\n",
    "\n",
    "    # Initialize A_factor if not present or different device/dtype\n",
    "    if not hasattr(layer, 'A_factor') or layer.A_factor is None or \\\n",
    "       layer.A_factor.shape[0] != x_reshaped.shape[1] or \\\n",
    "       layer.A_factor.device != x_reshaped.device or layer.A_factor.dtype != x_reshaped.dtype:\n",
    "        layer.A_factor = torch.zeros(x_reshaped.shape[1], x_reshaped.shape[1], device=x_reshaped.device, dtype=x_reshaped.dtype)\n",
    "\n",
    "    # KFAC A_factor update: EMA of (x.T @ x) / N\n",
    "    # This is M2 (second moment matrix), not covariance if mean is non-zero.\n",
    "    # KFAC uses E[aa^T]\n",
    "    current_A_factor_val = (x_reshaped.T @ x_reshaped) / x_reshaped.size(0)\n",
    "    layer.A_factor = 0.95 * layer.A_factor + 0.05 * current_A_factor_val\n",
    "    \n",
    "    # Record input/output covariances (though not directly used in subsequent GRIT steps provided)\n",
    "    # Ensure layer has these attributes initialized correctly.\n",
    "    if not hasattr(layer, 'input_cov') or layer.input_cov is None:\n",
    "        layer.input_cov = torch.zeros_like(layer.A_factor)\n",
    "    if not hasattr(layer, 'output_cov') or layer.output_cov is None:\n",
    "        out_features = outputs.shape[-1]\n",
    "        layer.output_cov = torch.zeros(out_features, out_features, device=outputs.device, dtype=outputs.dtype)\n",
    "\n",
    "    if x_reshaped.size(0) > 1:\n",
    "        layer.input_cov = (x_reshaped.T @ x_reshaped) / (x_reshaped.size(0) -1) # Sample covariance\n",
    "    \n",
    "    # Process outputs for output_cov\n",
    "    if outputs.ndim > 2:\n",
    "        outputs_reshaped = outputs.reshape(-1, outputs.shape[-1])\n",
    "    else:\n",
    "        outputs_reshaped = outputs\n",
    "    \n",
    "    if outputs_reshaped.size(0) > 1:\n",
    "        layer.output_cov = (outputs_reshaped.T @ outputs_reshaped) / (outputs_reshaped.size(0) -1)\n",
    "\n",
    "def instrument_forward(model):\n",
    "    \"\"\"Registers forward_hook on Linear and Conv2d layers.\"\"\"\n",
    "    for layer in model.modules():\n",
    "        if isinstance(layer, (nn.Linear, nn.Conv2d)): \n",
    "            # Initialize dummy factors if they don't exist, to avoid issues on first pass\n",
    "            # Hooks will overwrite these. This is mainly for layers not hit by backward pass early.\n",
    "            if not hasattr(layer, 'A_factor'):\n",
    "                # Determine in_features for Linear vs Conv2d\n",
    "                if isinstance(layer, nn.Linear):\n",
    "                    in_dim = layer.in_features\n",
    "                elif isinstance(layer, nn.Conv2d):\n",
    "                    in_dim = layer.in_channels * layer.kernel_size[0] * layer.kernel_size[1] # Simplified view\n",
    "                else:\n",
    "                    in_dim = 0 # Should not happen due to outer check\n",
    "\n",
    "                if in_dim > 0:\n",
    "                    try:\n",
    "                        device = next(layer.parameters()).device\n",
    "                    except StopIteration:\n",
    "                        device = torch.device(\"cpu\")\n",
    "                    layer.A_factor = torch.zeros(in_dim, in_dim, device=device)\n",
    "\n",
    "            layer.register_forward_hook(forward_hook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Backward Pass Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T12:55:18.568592Z",
     "iopub.status.busy": "2025-05-21T12:55:18.568408Z",
     "iopub.status.idle": "2025-05-21T12:55:18.588844Z",
     "shell.execute_reply": "2025-05-21T12:55:18.588225Z",
     "shell.execute_reply.started": "2025-05-21T12:55:18.568578Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def backward_hook(layer, grad_input, grad_output):\n",
    "    \"\"\"\n",
    "    Hook to compute gradient covariances (B_factor) and update Kronecker factor.\n",
    "    Attaches B_factor and kron_factor directly to the layer.\n",
    "    \"\"\"\n",
    "    grad = grad_output[0].detach()\n",
    "    # Ensure grad is 2D: (batch_size_effective, out_features)\n",
    "    if grad.ndim > 2:\n",
    "        grad_reshaped = grad.reshape(-1, grad.shape[-1])\n",
    "    else:\n",
    "        grad_reshaped = grad\n",
    "\n",
    "    if grad_reshaped.shape[0] == 0: return\n",
    "\n",
    "    # Initialize B_factor if not present or different device/dtype\n",
    "    if not hasattr(layer, 'B_factor') or layer.B_factor is None or \\\n",
    "       layer.B_factor.shape[0] != grad_reshaped.shape[1] or \\\n",
    "       layer.B_factor.device != grad_reshaped.device or layer.B_factor.dtype != grad_reshaped.dtype:\n",
    "        layer.B_factor = torch.zeros(grad_reshaped.shape[1], grad_reshaped.shape[1], device=grad_reshaped.device, dtype=grad_reshaped.dtype)\n",
    "\n",
    "    # KFAC B_factor update: EMA of (grad.T @ grad) / N\n",
    "    # This is M2 (second moment matrix) for gradients. KFAC uses E[gg^T]\n",
    "    current_B_factor_val = (grad_reshaped.T @ grad_reshaped) / grad_reshaped.size(0)\n",
    "    layer.B_factor = 0.95 * layer.B_factor + 0.05 * current_B_factor_val\n",
    "    \n",
    "    # Update Kronecker factors\n",
    "    if hasattr(layer, 'A_factor') and layer.A_factor is not None and \\\n",
    "       layer.A_factor.numel() > 0 and layer.B_factor.numel() > 0:\n",
    "        try:\n",
    "            # Ensure A_factor and B_factor are 2D square matrices before kron\n",
    "            if layer.A_factor.ndim == 2 and layer.B_factor.ndim == 2 and \\\n",
    "               layer.A_factor.shape[0] == layer.A_factor.shape[1] and \\\n",
    "               layer.B_factor.shape[0] == layer.B_factor.shape[1]:\n",
    "                layer.kron_factor = torch.kron(layer.A_factor, layer.B_factor)\n",
    "            else:\n",
    "                # print(f\"Warning: A_factor or B_factor for layer is not a 2D square matrix. Skipping kron_factor update.\")\n",
    "                # print(f\"A_factor shape: {layer.A_factor.shape}, B_factor shape: {layer.B_factor.shape}\")\n",
    "                if not hasattr(layer, 'kron_factor'): layer.kron_factor = None\n",
    "        except Exception as e:\n",
    "            # print(f\"Error computing Kronecker product for layer: {e}. kron_factor not updated.\")\n",
    "            if not hasattr(layer, 'kron_factor'): layer.kron_factor = None\n",
    "    else:\n",
    "        if not hasattr(layer, 'kron_factor'): layer.kron_factor = None\n",
    "\n",
    "\n",
    "def instrument_backward(model):\n",
    "    \"\"\"Registers backward_hook on Linear and Conv2d layers.\"\"\"\n",
    "    for layer in model.modules():\n",
    "        if isinstance(layer, (nn.Linear, nn.Conv2d)): \n",
    "            if not hasattr(layer, 'B_factor'): # Initialize if not present\n",
    "                 if isinstance(layer, nn.Linear):\n",
    "                    out_dim = layer.out_features\n",
    "                 elif isinstance(layer, nn.Conv2d):\n",
    "                    out_dim = layer.out_channels\n",
    "                 else:\n",
    "                    out_dim = 0\n",
    "                 if out_dim > 0:\n",
    "                    try:\n",
    "                        device = next(layer.parameters()).device\n",
    "                    except StopIteration:\n",
    "                        device = torch.device(\"cpu\")\n",
    "                    layer.B_factor = torch.zeros(out_dim, out_dim, device=device)\n",
    "            \n",
    "            # Using register_full_backward_hook for broader compatibility\n",
    "            layer.register_full_backward_hook(backward_hook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Natural Gradient Calculation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T12:55:18.589860Z",
     "iopub.status.busy": "2025-05-21T12:55:18.589647Z",
     "iopub.status.idle": "2025-05-21T12:55:18.609467Z",
     "shell.execute_reply": "2025-05-21T12:55:18.608853Z",
     "shell.execute_reply.started": "2025-05-21T12:55:18.589838Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_lora_fisher(lora_layer: FlexLoRA, damping: float = 1e-6) -> torch.Tensor | None:\n",
    "    \"\"\"\n",
    "    Computes the Fisher Information Matrix for concatenated LoRA parameters [vec(A); vec(B)].\n",
    "    Returns F + damping * I.\n",
    "    Returns None if gradients are not available or shapes are problematic.\n",
    "    \"\"\"\n",
    "    if not hasattr(lora_layer, 'lora_A') or not hasattr(lora_layer, 'lora_B') or \\\n",
    "       lora_layer.lora_A is None or lora_layer.lora_B is None:\n",
    "        # print(f\"compute_lora_fisher: lora_A or lora_B not found in {type(lora_layer).__name__}\")\n",
    "        return None\n",
    "        \n",
    "    A = lora_layer.lora_A\n",
    "    B = lora_layer.lora_B\n",
    "\n",
    "    if A.grad is None or B.grad is None:\n",
    "        # print(f\"compute_lora_fisher: Gradients for lora_A or lora_B are None in {type(lora_layer).__name__}\")\n",
    "        return None\n",
    "    \n",
    "    # Ensure A and B have expected dimensions (rank > 0)\n",
    "    if A.ndim < 2 or B.ndim < 2 or A.shape[1] == 0 or B.shape[0] == 0: # rank is A.shape[1] or B.shape[0]\n",
    "        # print(f\"compute_lora_fisher: lora_A or lora_B has zero rank or unexpected ndim. A_shape: {A.shape}, B_shape: {B.shape}\")\n",
    "        return None\n",
    "\n",
    "    # grad_A = A.grad.reshape(-1,1) # Not used in F computation directly\n",
    "    # grad_B = B.grad.reshape(-1,1) # Not used in F computation directly\n",
    "    \n",
    "    # Fisher for concatenated [vec(A); vec(B)]\n",
    "    # F = torch.kron(B @ B.T, torch.eye(A.shape[0])) + \\\n",
    "    #     torch.kron(torch.eye(B.shape[1]), A.T @ A)\n",
    "    # This Fisher is for FIM of W_eff = W_base + AB if A,B are small perturbations.\n",
    "    # Or it is an approximation of the FIM for parameters A and B.\n",
    "    # Let's assume A.shape = (in_features, rank), B.shape = (rank, out_features)\n",
    "    \n",
    "    in_features_A = A.shape[0]\n",
    "    rank_A = A.shape[1]\n",
    "    rank_B = B.shape[0]\n",
    "    out_features_B = B.shape[1]\n",
    "\n",
    "    if rank_A != rank_B:\n",
    "        # print(f\"compute_lora_fisher: Rank mismatch between lora_A ({rank_A}) and lora_B ({rank_B}). Cannot compute Fisher.\")\n",
    "        return None\n",
    "    if rank_A == 0 : # current_rank can be 0\n",
    "        # print(f\"compute_lora_fisher: Rank is 0. Cannot compute Fisher.\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # Term 1: kron(B @ B.T, I_in)  where B is (rank, out_features)\n",
    "        # B @ B.T gives (rank, rank)\n",
    "        # I_in is (in_features, in_features)\n",
    "        term1_kron1 = B @ B.T\n",
    "        term1_kron2 = torch.eye(in_features_A, device=A.device, dtype=A.dtype)\n",
    "        F_term1 = torch.kron(term1_kron1, term1_kron2) # Shape: (rank*in, rank*in) - This seems for vec(A)\n",
    "\n",
    "        # Term 2: kron(I_out, A.T @ A) where A is (in_features, rank)\n",
    "        # A.T @ A gives (rank, rank)\n",
    "        # I_out is (out_features, out_features)\n",
    "        term2_kron1 = torch.eye(out_features_B, device=B.device, dtype=B.dtype)\n",
    "        term2_kron2 = A.T @ A \n",
    "        F_term2 = torch.kron(term2_kron1, term2_kron2) # Shape: (out*rank, out*rank) - This seems for vec(B)\n",
    "        \n",
    "        # The paper's structure for F_LoRA = [F_A, F_AB; F_BA, F_B] might be more appropriate.\n",
    "        # The formula sums two Kronecker products directly.\n",
    "        # This implies the FIM is block diagonal for vec(A) and vec(B), or this is a simplification.\n",
    "        # If F is for [vec(A); vec(B)], its size should be (in*rank + rank*out, in*rank + rank*out).\n",
    "        # The provided formula F = kron(B B^T, I_A_in) + kron(I_B_out, A^T A)\n",
    "        # assumes that the Fisher is block-diagonal and these are the blocks for A and B.\n",
    "        # Or, that vec(A) and vec(B) are independent.\n",
    "        # Let's assume this is an approximation of F_A and F_B that are then used separately.\n",
    "        # OR, F_A = B B.T kronecker I_in_A  and F_B = I_out_B kronecker A.T A\n",
    "        # This means F_A is (in_features * rank, in_features * rank)\n",
    "        # And F_B is (out_features * rank, out_features * rank)\n",
    "        # If so, the Fisher matrix F for the concatenated parameters [vec(A); vec(B)]\n",
    "        # would be a block diagonal matrix: diag(F_A, F_B)\n",
    "        # F_A_dim = in_features_A * rank_A\n",
    "        # F_B_dim = out_features_B * rank_A # rank_A == rank_B\n",
    "        \n",
    "        # If the formula is for the full FIM of [vec(A); vec(B)] and it's a sum,\n",
    "        # then F_term1 and F_term2 must have the same dimensions. This is not generally true.\n",
    "        # (rank*in_features_A) vs (rank*out_features_B)\n",
    "        # This strongly suggests the formula is for a simplified or block-diagonal Fisher.\n",
    "\n",
    "        # Perhaps F_A = torch.kron(B @ B.T, torch.eye(A.shape[0]))  (for vec(A))\n",
    "        # And F_B_approx = torch.kron(torch.eye(B.shape[1]), A.T @ A) (for vec(B))\n",
    "        # And the overall update is done separately for A and B using these.\n",
    "        # F is intended as a single matrix of a particular structure.\n",
    "        \n",
    "        # Given the direct sum, they MUST be the same size. This only happens if in_features_A == out_features_B.\n",
    "        # This is very restrictive.\n",
    "        \n",
    "        # Block diagonal Fisher:\n",
    "        # F_A_block = torch.kron(B.data @ B.data.T, torch.eye(in_features_A, device=A.device, dtype=A.dtype))\n",
    "        # F_B_block = torch.kron(torch.eye(out_features_B, device=B.device, dtype=B.dtype), A.data.T @ A.data)\n",
    "\n",
    "        # Total dimension for F for [vec(A); vec(B)]\n",
    "        total_param_dim = (in_features_A * rank_A) + (rank_A * out_features_B)\n",
    "        F_matrix = torch.zeros(total_param_dim, total_param_dim, device=A.device, dtype=A.dtype)\n",
    "\n",
    "        # Populate F_A_block for vec(A) parameters\n",
    "        dim_A_flat = in_features_A * rank_A\n",
    "        # Using .detach() for safer access to tensor data without autograd history\n",
    "        F_A_block_calc = torch.kron(B.detach() @ B.detach().T, torch.eye(in_features_A, device=A.device, dtype=A.dtype))\n",
    "        F_matrix[:dim_A_flat, :dim_A_flat] = F_A_block_calc\n",
    "\n",
    "        # Populate F_B_block for vec(B) parameters\n",
    "        dim_B_flat = rank_A * out_features_B\n",
    "        # Using .detach() for safer access to tensor data without autograd history\n",
    "        F_B_block_calc = torch.kron(torch.eye(out_features_B, device=B.device, dtype=B.dtype), A.detach().T @ A.detach())\n",
    "        F_matrix[dim_A_flat:, dim_A_flat:] = F_B_block_calc\n",
    "        \n",
    "        # Add damping to the combined block-diagonal Fisher\n",
    "        return F_matrix + damping * torch.eye(total_param_dim, device=A.device, dtype=A.dtype)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in compute_lora_fisher for {type(lora_layer).__name__}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T12:55:18.610297Z",
     "iopub.status.busy": "2025-05-21T12:55:18.610079Z",
     "iopub.status.idle": "2025-05-21T12:55:18.625079Z",
     "shell.execute_reply": "2025-05-21T12:55:18.624470Z",
     "shell.execute_reply.started": "2025-05-21T12:55:18.610282Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 3.2 LoRA-Constrained Update (adapted for FlexLoRA)\n",
    "\n",
    "def natural_gradient_step(model: nn.Module, lr: float = 0.001, damping_lora_fisher: float = 1e-6): # Added damping_lora_fisher\n",
    "    \"\"\"\n",
    "    Performs a natural gradient update step for LoRA parameters using LoRA-specific Fisher.\n",
    "    \"\"\"\n",
    "    for module_name, module in model.named_modules():\n",
    "        if isinstance(module, FlexLoRA):\n",
    "            if module.current_rank == 0: # Skip if LoRA rank is zero\n",
    "                # print(f\"Skipping NG for {module_name}: LoRA rank is 0.\")\n",
    "                continue\n",
    "            if not hasattr(module, 'lora_A') or not hasattr(module, 'lora_B') or \\\n",
    "               module.lora_A.grad is None or module.lora_B.grad is None:\n",
    "                # print(f\"Skipping NG for {module_name}: missing LoRA params or grads.\")\n",
    "                continue\n",
    "\n",
    "            # Compute LoRA-specific Fisher\n",
    "            F_lora = compute_lora_fisher(module, damping=damping_lora_fisher)\n",
    "\n",
    "            if F_lora is None or F_lora.numel() == 0:\n",
    "                # print(f\"Skipping NG for {module_name}: LoRA Fisher computation failed or resulted in empty tensor. Falling back to SGD.\")\n",
    "                with torch.no_grad():\n",
    "                    if module.lora_A.grad is not None:\n",
    "                        module.lora_A.data -= lr * module.lora_A.grad\n",
    "                        module.lora_A.grad.zero_()\n",
    "                    if module.lora_B.grad is not None:\n",
    "                        module.lora_B.data -= lr * module.lora_B.grad\n",
    "                        module.lora_B.grad.zero_()\n",
    "                continue\n",
    "            \n",
    "            grad_A_flat = module.lora_A.grad.flatten()\n",
    "            grad_B_flat = module.lora_B.grad.flatten()\n",
    "            grad_cat = torch.cat([grad_A_flat, grad_B_flat]) # Shape: (in*rank + rank*out)\n",
    "            \n",
    "            if F_lora.shape[0] != grad_cat.shape[0] or F_lora.shape[1] != grad_cat.shape[0]:\n",
    "                print(f\"Warning: Shape mismatch for F_lora_inv @ grad_cat in LoRA layer {module_name}. \"\n",
    "                      f\"F_lora shape: {F_lora.shape}, grad_cat shape: {grad_cat.shape}. \"\n",
    "                      f\"Falling back to SGD for this LoRA layer.\")\n",
    "                with torch.no_grad():\n",
    "                    if module.lora_A.grad is not None:\n",
    "                        module.lora_A.data -= lr * module.lora_A.grad\n",
    "                        module.lora_A.grad.zero_()\n",
    "                    if module.lora_B.grad is not None:\n",
    "                        module.lora_B.data -= lr * module.lora_B.grad\n",
    "                        module.lora_B.grad.zero_()\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # Invert Fisher: F_lora_inv = F_lora.inverse() or torch.linalg.pinv(F_lora)\n",
    "                # Using pinv for stability\n",
    "                F_lora_inv = torch.linalg.pinv(F_lora)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    ng_update = F_lora_inv @ grad_cat\n",
    "                \n",
    "                    num_elements_A = module.lora_A.numel()\n",
    "                    module.lora_A.data -= lr * ng_update[:num_elements_A].reshape_as(module.lora_A)\n",
    "                    module.lora_B.data -= lr * ng_update[num_elements_A:].reshape_as(module.lora_B)\n",
    "                \n",
    "                    if module.lora_A.grad is not None: module.lora_A.grad.zero_()\n",
    "                    if module.lora_B.grad is not None: module.lora_B.grad.zero_()\n",
    "            except RuntimeError as e:\n",
    "                print(f\"RuntimeError during natural gradient update for {module_name}: {e}. Falling back to SGD.\")\n",
    "                with torch.no_grad(): # Fallback to SGD\n",
    "                    if module.lora_A.grad is not None:\n",
    "                        module.lora_A.data -= lr * module.lora_A.grad\n",
    "                        module.lora_A.grad.zero_()\n",
    "                    if module.lora_B.grad is not None:\n",
    "                        module.lora_B.data -= lr * module.lora_B.grad\n",
    "                        module.lora_B.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Neural Reprojection (adapted for FlexLoRA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T12:55:18.626069Z",
     "iopub.status.busy": "2025-05-21T12:55:18.625828Z",
     "iopub.status.idle": "2025-05-21T12:55:18.645272Z",
     "shell.execute_reply": "2025-05-21T12:55:18.644643Z",
     "shell.execute_reply.started": "2025-05-21T12:55:18.626048Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def neural_reprojection(\n",
    "    lora_layer: FlexLoRA, \n",
    "    rho_eigen_sum: float = 0.9, \n",
    "    damping_lora_fisher: float = 1e-6 # Damping for LoRA Fisher used here\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Performs neural reprojection on LoRA parameters.\n",
    "    Uses a LoRA-specific Fisher matrix to find the principal subspace for reprojection.\n",
    "    WARNING: The projection math (delta_A = lora_A @ U_k @ U_k.T) is dimensionally problematic\n",
    "    and is SKIPPED. This function currently only calculates k and U_k.\n",
    "    \"\"\"\n",
    "    if not isinstance(lora_layer, FlexLoRA):\n",
    "        return\n",
    "    if lora_layer.current_rank == 0:\n",
    "        return\n",
    "    \n",
    "    # Compute LoRA-specific Fisher matrix (F_lora for [vec(A); vec(B)])\n",
    "    F_lora = compute_lora_fisher(lora_layer, damping=damping_lora_fisher)\n",
    "\n",
    "    if F_lora is None or F_lora.numel() == 0 or F_lora.shape[0] == 0:\n",
    "        # print(f\"Skipping reprojection for {type(lora_layer).__name__}: LoRA Fisher not available or empty.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # SVD of F_lora. S contains singular values (eigenvalues if F_lora is PSD)\n",
    "        # U contains left singular vectors (eigenvectors if F_lora is symmetric)\n",
    "        # F_lora should be symmetric by construction (if A, B data used, or if it's from KFAC factors that are symm)\n",
    "        # The compute_lora_fisher builds it block-diagonally from symmetric blocks.\n",
    "        U_f, S_eigenvalues_f, Vh_f = torch.linalg.svd(F_lora)\n",
    "    except Exception as e:\n",
    "        # print(f\"SVD failed for F_lora in neural_reprojection for {type(lora_layer).__name__}: {e}. Skipping.\")\n",
    "        return\n",
    "    \n",
    "    if S_eigenvalues_f.numel() == 0:\n",
    "        # print(f\"Warning: No singular values from SVD of F_lora in {type(lora_layer).__name__}. Skipping reprojection.\")\n",
    "        return\n",
    "\n",
    "    total_eigenvalue_sum = torch.sum(S_eigenvalues_f)\n",
    "    if total_eigenvalue_sum <= 1e-9: # Avoid division by zero or issues with non-positive sums\n",
    "        # print(f\"Warning: Total eigenvalue sum of F_lora is not positive ({total_eigenvalue_sum}) for {type(lora_layer).__name__}. Cannot determine k. Skipping.\")\n",
    "        return\n",
    "        \n",
    "    cumulative_eigenvalue_sum = torch.cumsum(S_eigenvalues_f, dim=0)\n",
    "    k_candidates = torch.where(cumulative_eigenvalue_sum >= rho_eigen_sum * total_eigenvalue_sum)[0]\n",
    "    \n",
    "    if len(k_candidates) > 0:\n",
    "        k = k_candidates[0].item() + 1 \n",
    "    else:\n",
    "        k = S_eigenvalues_f.shape[0] \n",
    "\n",
    "    k = max(1, min(k, S_eigenvalues_f.shape[0]))\n",
    "\n",
    "    # print(f\"Calculated k for reprojection in {type(lora_layer).__name__}: {k} (from {S_eigenvalues_f.shape[0]} total F_lora eigenvalues, rho={rho_eigen_sum})\")\n",
    "    \n",
    "    # U_k are the top k eigenvectors of F_lora.\n",
    "    # These eigenvectors correspond to the concatenated [vec(A); vec(B)] space.\n",
    "    U_k_lora_fisher = U_f[:, :k] # Shape: ( (in*rank + rank*out), k_eigen_reproj )\n",
    "    \n",
    "    # Original projection logic was commented out due to dimensional issues:\n",
    "    # delta_A = layer.lora_A @ U_k @ U_k.T # This U_k would need to be for A's space\n",
    "    # delta_B = U_k.T @ layer.lora_B # This U_k would need to be for B's space\n",
    "    \n",
    "    # If U_k_lora_fisher is for the concatenated space, applying it directly to lora_A and lora_B separately is not straightforward.\n",
    "    # The reprojection should be: new_params_flat = U_k @ U_k.T @ current_params_flat\n",
    "    # Then unflatten and update lora_A and lora_B.\n",
    "    \n",
    "    # Flatten current LoRA parameters\n",
    "    lora_A_flat = lora_layer.lora_A.data.flatten()\n",
    "    lora_B_flat = lora_layer.lora_B.data.flatten()\n",
    "    lora_params_flat = torch.cat([lora_A_flat, lora_B_flat])\n",
    "\n",
    "    if U_k_lora_fisher.shape[0] != lora_params_flat.shape[0]:\n",
    "        # print(f\"Warning: Dimension mismatch in neural_reprojection for {type(lora_layer).__name__}. \"\n",
    "        #       f\"U_k_lora_fisher dimension {U_k_lora_fisher.shape[0]} vs LoRA params dimension {lora_params_flat.shape[0]}. Skipping update.\")\n",
    "        return\n",
    "\n",
    "    # Project onto the subspace spanned by U_k_lora_fisher\n",
    "    projected_lora_params_flat = U_k_lora_fisher @ (U_k_lora_fisher.T @ lora_params_flat)\n",
    "    \n",
    "    # Reshape and update LoRA parameters\n",
    "    with torch.no_grad():\n",
    "        num_elements_A = lora_layer.lora_A.numel()\n",
    "        new_lora_A_flat = projected_lora_params_flat[:num_elements_A]\n",
    "        new_lora_B_flat = projected_lora_params_flat[num_elements_A:]\n",
    "        \n",
    "        lora_layer.lora_A.data.copy_(new_lora_A_flat.reshape_as(lora_layer.lora_A.data))\n",
    "        lora_layer.lora_B.data.copy_(new_lora_B_flat.reshape_as(lora_layer.lora_B.data))\n",
    "        \n",
    "    # print(f\"Neural reprojection applied to {type(lora_layer).__name__} using U_k from LoRA Fisher (k={k}).\")\n",
    "    pass # Actual update logic is now included above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T12:55:18.646046Z",
     "iopub.status.busy": "2025-05-21T12:55:18.645868Z",
     "iopub.status.idle": "2025-05-21T12:55:18.664792Z",
     "shell.execute_reply": "2025-05-21T12:55:18.664054Z",
     "shell.execute_reply.started": "2025-05-21T12:55:18.646032Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class SVTracker(nn.Module):\n",
    "    \"\"\"\n",
    "    Tracks the history of singular values for a FlexLoRA layer's effective matrix (lora_A @ lora_B).\n",
    "    Assumes the associated FlexLoRA layer has `lora_A`, `lora_B`, and `max_rank` attributes.\n",
    "    \"\"\"\n",
    "    def __init__(self, flex_lora_layer, window_size: int = 100):\n",
    "        super().__init__()\n",
    "        if not hasattr(flex_lora_layer, 'lora_A') or not hasattr(flex_lora_layer, 'lora_B'):\n",
    "            raise ValueError(\"Provided layer must have 'lora_A' and 'lora_B' parameters.\")\n",
    "        if not hasattr(flex_lora_layer, 'max_rank'):\n",
    "            raise ValueError(\"Provided FlexLoRA layer must have a 'max_rank' attribute.\")\n",
    "            \n",
    "        self.flex_lora_layer = flex_lora_layer\n",
    "        self.window_size = window_size\n",
    "        \n",
    "        self.register_buffer('sv_history', \n",
    "                             torch.zeros(window_size, flex_lora_layer.max_rank, \n",
    "                                         device=flex_lora_layer.lora_A.device))\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"Calculates SVD of lora_A @ lora_B and updates history.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            lora_A_data = self.flex_lora_layer.lora_A.data\n",
    "            lora_B_data = self.flex_lora_layer.lora_B.data\n",
    "            effective_matrix = lora_A_data @ lora_B_data\n",
    "            \n",
    "            try:\n",
    "                # SVD: U, S, Vh = svd(A). S are singular values.\n",
    "                S = torch.linalg.svdvals(effective_matrix)\n",
    "            except Exception as e:\n",
    "                # print(f\"SVD failed in SVTracker: {e}. Skipping history update.\")\n",
    "                return\n",
    "\n",
    "            # Pad or truncate S to match self.flex_lora_layer.max_rank for consistent history storage\n",
    "            num_singular_values = S.shape[0]\n",
    "            max_r = self.flex_lora_layer.max_rank\n",
    "            \n",
    "            s_padded = torch.zeros(max_r, device=S.device)\n",
    "            if num_singular_values > 0:\n",
    "                s_padded[:min(num_singular_values, max_r)] = S[:min(num_singular_values, max_r)]\n",
    "            \n",
    "            self.sv_history = torch.roll(self.sv_history, shifts=1, dims=0)\n",
    "            self.sv_history[0, :] = s_padded.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T12:55:18.666262Z",
     "iopub.status.busy": "2025-05-21T12:55:18.665624Z",
     "iopub.status.idle": "2025-05-21T12:55:18.685172Z",
     "shell.execute_reply": "2025-05-21T12:55:18.684427Z",
     "shell.execute_reply.started": "2025-05-21T12:55:18.666241Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class RankScheduler:\n",
    "    def __init__(self, initial_rank: int, min_rank: int, max_rank: int, # Added min/max rank from FlexLoRA\n",
    "                 warmup_steps: int = 1000, ema_alpha: float = 0.1): # Added ema_alpha for sv_ema\n",
    "        self.current_rank = initial_rank\n",
    "        self.min_rank = min_rank # Store min_rank\n",
    "        self.max_rank = max_rank # Store max_rank\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.step_counter = 0\n",
    "        self.ema_alpha = ema_alpha # For smoothing sv_ratio\n",
    "        self.sv_ema = None # EMA of singular value ratio\n",
    "\n",
    "    def update(self, sv_ratio: float): # sv_ratio is sigma_1/sigma_r from rank_adjustment's perspective\n",
    "        self.step_counter += 1\n",
    "        \n",
    "        if self.sv_ema is None:\n",
    "            self.sv_ema = sv_ratio\n",
    "        else:\n",
    "            self.sv_ema = self.ema_alpha * sv_ratio + (1 - self.ema_alpha) * self.sv_ema\n",
    "\n",
    "        if self.step_counter < self.warmup_steps:\n",
    "            return self.current_rank # Keep current rank during warmup\n",
    "\n",
    "        # Logic:\n",
    "        # if self.step_counter > 0.8: # Typo: self.step_counter > 0.8 ? Should be sv_ema\n",
    "        # Assuming the condition is based on self.sv_ema\n",
    "        if self.sv_ema is not None: # Check if sv_ema is initialized\n",
    "            if self.sv_ema < 0.5: # If ratio is small, indicates potential for rank reduction\n",
    "                # Decrease rank, but not below min_rank\n",
    "                self.current_rank = max(self.min_rank, self.current_rank - 2) \n",
    "            elif self.sv_ema > 0.8: # If ratio is large, potential for rank increase\n",
    "                # Increase rank, but not above max_rank\n",
    "                self.current_rank = min(self.max_rank, self.current_rank + 2)\n",
    "        \n",
    "        return self.current_rank\n",
    "    \n",
    "    def get_rank(self):\n",
    "        return self.current_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T12:55:18.686236Z",
     "iopub.status.busy": "2025-05-21T12:55:18.685993Z",
     "iopub.status.idle": "2025-05-21T12:55:18.703407Z",
     "shell.execute_reply": "2025-05-21T12:55:18.702936Z",
     "shell.execute_reply.started": "2025-05-21T12:55:18.686217Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def rank_adjustment(\n",
    "    layer: FlexLoRA, \n",
    "    sv_tracker: SVTracker, \n",
    "    rank_scheduler: RankScheduler, # Pass the scheduler instance for this layer\n",
    "    # rank_eta, rank_tau are removed as scheduler now handles logic\n",
    "    sv_history_min_samples: int = 5 \n",
    "    ):\n",
    "    \"\"\"\n",
    "    Adjusts the rank of a FlexLoRA layer using an external RankScheduler\n",
    "    based on its singular value history (sigma_1/sigma_r ratio from mean_sv).\n",
    "    \"\"\"\n",
    "    if not all(hasattr(layer, attr) for attr in \n",
    "               ['current_rank', 'min_rank', 'max_rank', 'lora_A', 'lora_B']):\n",
    "        raise ValueError(\"FlexLoRA layer is missing required attributes for rank adjustment.\")\n",
    "\n",
    "    if sv_tracker.sv_history[:, 0].count_nonzero() < sv_history_min_samples :\n",
    "        # print(f\"Warning: Not enough singular value history for layer. Skipping rank adjustment. Need {sv_history_min_samples} samples.\")\n",
    "        return\n",
    "\n",
    "    with torch.no_grad():\n",
    "        mean_sv = sv_tracker.sv_history.mean(dim=0) \n",
    "        \n",
    "        current_rank_val = layer.current_rank # Use a different name to avoid conflict with scheduler's current_rank\n",
    "        if current_rank_val == 0 or current_rank_val > layer.max_rank: # current_rank_val == 0 means LoRA is inactive or uninitialized for adjustment\n",
    "            return\n",
    "        \n",
    "        if mean_sv.numel() == 0 or current_rank_val > mean_sv.shape[0]:\n",
    "            # print(\"Warning: Mean singular values not available or current rank exceeds available SVs. Skipping rank adjustment.\")\n",
    "            return\n",
    "\n",
    "        sigma_1 = mean_sv[0].item()\n",
    "        sigma_r = mean_sv[current_rank_val - 1].item() if current_rank_val > 0 else 0.0\n",
    "\n",
    "        if sigma_1 < 1e-9 or sigma_r < 1e-9: # Avoid division by zero or unstable ratios\n",
    "            # print(f\"Warning: sigma_1 ({sigma_1}) or sigma_r ({sigma_r}) is near zero. Skipping rank update based on sv_ratio.\")\n",
    "            return \n",
    "        \n",
    "        sv_ratio_metric = sigma_1 / sigma_r\n",
    "        \n",
    "        new_rank = rank_scheduler.update(sv_ratio_metric) \n",
    "        new_rank = int(max(layer.min_rank, min(new_rank, layer.max_rank)))\n",
    "\n",
    "\n",
    "        if new_rank == current_rank_val:\n",
    "            return\n",
    "        \n",
    "        old_lora_A = layer.lora_A.data\n",
    "        old_lora_B = layer.lora_B.data\n",
    "        device = old_lora_A.device\n",
    "\n",
    "        # Preserve weights using SVD of W_lora = old_lora_A @ old_lora_B\n",
    "        # This is generally more robust than padding/truncating A and B directly.\n",
    "        W_lora = old_lora_A @ old_lora_B\n",
    "        try:\n",
    "            U, S, Vh = torch.linalg.svd(W_lora)\n",
    "        except Exception as e:\n",
    "            print(f\"SVD failed during rank adjustment for layer {type(layer).__name__}: {e}. Rank not changed.\")\n",
    "            return\n",
    "\n",
    "        # Determine the effective rank for SVD reconstruction.\n",
    "        # This rank cannot exceed the number of singular values, S.shape[0] (min of W_lora dimensions).\n",
    "        # layer.max_rank should ideally be configured to be <= S.shape[0].\n",
    "        effective_rank_for_svd = new_rank\n",
    "        if new_rank > S.shape[0]:\n",
    "            print(f\"Warning: Target new_rank {new_rank} exceeds available singular values {S.shape[0]} for layer {type(layer).__name__}. \"\n",
    "                  f\"Capping rank to {S.shape[0]}. Consider adjusting layer.max_rank.\")\n",
    "            effective_rank_for_svd = S.shape[0]\n",
    "            new_rank = effective_rank_for_svd # Update new_rank to the capped value for consistency\n",
    "\n",
    "        if new_rank == current_rank_val and effective_rank_for_svd == new_rank : # Check again if capping resulted in no change\n",
    "             return\n",
    "\n",
    "\n",
    "        # Reconstruct LoRA matrices A' and B' such that A'B' approximates W_lora.\n",
    "        # A_new = U[:, :k] @ diag(sqrt(S[:k]))\n",
    "        # B_new = diag(sqrt(S[:k])) @ Vh[:k, :]\n",
    "        # This distributes singular values (sqrt) to both A and B.\n",
    "        \n",
    "        # Handle case where effective_rank_for_svd is 0 (e.g. if min_rank is 0 and it's chosen)\n",
    "        if effective_rank_for_svd == 0:\n",
    "            new_lora_A_data = torch.empty((old_lora_A.shape[0], 0), device=device, dtype=old_lora_A.dtype)\n",
    "            new_lora_B_data = torch.empty((0, old_lora_B.shape[1]), device=device, dtype=old_lora_B.dtype)\n",
    "        else:\n",
    "            U_k = U[:, :effective_rank_for_svd]\n",
    "            S_k = S[:effective_rank_for_svd]\n",
    "            Vh_k = Vh[:effective_rank_for_svd, :]\n",
    "\n",
    "            # Singular values (S_k) are non-negative. torch.sqrt(0) is 0.\n",
    "            sqrt_S_k = torch.sqrt(S_k)\n",
    "            diag_sqrt_S_k = torch.diag(sqrt_S_k)\n",
    "\n",
    "            new_lora_A_data = U_k @ diag_sqrt_S_k\n",
    "            new_lora_B_data = diag_sqrt_S_k @ Vh_k\n",
    "        \n",
    "        layer.lora_A = nn.Parameter(new_lora_A_data.to(device))\n",
    "        layer.lora_B = nn.Parameter(new_lora_B_data.to(device))\n",
    "        layer.current_rank = new_rank # new_rank might have been capped by S.shape[0]\n",
    "        rank_scheduler.current_rank = new_rank # Also update scheduler's internal current_rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Parameter Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T12:55:18.704470Z",
     "iopub.status.busy": "2025-05-21T12:55:18.704230Z",
     "iopub.status.idle": "2025-05-21T12:55:18.727385Z",
     "shell.execute_reply": "2025-05-21T12:55:18.726657Z",
     "shell.execute_reply.started": "2025-05-21T12:55:18.704446Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 5.1 Momentum Smoothing (Using existing robust version from Training.py)\n",
    "class MomentumFusion:\n",
    "    def __init__(self, beta: float = 0.9):\n",
    "        self.beta = beta\n",
    "        self.momentum_buffer = {} # Using dict for FlexLoRA parameters by name\n",
    "\n",
    "    def __call__(self, model: nn.Module):\n",
    "        with torch.no_grad():\n",
    "            for name, module in model.named_modules():\n",
    "                if isinstance(module, FlexLoRA):\n",
    "                    if module.current_rank == 0: continue # Skip if no LoRA adaptation\n",
    "\n",
    "                    params_to_fuse = {}\n",
    "                    if hasattr(module, 'lora_A') and module.lora_A is not None:\n",
    "                        params_to_fuse[\"lora_A\"] = module.lora_A\n",
    "                    if hasattr(module, 'lora_B') and module.lora_B is not None:\n",
    "                        params_to_fuse[\"lora_B\"] = module.lora_B\n",
    "                    \n",
    "                    for p_name, param in params_to_fuse.items():\n",
    "                        if param.requires_grad: # Only fuse trainable LoRA params\n",
    "                            full_param_name = f\"{name}.{p_name}\"\n",
    "                            if full_param_name not in self.momentum_buffer:\n",
    "                                self.momentum_buffer[full_param_name] = torch.zeros_like(param.data)\n",
    "                            \n",
    "                            buf = self.momentum_buffer[full_param_name]\n",
    "                            buf.mul_(self.beta).add_(param.data, alpha=1 - self.beta)\n",
    "                            param.data.copy_(buf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic Rank Adjustment System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T12:55:18.728310Z",
     "iopub.status.busy": "2025-05-21T12:55:18.728074Z",
     "iopub.status.idle": "2025-05-21T12:55:18.745500Z",
     "shell.execute_reply": "2025-05-21T12:55:18.744912Z",
     "shell.execute_reply.started": "2025-05-21T12:55:18.728293Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 5.2 Spectral Norm Enforcement (Using existing robust version from Training.py)\n",
    "def enforce_spectral_constraints(model: nn.Module, max_singular_value: float = 1.0):\n",
    "    \"\"\"\n",
    "    Enforces spectral norm constraints on the base weights of FlexLoRA layers.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        for module in model.modules():\n",
    "            if isinstance(module, FlexLoRA):\n",
    "                # Apply to the original base layer's weight\n",
    "                if hasattr(module.base_layer, 'weight') and module.base_layer.weight is not None:\n",
    "                    base_weight = module.base_layer.weight \n",
    "                    \n",
    "                    U, S, V_T = torch.linalg.svd(base_weight.data, full_matrices=False)\n",
    "                    S_clamped = torch.clamp(S, max=max_singular_value)\n",
    "                    \n",
    "                    # Reconstruct the weight matrix\n",
    "                    new_weight = U @ torch.diag(S_clamped) @ V_T\n",
    "                    base_weight.data.copy_(new_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T12:55:18.746801Z",
     "iopub.status.busy": "2025-05-21T12:55:18.746346Z",
     "iopub.status.idle": "2025-05-21T12:55:18.766543Z",
     "shell.execute_reply": "2025-05-21T12:55:18.765998Z",
     "shell.execute_reply.started": "2025-05-21T12:55:18.746778Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Training Loop Integration\n",
    "def grit_train_epoch(\n",
    "    model: nn.Module,\n",
    "    dataloader: torch.utils.data.DataLoader,\n",
    "    criterion: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer, # Optimizer for non-LoRA params\n",
    "    fusion_module: MomentumFusion,\n",
    "    sv_trackers: dict,                 # NEW: Dict mapping FlexLoRA layer names to SVTracker instances\n",
    "    rank_schedulers: dict,             # NEW: Dict mapping FlexLoRA layer names to RankScheduler instances\n",
    "    device: torch.device,\n",
    "    current_epoch: int,\n",
    "    lr_natural_gradient: float = 0.001, # LR for natural_gradient_step\n",
    "    damping_lora_fisher: float = 1e-6, # Damping for LoRA Fisher in NG and Reprojection\n",
    "    reprojection_rho_eigen_sum: float = 0.9, # RENAMED from reprojection_keep_ratio, new default example\n",
    "    spectral_constraint_sv_max: float = 1.0,\n",
    "    rank_adjust_sv_history_min_samples: int = 5 # NEW: Min samples for rank adjustment decisions\n",
    "    ):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    # Instrument model hooks (called once per epoch)\n",
    "    # Note: This means factors A and B are EMA over the entire epoch.\n",
    "    # KFAC typically updates factors more frequently or uses per-batch stats.\n",
    "    instrument_forward(model)  # Attaches A_factor\n",
    "    instrument_backward(model) # Attaches B_factor and kron_factor\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    for batch_idx, batch in enumerate(dataloader):\n",
    "        # Adapt this to your specific batch structure.\n",
    "        input_ids = batch[0].to(device)\n",
    "        targets = batch[1].to(device)\n",
    "        # attention_mask = batch.get('attention_mask', None) # If used\n",
    "        # if attention_mask is not None: attention_mask = attention_mask.to(device)\n",
    "\n",
    "        optimizer.zero_grad() # Zero gradients for all parameters\n",
    "\n",
    "        # Standard forward pass\n",
    "        # KFAC forward hooks run here (from instrument_forward), updating layer.A_factor\n",
    "        # outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        outputs = model(input_ids) \n",
    "        \n",
    "        loss = criterion(outputs.view(-1, outputs.size(-1)), targets.view(-1))\n",
    "        \n",
    "        # Standard backward pass\n",
    "        # KFAC backward hooks run here (from instrument_backward), updating layer.B_factor and layer.kron_factor\n",
    "        loss.backward()       # Compute gradients for all trainable parameters\n",
    "\n",
    "        # --- GRIT Specific Steps ---\n",
    "        # 1. Natural Gradient Step (for LoRA parameters)\n",
    "        # This function internally handles LoRA grads (updates and zeros them if successful or falls back to SGD).\n",
    "        natural_gradient_step(model, lr=lr_natural_gradient, damping_lora_fisher=damping_lora_fisher)\n",
    "        \n",
    "        # Standard optimizer step for any other parameters (e.g., embeddings, non-LoRA layers, classification head)\n",
    "        # LoRA parameters whose grads were handled by natural_gradient_step (and zeroed) won't be affected.\n",
    "        optimizer.step() # Optimizer step AFTER NG step.\n",
    "\n",
    "        # 2. Dynamic Rank Allocation for FlexLoRA layers (NEW)\n",
    "        # This step adjusts ranks and potentially re-initializes LoRA parameters for the *next* iteration.\n",
    "        # It occurs after gradient updates for the current iteration.\n",
    "        # Assumes SVTracker and RankScheduler instances are provided for relevant layers.\n",
    "        # Assumes rank_adjustment and SVTracker.forward are defined elsewhere.\n",
    "        with torch.no_grad(): # Rank adjustment should not affect gradient computation\n",
    "            for name, module_candidate in model.named_modules():\n",
    "                if isinstance(module_candidate, FlexLoRA):\n",
    "                    # Ensure the layer is active and has corresponding tracker/scheduler\n",
    "                    if module_candidate.current_rank > 0 and \\\n",
    "                       name in sv_trackers and \\\n",
    "                       name in rank_schedulers:\n",
    "                        \n",
    "                        current_sv_tracker = sv_trackers[name]\n",
    "                        current_rank_scheduler = rank_schedulers[name]\n",
    "                        \n",
    "                        # Call the SVTracker's forward method.\n",
    "                        # This method is assumed to update the tracker's internal state,\n",
    "                        # possibly using data collected by hooks during the model's forward/backward pass,\n",
    "                        # or by analyzing current weights/gradients.\n",
    "                        current_sv_tracker.forward() \n",
    "                        \n",
    "                        # Perform rank adjustment\n",
    "                        rank_adjustment(\n",
    "                            layer=module_candidate, \n",
    "                            sv_tracker=current_sv_tracker, \n",
    "                            rank_scheduler=current_rank_scheduler, \n",
    "                            sv_history_min_samples=rank_adjust_sv_history_min_samples\n",
    "                        )\n",
    "                \n",
    "        # 3. Neural Reprojection (applied to FlexLoRA layers) - (was 2)\n",
    "        # This happens every step.\n",
    "        for module_candidate in model.modules():\n",
    "            if isinstance(module_candidate, FlexLoRA):\n",
    "                # The neural_reprojection function expects a FlexLoRA layer\n",
    "                # and uses its base_layer.kron_factor.\n",
    "                neural_reprojection(module_candidate, rho_eigen_sum=reprojection_rho_eigen_sum, damping_lora_fisher=damping_lora_fisher)\n",
    "                \n",
    "        # 4. Parameter Fusion (LoRA momentum) - (was 3)\n",
    "        fusion_module(model)\n",
    "        \n",
    "        # 5. Spectral Constraints (on base weights of LoRA layers) - (was 4)\n",
    "        enforce_spectral_constraints(model, max_singular_value=spectral_constraint_sv_max)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if batch_idx % 50 == 0 : # Simple logging\n",
    "            print(f\"Epoch {current_epoch}, Batch {batch_idx}/{len(dataloader)}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch {current_epoch} Average Loss: {avg_loss:.4f}\")\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T12:55:18.767487Z",
     "iopub.status.busy": "2025-05-21T12:55:18.767254Z",
     "iopub.status.idle": "2025-05-21T12:55:18.787511Z",
     "shell.execute_reply": "2025-05-21T12:55:18.786939Z",
     "shell.execute_reply.started": "2025-05-21T12:55:18.767468Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- Dummy Model and Data for Illustration ---\n",
    "class SimpleModelWithLoRA(nn.Module):\n",
    "    def __init__(self, vocab_size=100, dim=32, lora_rank_val=4): # Renamed lora_rank to lora_rank_val\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, dim)\n",
    "        self.linear1 = nn.Linear(dim, dim * 2) # Non-LoRA\n",
    "        self.relu = nn.ReLU()\n",
    "        # This nn.Linear will be replaced by FlexLoRA\n",
    "        self.lora_target_linear = nn.Linear(dim * 2, dim) \n",
    "        self.output_head = nn.Linear(dim, vocab_size) # Non-LoRA\n",
    "        \n",
    "        # Apply LoRA to self.lora_target_linear\n",
    "        # The lora_inject_model function is from Grit.Infrastructure\n",
    "        # It replaces the nn.Linear module named 'lora_target_linear' with a FlexLoRA module.\n",
    "        # The FlexLoRA module will then contain the original nn.Linear as its `base_layer`.\n",
    "        lora_config = {\n",
    "            # Substring matching for names like \"lora_target_linear\"\n",
    "            'layers_to_adapt_substrings': ['lora_target_linear'], \n",
    "            'rank_range': (lora_rank_val, lora_rank_val), # Fixed rank for simplicity\n",
    "            # 'lora_alpha' is not explicitly in FlexLoRA init in Infrastructure.py, but can be added if needed by FlexLoRA\n",
    "        }\n",
    "        # lora_inject_model is expected to modify the model in-place.\n",
    "        # It searches for modules whose names contain substrings from 'layers_to_adapt_substrings'\n",
    "        # and replaces them with FlexLoRA instances.\n",
    "        lora_inject(self, **lora_config)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.mean(dim=1) \n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.lora_target_linear(x) # This is now the FlexLoRA wrapped layer\n",
    "        x = self.output_head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T12:55:18.788472Z",
     "iopub.status.busy": "2025-05-21T12:55:18.788259Z",
     "iopub.status.idle": "2025-05-21T12:55:18.994651Z",
     "shell.execute_reply": "2025-05-21T12:55:18.994071Z",
     "shell.execute_reply.started": "2025-05-21T12:55:18.788458Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Model structure after LoRA injection:\n",
      "SimpleModelWithLoRA(\n",
      "  (embedding): Embedding(100, 32)\n",
      "  (linear1): Linear(in_features=32, out_features=64, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (lora_target_linear): FlexLoRA(initial_rank=4, current_rank=4, min_rank=4, max_rank=4, base_layer=Linear(64x32))\n",
      "  (output_head): Linear(in_features=32, out_features=100, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# --- Setup ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "vocab_size_const = 100\n",
    "dim_const = 32\n",
    "lora_rank_const = 4 # For FlexLoRA\n",
    "batch_size_const = 8\n",
    "seq_len_const = 10\n",
    "\n",
    "# Instantiate model\n",
    "model_instance = SimpleModelWithLoRA(vocab_size_const, dim_const, lora_rank_const).to(device)\n",
    "print(\"Model structure after LoRA injection:\")\n",
    "print(model_instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T12:55:18.995602Z",
     "iopub.status.busy": "2025-05-21T12:55:18.995370Z",
     "iopub.status.idle": "2025-05-21T12:55:19.003326Z",
     "shell.execute_reply": "2025-05-21T12:55:19.002656Z",
     "shell.execute_reply.started": "2025-05-21T12:55:18.995578Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found FlexLoRA layer: lora_target_linear with rank 4\n",
      "  Base layer of lora_target_linear: Linear(in_features=64, out_features=32, bias=True)\n"
     ]
    }
   ],
   "source": [
    "# --- Verify LoRA injection and find FlexLoRA layers for later checks ---\n",
    "found_flexlora = False\n",
    "for name, module in model_instance.named_modules():\n",
    "    if isinstance(module, FlexLoRA):\n",
    "        print(f\"Found FlexLoRA layer: {name} with rank {module.current_rank}\")\n",
    "        found_flexlora = True\n",
    "        # Expected: `lora_target_linear` should be FlexLoRA\n",
    "        # Its `base_layer` should be the original nn.Linear(dim * 2, dim)\n",
    "        if hasattr(module, 'base_layer') and isinstance(module.base_layer, nn.Linear):\n",
    "            print(f\"  Base layer of {name}: {module.base_layer}\")\n",
    "        else:\n",
    "            print(f\"  Error: FlexLoRA layer {name} does not have a valid nn.Linear base_layer.\")\n",
    "if not found_flexlora:\n",
    "    print(\"Error: No FlexLoRA layers were injected into the model. Check lora_inject_model and config.\")\n",
    "    exit() # Stop if LoRA injection failed, as GRIT steps depend on it.\n",
    "\n",
    "\n",
    "# Dummy Dataloader\n",
    "dummy_input_data = torch.randint(0, vocab_size_const, (batch_size_const * 5, seq_len_const))\n",
    "dummy_labels_data = torch.randint(0, vocab_size_const, (batch_size_const * 5,)) # For CrossEntropyLoss\n",
    "dummy_dataset_instance = torch.utils.data.TensorDataset(dummy_input_data, dummy_labels_data)\n",
    "dataloader_instance = torch.utils.data.DataLoader(dummy_dataset_instance, batch_size=batch_size_const)\n",
    "\n",
    "# Loss and Optimizer\n",
    "# Optimizer for non-LoRA params and potentially base model if not frozen by FlexLoRA.\n",
    "# FlexLoRA freezes base layer, so this optimizer trains embedding, linear1, output_head.\n",
    "# LoRA params are updated by natural_gradient_step (or SGD fallback within it).\n",
    "criterion_instance = nn.CrossEntropyLoss()\n",
    "optimizer_instance = torch.optim.AdamW(\n",
    "    [p for p in model_instance.parameters() if p.requires_grad], # Should pick up non-LoRA params\n",
    "    lr=1e-3\n",
    ")\n",
    "\n",
    "# GRIT components\n",
    "momentum_fuser_instance = MomentumFusion(beta=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T12:55:19.007046Z",
     "iopub.status.busy": "2025-05-21T12:55:19.006826Z",
     "iopub.status.idle": "2025-05-21T12:55:20.117346Z",
     "shell.execute_reply": "2025-05-21T12:55:20.116747Z",
     "shell.execute_reply.started": "2025-05-21T12:55:19.007032Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Initializing SVTrackers and RankSchedulers for FlexLoRA layers ---\n",
      "  Initialized SVTracker and RankScheduler for FlexLoRA layer: lora_target_linear (initial_rank=4, min_rank=4, max_rank=4)\n",
      "Successfully initialized SVTrackers and RankSchedulers for 1 FlexLoRA layer(s).\n",
      "--- Starting GRIT Training for 2 epochs ---\n",
      "--- Epoch 1 ---\n",
      "Epoch 1, Batch 0/5, Loss: 4.6314\n",
      "Epoch 1 Average Loss: 4.6271\n",
      "--- Epoch 1 Completed. Avg Loss: 4.6271 ---\n",
      "--- Epoch 2 ---\n",
      "Epoch 2, Batch 0/5, Loss: 4.5951\n",
      "Epoch 2 Average Loss: 4.5996\n",
      "--- Epoch 2 Completed. Avg Loss: 4.5996 ---\n",
      "Illustrative GRIT training based on pipeline finished.\n"
     ]
    }
   ],
   "source": [
    "# --- Setup for SVTrackers and RankSchedulers ---\n",
    "# and model_instance is already created and populated with FlexLoRA layers.\n",
    "# FlexLoRA modules are expected to have 'lora_A', 'lora_B', 'current_rank', and 'max_rank' attributes.\n",
    "# The 'min_rank' attribute on FlexLoRA modules is optional; if not present, a default is used for RankScheduler.\n",
    "\n",
    "sv_trackers = {}\n",
    "rank_schedulers = {}\n",
    "\n",
    "# Configuration constants for SVTracker and RankScheduler\n",
    "# These values are illustrative and can be adjusted or moved to a central configuration.\n",
    "SV_TRACKER_WINDOW_SIZE_CONST = 100\n",
    "RANK_SCHEDULER_WARMUP_STEPS_CONST = 1000 # As per RankScheduler example\n",
    "RANK_SCHEDULER_EMA_ALPHA_CONST = 0.1    # As per RankScheduler example\n",
    "DEFAULT_MIN_RANK_FOR_SCHEDULER = 1      # Default min_rank if not on FlexLoRA module\n",
    "# Parameter for rank_adjustment logic (called inside grit_train_epoch)\n",
    "RANK_ADJUST_SV_HISTORY_MIN_SAMPLES_CONST = 5 \n",
    "\n",
    "print(\"--- Initializing SVTrackers and RankSchedulers for FlexLoRA layers ---\")\n",
    "flexlora_layers_configured_count = 0\n",
    "for name, module in model_instance.named_modules():\n",
    "    if isinstance(module, FlexLoRA): # FlexLoRA class must be in scope\n",
    "        # Validate that the module has the necessary attributes for SVTracker and RankScheduler\n",
    "        if not all(hasattr(module, attr) for attr in ['lora_A', 'lora_B']):\n",
    "            print(f\"  Skipping {name}: FlexLoRA layer is missing 'lora_A' or 'lora_B' attributes required by SVTracker.\")\n",
    "            continue\n",
    "        if not hasattr(module, 'max_rank'):\n",
    "            print(f\"  Skipping {name}: FlexLoRA layer is missing 'max_rank' attribute required by SVTracker and RankScheduler.\")\n",
    "            continue\n",
    "        if not hasattr(module, 'current_rank'): # Used as initial_rank for RankScheduler\n",
    "            print(f\"  Skipping {name}: FlexLoRA layer is missing 'current_rank' attribute for RankScheduler.\")\n",
    "            continue\n",
    "\n",
    "        # Determine min_rank for RankScheduler\n",
    "        # Uses module.min_rank if available, otherwise defaults to DEFAULT_MIN_RANK_FOR_SCHEDULER\n",
    "        min_rank_val = getattr(module, 'min_rank', DEFAULT_MIN_RANK_FOR_SCHEDULER)\n",
    "\n",
    "        # Ensure rank configuration is valid: min_rank <= current_rank (initial) <= max_rank\n",
    "        if not (min_rank_val <= module.current_rank <= module.max_rank):\n",
    "            print(f\"  Skipping {name}: Invalid rank configuration. \"\n",
    "                  f\"Condition min_rank ({min_rank_val}) <= initial_rank ({module.current_rank}) \"\n",
    "                  f\"<= max_rank ({module.max_rank}) is not met.\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            sv_trackers[name] = SVTracker(flex_lora_layer=module, window_size=SV_TRACKER_WINDOW_SIZE_CONST)\n",
    "            rank_schedulers[name] = RankScheduler(\n",
    "                initial_rank=module.current_rank,\n",
    "                min_rank=min_rank_val,\n",
    "                max_rank=module.max_rank,\n",
    "                warmup_steps=RANK_SCHEDULER_WARMUP_STEPS_CONST,\n",
    "                ema_alpha=RANK_SCHEDULER_EMA_ALPHA_CONST\n",
    "            )\n",
    "            print(f\"  Initialized SVTracker and RankScheduler for FlexLoRA layer: {name} \"\n",
    "                  f\"(initial_rank={module.current_rank}, min_rank={min_rank_val}, max_rank={module.max_rank})\")\n",
    "            flexlora_layers_configured_count += 1\n",
    "        except ValueError as e:\n",
    "            print(f\"  Error initializing SVTracker or RankScheduler for {name}: {e}\")\n",
    "        except Exception as e: # Catch any other unexpected errors during init\n",
    "            print(f\"  Unexpected error initializing for {name}: {e}\")\n",
    "\n",
    "\n",
    "if flexlora_layers_configured_count == 0:\n",
    "    print(\"Warning: No FlexLoRA layers were successfully configured with SVTrackers and RankSchedulers. \"\n",
    "          \"Dynamic rank adjustment might not function as expected.\")\n",
    "else:\n",
    "    print(f\"Successfully initialized SVTrackers and RankSchedulers for {flexlora_layers_configured_count} FlexLoRA layer(s).\")\n",
    "\n",
    "\n",
    "# --- Training Loop ---\n",
    "num_epochs_const = 2\n",
    "print(f\"--- Starting GRIT Training for {num_epochs_const} epochs ---\")\n",
    "for epoch_num in range(num_epochs_const):\n",
    "    print(f\"--- Epoch {epoch_num + 1} ---\")\n",
    "    avg_epoch_loss_val = grit_train_epoch(\n",
    "        model=model_instance,\n",
    "        dataloader=dataloader_instance,\n",
    "        criterion=criterion_instance,\n",
    "        optimizer=optimizer_instance,\n",
    "        fusion_module=momentum_fuser_instance,\n",
    "        device=device,\n",
    "        current_epoch=epoch_num + 1,\n",
    "        lr_natural_gradient=5e-4, \n",
    "        damping_lora_fisher=1e-5, # Or your desired value for LoRA Fisher damping\n",
    "        reprojection_rho_eigen_sum=0.9, \n",
    "        spectral_constraint_sv_max=1.5,\n",
    "        # --- New parameters for dynamic rank adjustment ---\n",
    "        sv_trackers=sv_trackers,\n",
    "        rank_schedulers=rank_schedulers,\n",
    "        rank_adjust_sv_history_min_samples=RANK_ADJUST_SV_HISTORY_MIN_SAMPLES_CONST\n",
    "    )\n",
    "    print(f\"--- Epoch {epoch_num + 1} Completed. Avg Loss: {avg_epoch_loss_val:.4f} ---\")\n",
    "\n",
    "print(\"Illustrative GRIT training based on pipeline finished.\")\n",
    "# Further steps: Save model, evaluate, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRIT Stage 3: Specialized Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T12:55:20.118182Z",
     "iopub.status.busy": "2025-05-21T12:55:20.118008Z",
     "iopub.status.idle": "2025-05-21T12:55:20.121944Z",
     "shell.execute_reply": "2025-05-21T12:55:20.121266Z",
     "shell.execute_reply.started": "2025-05-21T12:55:20.118167Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "# Assume FlexLoRA is available from Grit.Infrastructure\n",
    "# from Grit.Infrastructure import FlexLoRA "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Dynamic Rank Adjustment System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T12:55:20.122893Z",
     "iopub.status.busy": "2025-05-21T12:55:20.122606Z",
     "iopub.status.idle": "2025-05-21T12:55:20.136399Z",
     "shell.execute_reply": "2025-05-21T12:55:20.135724Z",
     "shell.execute_reply.started": "2025-05-21T12:55:20.122865Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# class SVTracker(nn.Module):\n",
    "#     \"\"\"\n",
    "#     Tracks the history of singular values for a FlexLoRA layer's effective matrix (lora_A @ lora_B).\n",
    "#     Assumes the associated FlexLoRA layer has `lora_A`, `lora_B`, and `max_rank` attributes.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, flex_lora_layer, window_size: int = 100):\n",
    "#         super().__init__()\n",
    "#         if not hasattr(flex_lora_layer, 'lora_A') or not hasattr(flex_lora_layer, 'lora_B'):\n",
    "#             raise ValueError(\"Provided layer must have 'lora_A' and 'lora_B' parameters.\")\n",
    "#         if not hasattr(flex_lora_layer, 'max_rank'):\n",
    "#             raise ValueError(\"Provided FlexLoRA layer must have a 'max_rank' attribute.\")\n",
    "            \n",
    "#         self.flex_lora_layer = flex_lora_layer\n",
    "#         self.window_size = window_size\n",
    "        \n",
    "#         self.register_buffer('sv_history', \n",
    "#                              torch.zeros(window_size, flex_lora_layer.max_rank, \n",
    "#                                          device=flex_lora_layer.lora_A.device))\n",
    "\n",
    "#     def forward(self):\n",
    "#         \"\"\"Calculates SVD of lora_A @ lora_B and updates history.\"\"\"\n",
    "#         with torch.no_grad():\n",
    "#             lora_A_data = self.flex_lora_layer.lora_A.data\n",
    "#             lora_B_data = self.flex_lora_layer.lora_B.data\n",
    "#             effective_matrix = lora_A_data @ lora_B_data\n",
    "            \n",
    "#             try:\n",
    "#                 # SVD: U, S, Vh = svd(A). S are singular values.\n",
    "#                 S = torch.linalg.svdvals(effective_matrix)\n",
    "#             except Exception as e:\n",
    "#                 # print(f\"SVD failed in SVTracker: {e}. Skipping history update.\")\n",
    "#                 return\n",
    "\n",
    "#             # Pad or truncate S to match self.flex_lora_layer.max_rank for consistent history storage\n",
    "#             num_singular_values = S.shape[0]\n",
    "#             max_r = self.flex_lora_layer.max_rank\n",
    "            \n",
    "#             s_padded = torch.zeros(max_r, device=S.device)\n",
    "#             if num_singular_values > 0:\n",
    "#                 s_padded[:min(num_singular_values, max_r)] = S[:min(num_singular_values, max_r)]\n",
    "            \n",
    "#             self.sv_history = torch.roll(self.sv_history, shifts=1, dims=0)\n",
    "#             self.sv_history[0, :] = s_padded.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T12:55:20.137165Z",
     "iopub.status.busy": "2025-05-21T12:55:20.136974Z",
     "iopub.status.idle": "2025-05-21T12:55:20.154594Z",
     "shell.execute_reply": "2025-05-21T12:55:20.153910Z",
     "shell.execute_reply.started": "2025-05-21T12:55:20.137142Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# class RankScheduler:\n",
    "#     def __init__(self, initial_rank: int, min_rank: int, max_rank: int, # Added min/max rank from FlexLoRA\n",
    "#                  warmup_steps: int = 1000, ema_alpha: float = 0.1): # Added ema_alpha for sv_ema\n",
    "#         self.current_rank = initial_rank\n",
    "#         self.min_rank = min_rank # Store min_rank\n",
    "#         self.max_rank = max_rank # Store max_rank\n",
    "#         self.warmup_steps = warmup_steps\n",
    "#         self.step_counter = 0\n",
    "#         self.ema_alpha = ema_alpha # For smoothing sv_ratio\n",
    "#         self.sv_ema = None # EMA of singular value ratio\n",
    "\n",
    "#     def update(self, sv_ratio: float): # sv_ratio is sigma_1/sigma_r from rank_adjustment's perspective\n",
    "#         self.step_counter += 1\n",
    "        \n",
    "#         if self.sv_ema is None:\n",
    "#             self.sv_ema = sv_ratio\n",
    "#         else:\n",
    "#             self.sv_ema = self.ema_alpha * sv_ratio + (1 - self.ema_alpha) * self.sv_ema\n",
    "\n",
    "#         if self.step_counter < self.warmup_steps:\n",
    "#             return self.current_rank # Keep current rank during warmup\n",
    "\n",
    "#         # Logic:\n",
    "#         # if self.step_counter > 0.8: # Typo: self.step_counter > 0.8 ? Should be sv_ema\n",
    "#         # Assuming the condition is based on self.sv_ema\n",
    "#         if self.sv_ema is not None: # Check if sv_ema is initialized\n",
    "#             if self.sv_ema < 0.5: # If ratio is small, indicates potential for rank reduction\n",
    "#                 # Decrease rank, but not below min_rank\n",
    "#                 self.current_rank = max(self.min_rank, self.current_rank - 2) \n",
    "#             elif self.sv_ema > 0.8: # If ratio is large, potential for rank increase\n",
    "#                 # Increase rank, but not above max_rank\n",
    "#                 self.current_rank = min(self.max_rank, self.current_rank + 2)\n",
    "        \n",
    "#         return self.current_rank\n",
    "    \n",
    "#     def get_rank(self):\n",
    "#         return self.current_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T12:55:20.155574Z",
     "iopub.status.busy": "2025-05-21T12:55:20.155343Z",
     "iopub.status.idle": "2025-05-21T12:55:20.172516Z",
     "shell.execute_reply": "2025-05-21T12:55:20.171843Z",
     "shell.execute_reply.started": "2025-05-21T12:55:20.155554Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# def rank_adjustment(\n",
    "#     layer: FlexLoRA, \n",
    "#     sv_tracker: SVTracker, \n",
    "#     rank_scheduler: RankScheduler, # Pass the scheduler instance for this layer\n",
    "#     # rank_eta, rank_tau are removed as scheduler now handles logic\n",
    "#     sv_history_min_samples: int = 5 \n",
    "#     ):\n",
    "#     \"\"\"\n",
    "#     Adjusts the rank of a FlexLoRA layer using an external RankScheduler\n",
    "#     based on its singular value history (sigma_1/sigma_r ratio from mean_sv).\n",
    "#     \"\"\"\n",
    "#     if not all(hasattr(layer, attr) for attr in \n",
    "#                ['current_rank', 'min_rank', 'max_rank', 'lora_A', 'lora_B']):\n",
    "#         raise ValueError(\"FlexLoRA layer is missing required attributes for rank adjustment.\")\n",
    "\n",
    "#     if sv_tracker.sv_history[:, 0].count_nonzero() < sv_history_min_samples :\n",
    "#         # print(f\"Warning: Not enough singular value history for layer. Skipping rank adjustment. Need {sv_history_min_samples} samples.\")\n",
    "#         return\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         mean_sv = sv_tracker.sv_history.mean(dim=0) \n",
    "        \n",
    "#         current_rank_val = layer.current_rank # Use a different name to avoid conflict with scheduler's current_rank\n",
    "#         if current_rank_val == 0 or current_rank_val > layer.max_rank: # current_rank_val == 0 means LoRA is inactive or uninitialized for adjustment\n",
    "#             return\n",
    "        \n",
    "#         if mean_sv.numel() == 0 or current_rank_val > mean_sv.shape[0]:\n",
    "#             # print(\"Warning: Mean singular values not available or current rank exceeds available SVs. Skipping rank adjustment.\")\n",
    "#             return\n",
    "\n",
    "#         sigma_1 = mean_sv[0].item()\n",
    "#         sigma_r = mean_sv[current_rank_val - 1].item() if current_rank_val > 0 else 0.0\n",
    "\n",
    "#         if sigma_1 < 1e-9 or sigma_r < 1e-9: # Avoid division by zero or unstable ratios\n",
    "#             # print(f\"Warning: sigma_1 ({sigma_1}) or sigma_r ({sigma_r}) is near zero. Skipping rank update based on sv_ratio.\")\n",
    "#             return \n",
    "        \n",
    "#         sv_ratio_metric = sigma_1 / sigma_r\n",
    "        \n",
    "#         new_rank = rank_scheduler.update(sv_ratio_metric) \n",
    "#         new_rank = int(max(layer.min_rank, min(new_rank, layer.max_rank)))\n",
    "\n",
    "\n",
    "#         if new_rank == current_rank_val:\n",
    "#             return\n",
    "        \n",
    "#         old_lora_A = layer.lora_A.data\n",
    "#         old_lora_B = layer.lora_B.data\n",
    "#         device = old_lora_A.device\n",
    "\n",
    "#         # Preserve weights using SVD of W_lora = old_lora_A @ old_lora_B\n",
    "#         # This is generally more robust than padding/truncating A and B directly.\n",
    "#         W_lora = old_lora_A @ old_lora_B\n",
    "#         try:\n",
    "#             U, S, Vh = torch.linalg.svd(W_lora)\n",
    "#         except Exception as e:\n",
    "#             print(f\"SVD failed during rank adjustment for layer {type(layer).__name__}: {e}. Rank not changed.\")\n",
    "#             return\n",
    "\n",
    "#         # Determine the effective rank for SVD reconstruction.\n",
    "#         # This rank cannot exceed the number of singular values, S.shape[0] (min of W_lora dimensions).\n",
    "#         # layer.max_rank should ideally be configured to be <= S.shape[0].\n",
    "#         effective_rank_for_svd = new_rank\n",
    "#         if new_rank > S.shape[0]:\n",
    "#             print(f\"Warning: Target new_rank {new_rank} exceeds available singular values {S.shape[0]} for layer {type(layer).__name__}. \"\n",
    "#                   f\"Capping rank to {S.shape[0]}. Consider adjusting layer.max_rank.\")\n",
    "#             effective_rank_for_svd = S.shape[0]\n",
    "#             new_rank = effective_rank_for_svd # Update new_rank to the capped value for consistency\n",
    "\n",
    "#         if new_rank == current_rank_val and effective_rank_for_svd == new_rank : # Check again if capping resulted in no change\n",
    "#              return\n",
    "\n",
    "\n",
    "#         # Reconstruct LoRA matrices A' and B' such that A'B' approximates W_lora.\n",
    "#         # A_new = U[:, :k] @ diag(sqrt(S[:k]))\n",
    "#         # B_new = diag(sqrt(S[:k])) @ Vh[:k, :]\n",
    "#         # This distributes singular values (sqrt) to both A and B.\n",
    "        \n",
    "#         # Handle case where effective_rank_for_svd is 0 (e.g. if min_rank is 0 and it's chosen)\n",
    "#         if effective_rank_for_svd == 0:\n",
    "#             new_lora_A_data = torch.empty((old_lora_A.shape[0], 0), device=device, dtype=old_lora_A.dtype)\n",
    "#             new_lora_B_data = torch.empty((0, old_lora_B.shape[1]), device=device, dtype=old_lora_B.dtype)\n",
    "#         else:\n",
    "#             U_k = U[:, :effective_rank_for_svd]\n",
    "#             S_k = S[:effective_rank_for_svd]\n",
    "#             Vh_k = Vh[:effective_rank_for_svd, :]\n",
    "\n",
    "#             # Singular values (S_k) are non-negative. torch.sqrt(0) is 0.\n",
    "#             sqrt_S_k = torch.sqrt(S_k)\n",
    "#             diag_sqrt_S_k = torch.diag(sqrt_S_k)\n",
    "\n",
    "#             new_lora_A_data = U_k @ diag_sqrt_S_k\n",
    "#             new_lora_B_data = diag_sqrt_S_k @ Vh_k\n",
    "        \n",
    "#         layer.lora_A = nn.Parameter(new_lora_A_data.to(device))\n",
    "#         layer.lora_B = nn.Parameter(new_lora_B_data.to(device))\n",
    "#         layer.current_rank = new_rank # new_rank might have been capped by S.shape[0]\n",
    "#         rank_scheduler.current_rank = new_rank # Also update scheduler's internal current_rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Stability Assurance Mechanisms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T12:55:20.173382Z",
     "iopub.status.busy": "2025-05-21T12:55:20.173149Z",
     "iopub.status.idle": "2025-05-21T12:55:20.190421Z",
     "shell.execute_reply": "2025-05-21T12:55:20.189775Z",
     "shell.execute_reply.started": "2025-05-21T12:55:20.173367Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def fisher_clip(\n",
    "    gradient: torch.Tensor, \n",
    "    inv_fisher_factor_A: torch.Tensor, # Inverse of activation covariance (A_inv)\n",
    "    inv_fisher_factor_B: torch.Tensor, # Inverse of output gradient covariance (B_inv)\n",
    "    clip_norm: float = 1.0\n",
    "    ) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Clips a gradient in the Fisher-approximated natural gradient space.\n",
    "    Conceptually, natural_grad = F_inv @ grad.flatten(), where F_inv = kron(inv_B, inv_A).\n",
    "    This function computes it efficiently as vec(inv_B @ grad @ inv_A.T).\n",
    "    \"\"\"\n",
    "    if gradient is None:\n",
    "        return None\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        natural_grad_matrix = inv_fisher_factor_B @ gradient @ inv_fisher_factor_A\n",
    "        natural_grad_flat = natural_grad_matrix.flatten()\n",
    "        \n",
    "        grad_norm_fisher_space = torch.norm(natural_grad_flat)\n",
    "        \n",
    "        if grad_norm_fisher_space > clip_norm:\n",
    "            scale = clip_norm / (grad_norm_fisher_space + 1e-7) # Add epsilon for stability\n",
    "            return gradient * scale\n",
    "        return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T12:55:20.191257Z",
     "iopub.status.busy": "2025-05-21T12:55:20.191037Z",
     "iopub.status.idle": "2025-05-21T12:55:20.205740Z",
     "shell.execute_reply": "2025-05-21T12:55:20.205114Z",
     "shell.execute_reply.started": "2025-05-21T12:55:20.191243Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# class DampedKFAC(nn.Module):\n",
    "#     # Assuming 'd,d' were placeholders for feature dimensions\n",
    "#     def __init__(self, in_features: int, out_features: int, ema_decay: float = 0.95, damping: tuple[float, float] = (1e-7, 1e-5)):\n",
    "#         super().__init__()\n",
    "#         self.ema_decay = ema_decay\n",
    "#         self.damping = damping # Tuple (damping_A, damping_B)\n",
    "        \n",
    "#         # Using register_buffer for A_accum and B_accum\n",
    "#         self.register_buffer('A_accum', torch.zeros(in_features, in_features))\n",
    "#         self.register_buffer('B_accum', torch.zeros(out_features, out_features))\n",
    "#         self.A_initialized = False\n",
    "#         self.B_initialized = False\n",
    "\n",
    "#     def update(self, current_A_cov: torch.Tensor, current_B_cov: torch.Tensor = None):\n",
    "#         \"\"\"\n",
    "#         Update A_accum or B_accum with new covariance.\n",
    "#         If current_B_cov is None, only A_accum is updated.\n",
    "#         If current_A_cov is None, only B_accum is updated.\n",
    "#         \"\"\"\n",
    "#         with torch.no_grad():\n",
    "#             if current_A_cov is not None:\n",
    "#                 if not self.A_initialized:\n",
    "#                     self.A_accum.copy_(current_A_cov)\n",
    "#                     self.A_initialized = True\n",
    "#                 else:\n",
    "#                     self.A_accum.mul_(self.ema_decay).add_(current_A_cov, alpha=(1 - self.ema_decay))\n",
    "            \n",
    "#             if current_B_cov is not None:\n",
    "#                 if not self.B_initialized:\n",
    "#                     self.B_accum.copy_(current_B_cov)\n",
    "#                     self.B_initialized = True\n",
    "#                 else:\n",
    "#                     self.B_accum.mul_(self.ema_decay).add_(current_B_cov, alpha=(1 - self.ema_decay))\n",
    "\n",
    "#     def get_factors(self) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "#         \"\"\"Returns the current A_accum and B_accum factors.\"\"\"\n",
    "#         return self.A_accum, self.B_accum\n",
    "\n",
    "#     def get_inverse_factors(self) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "#         \"\"\"Computes and returns damped inverses (A_inv, B_inv).\"\"\"\n",
    "#         with torch.no_grad():\n",
    "#             device = self.A_accum.device\n",
    "            \n",
    "#             A_damped = self.A_accum + self.damping[0] * torch.eye(self.A_accum.size(0), device=device)\n",
    "#             B_damped = self.B_accum + self.damping[1] * torch.eye(self.B_accum.size(0), device=device)\n",
    "            \n",
    "#             try:\n",
    "#                 A_inv = torch.linalg.pinv(A_damped)\n",
    "#             except Exception as e:\n",
    "#                 # print(f\"Error inverting A_damped: {e}. Returning identity.\")\n",
    "#                 A_inv = torch.eye(self.A_accum.size(0), device=device)\n",
    "            \n",
    "#             try:\n",
    "#                 B_inv = torch.linalg.pinv(B_damped)\n",
    "#             except Exception as e:\n",
    "#                 # print(f\"Error inverting B_damped: {e}. Returning identity.\")\n",
    "#                 B_inv = torch.eye(self.B_accum.size(0), device=device)\n",
    "                \n",
    "#             return A_inv, B_inv\n",
    "\n",
    "#     def __repr__(self):\n",
    "#         return (f\"DampedKFAC(in={self.A_accum.shape[0]}, out={self.B_accum.shape[0]}, \"\n",
    "#                 f\"ema_decay={self.ema_decay}, damping={self.damping}, \"\n",
    "#                 f\"A_init={self.A_initialized}, B_init={self.B_initialized})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T12:55:20.206503Z",
     "iopub.status.busy": "2025-05-21T12:55:20.206324Z",
     "iopub.status.idle": "2025-05-21T12:55:20.223488Z",
     "shell.execute_reply": "2025-05-21T12:55:20.222844Z",
     "shell.execute_reply.started": "2025-05-21T12:55:20.206489Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# NaturalGradientClipper\n",
    "class NaturalGradientClipper:\n",
    "    def __init__(self, max_norm: float = 1.0):\n",
    "        self.max_norm = max_norm\n",
    "\n",
    "    def __call__(self, natural_grad: torch.Tensor) -> torch.Tensor:\n",
    "        norm = torch.norm(natural_grad)\n",
    "        if norm > self.max_norm:\n",
    "            return natural_grad * (self.max_norm / (norm + 1e-7)) # Added epsilon for stability\n",
    "        return natural_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Memory Optimization Framework\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T12:55:20.224450Z",
     "iopub.status.busy": "2025-05-21T12:55:20.224156Z",
     "iopub.status.idle": "2025-05-21T12:55:20.241865Z",
     "shell.execute_reply": "2025-05-21T12:55:20.241151Z",
     "shell.execute_reply.started": "2025-05-21T12:55:20.224427Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def block_svd(matrix: torch.Tensor, block_size: int = 256) -> tuple[list, list, list]:\n",
    "    \"\"\"\n",
    "    Performs SVD on blocks of a matrix. Assumes square blocks.\n",
    "    Returns lists of U_blocks, S_value_blocks, Vh_blocks (transposed V).\n",
    "    \"\"\"\n",
    "    U_blocks, S_blocks, Vh_blocks = [], [], []\n",
    "    block_size_rows, block_size_cols = block_size, block_size # Assuming square blocks from signature\n",
    "    \n",
    "    for i in range(0, matrix.size(0), block_size_rows):\n",
    "        for j in range(0, matrix.size(1), block_size_cols):\n",
    "            block = matrix[i:i+block_size_rows, j:j+block_size_cols].clone()\n",
    "            if block.numel() == 0: continue\n",
    "            try:\n",
    "                U, S, Vh = torch.linalg.svd(block) # linalg.svd returns Vh (V.T)\n",
    "                U_blocks.append(U)\n",
    "                S_blocks.append(S)\n",
    "                Vh_blocks.append(Vh) # Storing Vh as is consistent with linalg.svd\n",
    "            except Exception as e:\n",
    "                # print(f\"SVD failed for block ({i},{j}) of size {block.shape}: {e}\")\n",
    "                U_blocks.append(torch.empty(0,0, device=matrix.device))\n",
    "                S_blocks.append(torch.empty(0, device=matrix.device))\n",
    "                Vh_blocks.append(torch.empty(0,0, device=matrix.device))\n",
    "    return U_blocks, S_blocks, Vh_blocks\n",
    "\n",
    "class HalfPrecisionFisher(nn.Module):\n",
    "    \"\"\"\n",
    "    Manages KFAC A and B factors stored in half-precision (float16).\n",
    "    EMA updates are performed with care for precision.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features: int, out_features: int, ema_decay: float = 0.95):\n",
    "        super().__init__()\n",
    "        self.ema_decay = ema_decay # 0.95 decay\n",
    "        \n",
    "        self.register_buffer('A_half', torch.zeros(in_features, in_features, dtype=torch.float16))\n",
    "        self.register_buffer('B_half', torch.zeros(out_features, out_features, dtype=torch.float16))\n",
    "\n",
    "    def update(self, current_A_cov: torch.Tensor, current_B_cov: torch.Tensor):\n",
    "        \"\"\"Update with raw (float32) covariances from current batch.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            new_A_f32 = self.A_half.float() * self.ema_decay + \\\n",
    "                        current_A_cov.half().float() * (1 - self.ema_decay)\n",
    "            self.A_half = new_A_f32.half()\n",
    "\n",
    "            new_B_f32 = self.B_half.float() * self.ema_decay + \\\n",
    "                        current_B_cov.half().float() * (1 - self.ema_decay)\n",
    "            self.B_half = new_B_f32.half()\n",
    "    \n",
    "    def get_factors_f32(self) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Returns factors cast to float32 for use.\"\"\"\n",
    "        return self.A_half.float(), self.B_half.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T12:55:20.242928Z",
     "iopub.status.busy": "2025-05-21T12:55:20.242665Z",
     "iopub.status.idle": "2025-05-21T12:55:20.272336Z",
     "shell.execute_reply": "2025-05-21T12:55:20.271628Z",
     "shell.execute_reply.started": "2025-05-21T12:55:20.242907Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- GRIT Stage 3: Specialized Components --- (Illustrative Examples with Updated Names)\n",
      "\n",
      "1. Dynamic Rank Adjustment Example\n",
      "  SV History updated (step 0). Current rank: 8\n",
      "  SV History updated (step 5). Current rank: 8\n",
      "  SV History updated (step 10). Current rank: 8\n",
      "  SV History updated (step 15). Current rank: 8\n",
      "  Before rank adjustment: MockFlexLoRA(current_rank=8, min_rank=4, max_rank=12)\n",
      "  Created MockRankScheduler: initial_rank=8, min_rank=4, max_rank=12\n",
      "  After rank adjustment: MockFlexLoRA(current_rank=8, min_rank=4, max_rank=12), new lora_A shape: torch.Size([64, 8])\n"
     ]
    }
   ],
   "source": [
    "print(\"--- GRIT Stage 3: Specialized Components --- (Illustrative Examples with Updated Names)\")\n",
    "\n",
    "# Placeholder FlexLoRA for SVTracker and rank_adjustment demonstration\n",
    "class MockFlexLoRA(nn.Module):\n",
    "    def __init__(self, in_f, out_f, initial_rank=8, min_r=4, max_r=16, device='cpu'):\n",
    "        super().__init__()\n",
    "        self.in_features = in_f\n",
    "        self.out_features = out_f\n",
    "        # For FlexLoRA as updated, it would be:\n",
    "        self.initial_rank = initial_rank\n",
    "        self.current_rank = initial_rank\n",
    "        self.min_rank = min_r\n",
    "        self.max_rank = max_r\n",
    "        self.lora_A = nn.Parameter(torch.randn(in_f, self.current_rank, device=device) * 0.01)\n",
    "        self.lora_B = nn.Parameter(torch.zeros(self.current_rank, out_f, device=device))\n",
    "        self.device = device\n",
    "    def __repr__(self):\n",
    "        return f\"MockFlexLoRA(current_rank={self.current_rank}, min_rank={self.min_rank}, max_rank={self.max_rank})\"\n",
    "\n",
    "# 1. Dynamic Rank Adjustment Example\n",
    "print(\"\\n1. Dynamic Rank Adjustment Example\")\n",
    "# Ensure MockFlexLoRA matches the attributes FlexLoRA now has (initial_rank, current_rank, min_rank, max_rank)\n",
    "mock_layer_for_rank_adj = MockFlexLoRA(in_f=64, out_f=32, initial_rank=8, min_r=4, max_r=12, device='cpu')\n",
    "sv_tracker_instance = SVTracker(mock_layer_for_rank_adj, window_size=20)\n",
    "\n",
    "# Simulate some SV history updates by calling SVTracker's forward method\n",
    "for i in range(20):\n",
    "    # Ensure lora_A's rank matches current_rank if it were to change dynamically in a real scenario\n",
    "    if mock_layer_for_rank_adj.lora_A.shape[1] == mock_layer_for_rank_adj.current_rank:\n",
    "         mock_layer_for_rank_adj.lora_A.data += torch.randn_like(mock_layer_for_rank_adj.lora_A.data) * 0.001\n",
    "    # (Skipping lora_B changes for simplicity in this mock update)\n",
    "    sv_tracker_instance.forward() \n",
    "    if i % 5 == 0:\n",
    "        print(f\"  SV History updated (step {i}). Current rank: {mock_layer_for_rank_adj.current_rank}\")\n",
    "\n",
    "print(f\"  Before rank adjustment: {mock_layer_for_rank_adj}\")\n",
    "if sv_tracker_instance.sv_history.numel() > 0 and mock_layer_for_rank_adj.max_rank > 5:\n",
    "    # Forcing some singular values to be small in history for testing decay\n",
    "    # Ensure we don't index out of bounds if max_rank is small\n",
    "    if sv_tracker_instance.sv_history.shape[1] > 5 :\n",
    "        sv_tracker_instance.sv_history[:, 5:] *= 0.01 \n",
    "\n",
    "# (mock_layer_for_rank_adj and sv_tracker_instance are defined above in this cell)\n",
    "\n",
    "# Create a RankScheduler for the mock layer\n",
    "mock_rank_scheduler = RankScheduler(\n",
    "    initial_rank=mock_layer_for_rank_adj.current_rank,\n",
    "    min_rank=mock_layer_for_rank_adj.min_rank,\n",
    "    max_rank=mock_layer_for_rank_adj.max_rank,\n",
    "    warmup_steps=10, # Example value, adjust as needed\n",
    "    ema_alpha=0.1    # Example value, adjust as needed\n",
    ")\n",
    "print(f\"  Created MockRankScheduler: initial_rank={mock_rank_scheduler.current_rank}, min_rank={mock_rank_scheduler.min_rank}, max_rank={mock_rank_scheduler.max_rank}\")\n",
    "\n",
    "# Call rank_adjustment with new parameters\n",
    "rank_adjustment(\n",
    "    mock_layer_for_rank_adj, \n",
    "    sv_tracker_instance, \n",
    "    mock_rank_scheduler, # Pass the RankScheduler instance\n",
    "    sv_history_min_samples=5 # Example value\n",
    ")\n",
    "print(f\"  After rank adjustment: {mock_layer_for_rank_adj}, new lora_A shape: {mock_layer_for_rank_adj.lora_A.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T12:55:20.273284Z",
     "iopub.status.busy": "2025-05-21T12:55:20.273056Z",
     "iopub.status.idle": "2025-05-21T12:55:20.285614Z",
     "shell.execute_reply": "2025-05-21T12:55:20.285060Z",
     "shell.execute_reply.started": "2025-05-21T12:55:20.273262Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. Stability Assurance Example\n",
      "  DampedKFAC: A_inv shape torch.Size([64, 64]), B_inv shape torch.Size([32, 32])\n",
      "  Fisher Clip: Original grad norm 45.446922302246094, Clipped grad norm 0.48223498463630676\n",
      "  Fisher Clip: Original natural grad norm 47.1211, Clipped natural grad norm 0.5000\n"
     ]
    }
   ],
   "source": [
    "# 2. Stability Assurance Example\n",
    "print(\"\\n2. Stability Assurance Example\")\n",
    "# DampedKFAC (formerly DampedKroneckerFactors)\n",
    "damped_k_factors = DampedKFAC(in_features=64, out_features=32, damping=(1e-4, 1e-4)) # Pass damping as tuple\n",
    "dummy_A_cov = torch.rand(64, 64) * 0.1 + torch.eye(64) \n",
    "dummy_B_cov = torch.rand(32, 32) * 0.1 + torch.eye(32)\n",
    "dummy_A_cov = (dummy_A_cov + dummy_A_cov.T)/2\n",
    "dummy_B_cov = (dummy_B_cov + dummy_B_cov.T)/2\n",
    "damped_k_factors.update(dummy_A_cov, dummy_B_cov) # Call renamed update method\n",
    "inv_A, inv_B = damped_k_factors.get_inverse_factors() # Call renamed get_inverse_factors method\n",
    "print(f\"  DampedKFAC: A_inv shape {inv_A.shape}, B_inv shape {inv_B.shape}\")\n",
    "\n",
    "# fisher_clip (formerly fisher_clip_gradient)\n",
    "dummy_grad = torch.randn(32, 64) \n",
    "# fisher_clip expects inv_fisher_factor_A, inv_fisher_factor_B\n",
    "clipped_grad = fisher_clip(dummy_grad, inv_A, inv_B, clip_norm=0.5)\n",
    "print(f\"  Fisher Clip: Original grad norm {torch.norm(dummy_grad)}, Clipped grad norm {torch.norm(clipped_grad)}\")\n",
    "original_ng_norm = torch.norm(inv_B @ dummy_grad @ inv_A) # Natural grad norm\n",
    "clipped_ng_norm = torch.norm(inv_B @ clipped_grad @ inv_A)\n",
    "print(f\"  Fisher Clip: Original natural grad norm {original_ng_norm:.4f}, Clipped natural grad norm {clipped_ng_norm:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T12:55:20.286535Z",
     "iopub.status.busy": "2025-05-21T12:55:20.286384Z",
     "iopub.status.idle": "2025-05-21T12:55:20.327190Z",
     "shell.execute_reply": "2025-05-21T12:55:20.326656Z",
     "shell.execute_reply.started": "2025-05-21T12:55:20.286522Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. Memory Optimization Example\n",
      "  HalfPrecisionFisher: A_half dtype torch.float16, B_half dtype torch.float16\n",
      "  Retrieved factors (f32): A shape torch.Size([64, 64]), B shape torch.Size([32, 32])\n",
      "  Block SVD: Performed on torch.Size([512, 256]) matrix with block_size=128.\n",
      "    Got 8 U_blocks, 8 S_value_blocks, 8 Vh_blocks.\n",
      "    Example: First U_block shape torch.Size([128, 128]), First S_values_block shape torch.Size([128])\n",
      "\n",
      "--- End of Illustrative Examples (Updated Names) ---\n"
     ]
    }
   ],
   "source": [
    "# 3. Memory Optimization Example\n",
    "print(\"\\n3. Memory Optimization Example\")\n",
    "# HalfPrecisionFisher (formerly HalfPrecisionKroneckerFactors)\n",
    "hp_fisher_factors = HalfPrecisionFisher(in_features=64, out_features=32, ema_decay=0.95)\n",
    "hp_fisher_factors.update(dummy_A_cov, dummy_B_cov) # Call renamed update method\n",
    "hp_A_f32, hp_B_f32 = hp_fisher_factors.get_factors_f32()\n",
    "print(f\"  HalfPrecisionFisher: A_half dtype {hp_fisher_factors.A_half.dtype}, B_half dtype {hp_fisher_factors.B_half.dtype}\")\n",
    "print(f\"  Retrieved factors (f32): A shape {hp_A_f32.shape}, B shape {hp_B_f32.shape}\")\n",
    "\n",
    "# Block SVD (updated signature)\n",
    "large_matrix = torch.randn(512, 256)\n",
    "# block_svd now takes a single block_size argument, assuming square blocks or applying it to both dimensions\n",
    "# For non-square, one might pass different sizes, but current takes one.\n",
    "# Let's test with a size that divides one dim and not other to see behavior with current block_svd.\n",
    "# Or better, use a size that fits well like 128 for both if matrix is 512x256.\n",
    "# The implementation uses block_size for both block_size_rows and block_size_cols.\n",
    "U_blocks, S_val_blocks, Vh_blocks = block_svd(large_matrix, block_size=128) \n",
    "print(f\"  Block SVD: Performed on {large_matrix.shape} matrix with block_size=128.\")\n",
    "print(f\"    Got {len(U_blocks)} U_blocks, {len(S_val_blocks)} S_value_blocks, {len(Vh_blocks)} Vh_blocks.\")\n",
    "if U_blocks and U_blocks[0].numel() > 0 : # Check if first block is not empty\n",
    "    print(f\"    Example: First U_block shape {U_blocks[0].shape}, First S_values_block shape {S_val_blocks[0].shape if S_val_blocks[0].numel() > 0 else 'empty'}\")\n",
    "\n",
    "print(\"\\n--- End of Illustrative Examples (Updated Names) ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRIT Stage 4: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T12:55:20.327927Z",
     "iopub.status.busy": "2025-05-21T12:55:20.327751Z",
     "iopub.status.idle": "2025-05-21T12:55:20.332352Z",
     "shell.execute_reply": "2025-05-21T12:55:20.331652Z",
     "shell.execute_reply.started": "2025-05-21T12:55:20.327913Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import re\n",
    "import numpy as np\n",
    "import time # For ThroughputBenchmark\n",
    "\n",
    "# Placeholder for an evaluation function that returns a single performance score (higher is better)\n",
    "# This would need to be properly defined or imported based on the specific task and model.\n",
    "def evaluate_model(model: nn.Module, data_loader: torch.utils.data.DataLoader, device: torch.device) -> float:\n",
    "    \"\"\"\n",
    "    Placeholder for model evaluation.\n",
    "    Returns a single scalar performance metric (e.g., accuracy, F1-score).\n",
    "    Higher values should indicate better performance.\n",
    "    \"\"\"\n",
    "    print(f\"Warning: Using placeholder 'evaluate_model' in ForgettingMonitor. Please implement for actual metrics.\")\n",
    "    # Example: calculate loss, and return negative loss if higher is better\n",
    "    # model.eval()\n",
    "    # total_loss = 0\n",
    "    # num_batches = 0\n",
    "    # with torch.no_grad():\n",
    "    #     for batch in data_loader:\n",
    "    #         # Assuming batch format and loss calculation similar to a standard loop\n",
    "    #         # This part is highly dependent on your data and model specifics\n",
    "    #         # inputs, targets = batch[0].to(device), batch[1].to(device) \n",
    "    #         # outputs = model(inputs)\n",
    "    #         # loss = torch.nn.functional.cross_entropy(outputs, targets) # Example loss\n",
    "    #         # total_loss += loss.item()\n",
    "    #         num_batches += 1\n",
    "    # model.train()\n",
    "    # if num_batches == 0:\n",
    "    #     return 0.0\n",
    "    # return - (total_loss / num_batches) # Negative loss, so higher is better\n",
    "    return 0.5 # Dummy performance score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Convergence Metrics Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T12:55:20.333141Z",
     "iopub.status.busy": "2025-05-21T12:55:20.332966Z",
     "iopub.status.idle": "2025-05-21T12:55:20.353815Z",
     "shell.execute_reply": "2025-05-21T12:55:20.353250Z",
     "shell.execute_reply.started": "2025-05-21T12:55:20.333127Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CurvatureAnalyzer:\n",
    "    def __init__(self, model, damping=1e-3): # Model is the full model\n",
    "        self.model = model\n",
    "        self.damping = damping\n",
    "        # self.hessian_buffers = {} \n",
    "\n",
    "    def compute_layer_curvature(self, layer_with_factors): # layer should have A_factor, B_factor\n",
    "        # Assumes layer has A_factor and B_factor (e.g., from KFACLayer.kfac_manager.A_accum)\n",
    "        A_factor, B_factor = None, None\n",
    "        if hasattr(layer_with_factors, 'kfac_handler') and hasattr(layer_with_factors.kfac_handler, 'A_factor') and hasattr(layer_with_factors.kfac_handler, 'B_factor'):\n",
    "            A_factor = layer_with_factors.kfac_handler.A_factor\n",
    "            B_factor = layer_with_factors.kfac_handler.B_factor\n",
    "        elif hasattr(layer_with_factors, 'A_factor') and hasattr(layer_with_factors, 'B_factor'): # Direct attributes\n",
    "            A_factor = layer_with_factors.A_factor\n",
    "            B_factor = layer_with_factors.B_factor\n",
    "        \n",
    "        if A_factor is None or B_factor is None or A_factor.numel() == 0 or B_factor.numel() == 0:\n",
    "            # print(f\"compute_layer_curvature: Factors not found or empty for layer {type(layer_with_factors).__name__}\")\n",
    "            return None\n",
    "\n",
    "        A = A_factor + self.damping * torch.eye(A_factor.size(0), device=A_factor.device, dtype=A_factor.dtype)\n",
    "        B = B_factor + self.damping * torch.eye(B_factor.size(0), device=B_factor.device, dtype=B_factor.dtype)\n",
    "        \n",
    "        if A.shape[0] == 0 or B.shape[0] == 0 : return None # Cannot compute kron with empty dimension\n",
    "        return torch.kron(A, B) # In KFAC, this usually kron(B,A) for Fisher if A=act, B=grad_out\n",
    "    \n",
    "    def track_landscape(self, dataloader=None): # dataloader is not used in this version\n",
    "        eigenvalues = {}\n",
    "        for name, module_in_model in self.model.named_modules():\n",
    "            # Check if the module itself is what we are interested in (e.g. has kfac_handler)\n",
    "            # This logic depends on how KFAC factors are stored/accessed.\n",
    "            # Assuming KFAC factors are on the nn.Linear layer via 'kfac_handler' from instrument_layer\n",
    "            # or directly as A_factor, B_factor if KFACLayer itself is the module.\n",
    "            \n",
    "            # Try to get factors:\n",
    "            # This part is tricky because KFAC factors might be on module.kfac_handler (from KFACLayer)\n",
    "            # or if module_in_model *is* a KFACLayer instance (less likely from instrument_layer design).\n",
    "            # Let's assume we check for 'kfac_handler' which holds KFACLayer.\n",
    "            # The kfac_handler would then have A_factor and B_factor properties.\n",
    "            \n",
    "            layer_to_check_factors_on = module_in_model \n",
    "            # If FlexLoRA, we might want curvature of base_layer if it has KFAC\n",
    "            if isinstance(module_in_model, FlexLoRA) and hasattr(module_in_model.base_layer, 'kfac_handler'):\n",
    "                 layer_to_check_factors_on = module_in_model.base_layer\n",
    "\n",
    "\n",
    "            # Check if this layer_to_check_factors_on (e.g., an nn.Linear) has a KFAC handler\n",
    "            # and that handler has valid factors.\n",
    "            has_valid_factors = False\n",
    "            if hasattr(layer_to_check_factors_on, 'kfac_handler'):\n",
    "                handler = layer_to_check_factors_on.kfac_handler\n",
    "                if hasattr(handler, 'A_factor') and hasattr(handler, 'B_factor') and \\\n",
    "                   handler.A_factor is not None and handler.B_factor is not None and \\\n",
    "                   handler.A_factor.numel() > 0 and handler.B_factor.numel() > 0:\n",
    "                    has_valid_factors = True\n",
    "            \n",
    "            if has_valid_factors:\n",
    "                try:\n",
    "                    # Pass the module that has the kfac_handler (e.g. the nn.Linear)\n",
    "                    F = self.compute_layer_curvature(layer_to_check_factors_on) \n",
    "                    if F is not None and F.numel() > 0:\n",
    "                        # eigvalsh requires symmetric matrix. KFAC A,B should be, so F should be.\n",
    "                        eigvals = torch.linalg.eigvalsh(F) \n",
    "                        eigenvalues[name] = {\n",
    "                            'max': eigvals[-1].item(), # eigvalsh sorts ascending\n",
    "                            'min': eigvals[0].item(),\n",
    "                            'trace': eigvals.sum().item()\n",
    "                        }\n",
    "                    else:\n",
    "                        eigenvalues[name] = {'max': float('nan'), 'min': float('nan'), 'trace': float('nan')}\n",
    "                except Exception as e:\n",
    "                    # print(f\"Could not compute eigenvalues for layer {name}: {e}\")\n",
    "                    eigenvalues[name] = {'max': float('nan'), 'min': float('nan'), 'trace': float('nan')}\n",
    "        return eigenvalues\n",
    "\n",
    "def fisher_rao_distance(P: torch.Tensor, Q: torch.Tensor, epsilon=1e-7) -> float: # Adjusted epsilon \n",
    "    # P, Q are data matrices (samples x features)\n",
    "    if P.ndim != 2 or Q.ndim != 2 or P.shape[1] != Q.shape[1]:\n",
    "        # print(\"Fisher-Rao: P and Q must be 2D tensors with the same number of features.\")\n",
    "        return float('nan')\n",
    "    if P.shape[0] <= 1 or Q.shape[0] <= 1 or P.shape[1] == 0 : # Need >1 sample for cov, and non-zero features\n",
    "        # print(\"Fisher-Rao: Need >1 sample per distribution and non-zero features.\")\n",
    "        return float('nan')\n",
    "\n",
    "    device = P.device\n",
    "    \n",
    "    try:\n",
    "        # Covariance matrices\n",
    "        cov_p = torch.cov(P.T) + epsilon * torch.eye(P.size(1), device=device, dtype=P.dtype)\n",
    "        cov_q = torch.cov(Q.T) + epsilon * torch.eye(Q.size(1), device=device, dtype=Q.dtype)\n",
    "        \n",
    "        # Check for PSD and invertibility if using matrix_power(0.5) for sqrt\n",
    "        # Using cholesky decomposition for matrix sqrt: L L^T = M\n",
    "        # Or, eigenvalue decomposition: M = V diag(lambda) V^T => M^0.5 = V diag(sqrt(lambda)) V^T\n",
    "        # torch.linalg.matrix_power requires PSD for fractional powers.\n",
    "        # Let's use direct formula with matrix_power and trust input epsilon handles it.\n",
    "\n",
    "        # Ensure symmetry for matrix_power if needed (cov should be symmetric)\n",
    "        # cov_p = (cov_p + cov_p.T) / 2\n",
    "        # cov_q = (cov_q + cov_q.T) / 2\n",
    "\n",
    "        sqrt_p = torch.linalg.matrix_power(cov_p, 0.5)\n",
    "        \n",
    "        # cross_term = sqrt_p @ cov_q @ sqrt_p\n",
    "        # Corrected cross_term stabilization from original for matrix_power\n",
    "        cross_term_inner = sqrt_p @ cov_q @ sqrt_p\n",
    "        # Stabilize for matrix_power by ensuring it's symmetric and adding epsilon if needed\n",
    "        cross_term_inner_stabilized = (cross_term_inner + cross_term_inner.T) / 2 \n",
    "        cross_term_inner_stabilized = cross_term_inner_stabilized + epsilon * torch.eye(cross_term_inner_stabilized.size(0), device=device, dtype=P.dtype)\n",
    "        \n",
    "        sqrt_cross_term = torch.linalg.matrix_power(cross_term_inner_stabilized, 0.5)\n",
    "        \n",
    "        distance = torch.trace(cov_p + cov_q - 2 * sqrt_cross_term).item()\n",
    "        \n",
    "        # Distance should be non-negative. If it's slightly negative due to numerical precision, clamp to 0.\n",
    "        return max(0.0, distance)\n",
    "    except Exception as e:\n",
    "        # print(f\"Error in Fisher-Rao distance computation: {e}. Returning NaN.\")\n",
    "        return float('nan')\n",
    "\n",
    "\n",
    "def subspace_alignment(source_eigenvectors: torch.Tensor, target_eigenvectors: torch.Tensor) -> dict:\n",
    "    # source_eigenvectors and target_eigenvectors are matrices where columns are eigenvectors\n",
    "    # Shape: (feature_dim, num_eigenvectors)\n",
    "    if source_eigenvectors.numel() == 0 or target_eigenvectors.numel() == 0 or \\\n",
    "       source_eigenvectors.ndim != 2 or target_eigenvectors.ndim != 2 or \\\n",
    "       source_eigenvectors.shape[0] != target_eigenvectors.shape[0]: # Feature dim must match\n",
    "        # print(\"Subspace Alignment: Invalid input tensors. Check dims and feature size.\")\n",
    "        return {'mean_angle': float('nan'), 'max_angle': float('nan'), 'min_angle': float('nan')}\n",
    "\n",
    "    try:\n",
    "        # Orthonormalize bases if they are not already (e.g. if they are just SVD U matrices)\n",
    "        # Assuming they are already orthonormal bases (e.g. U from SVD)\n",
    "        # U_s = torch.linalg.qr(source_eigenvectors).Q\n",
    "        # U_t = torch.linalg.qr(target_eigenvectors).Q\n",
    "        U_s = source_eigenvectors\n",
    "        U_t = target_eigenvectors\n",
    "        \n",
    "        # k = min(U_s.shape[1], U_t.shape[1]) # Number of vectors to compare\n",
    "        # torch.diag(U_s.T @ U_t) implies comparing corresponding vectors\n",
    "        # if number of vectors (subspace dim) is the same.\n",
    "        # For principal angles between subspaces of potentially different dimensions k_s, k_t:\n",
    "        # We compute SVD of U_s^T @ U_t. The singular values are cosines of principal angles.\n",
    "        \n",
    "        # If U_s and U_t have different number of columns (vectors), U_s.T @ U_t is not square.\n",
    "        # Taking diag of non-square matrix extracts main diagonal.\n",
    "        # To get principal angles correctly:\n",
    "        correlation_matrix = U_s.T @ U_t\n",
    "        singular_values_of_corr = torch.linalg.svdvals(correlation_matrix) # These are cos(theta_i)\n",
    "        \n",
    "        # Ensure values are within [-1, 1] for acos\n",
    "        cos_angles = torch.clamp(singular_values_of_corr, -1.0 + 1e-7, 1.0 - 1e-7)\n",
    "        principal_angles_rad = torch.acos(cos_angles)\n",
    "        \n",
    "        if principal_angles_rad.numel() == 0: # No angles computed\n",
    "             return {'mean_angle': float('nan'), 'max_angle': float('nan'), 'min_angle': float('nan')}\n",
    "\n",
    "        return {\n",
    "            'mean_angle': torch.mean(principal_angles_rad).item(),\n",
    "            'max_angle': torch.max(principal_angles_rad).item(),\n",
    "            'min_angle': torch.min(principal_angles_rad).item()\n",
    "        }\n",
    "    except Exception as e:\n",
    "        # print(f\"Error in subspace alignment: {e}. Returning NaNs.\")\n",
    "        return {'mean_angle': float('nan'), 'max_angle': float('nan'), 'min_angle': float('nan')}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Quality Assessment Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T12:55:20.354551Z",
     "iopub.status.busy": "2025-05-21T12:55:20.354387Z",
     "iopub.status.idle": "2025-05-21T12:55:20.369801Z",
     "shell.execute_reply": "2025-05-21T12:55:20.369164Z",
     "shell.execute_reply.started": "2025-05-21T12:55:20.354539Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class InstructionValidator:\n",
    "    def __init__(self, criteria_mapping: dict): # str_criterion -> regex_pattern\n",
    "        self.criteria = criteria_mapping\n",
    "\n",
    "    # validate_single will be used by the main validate method\n",
    "    def _validate_single(self, instruction: str, response: str) -> float:\n",
    "        if not self.criteria:\n",
    "            return 1.0\n",
    "\n",
    "        scores = {}\n",
    "        for criterion, pattern in self.criteria.items():\n",
    "            try:\n",
    "                matches = re.findall(pattern, response, re.IGNORECASE)\n",
    "                scores[criterion] = len(matches) > 0\n",
    "            except Exception as e:\n",
    "                print(f\"Regex error for criterion '{criterion}' with pattern '{pattern}': {e}\")\n",
    "                scores[criterion] = False\n",
    "\n",
    "        if not scores: \n",
    "            return 0.0\n",
    "            \n",
    "        return sum(scores.values()) / len(scores)\n",
    "\n",
    "    # grit_evaluation_loop calls validate(test_samples). Assuming test_samples is a list.\n",
    "    def validate(self, test_samples: list[tuple[str, str]]) -> float:\n",
    "        \"\"\"\n",
    "        Validates a list of (instruction, response) pairs.\n",
    "        \"\"\"\n",
    "        if not test_samples:\n",
    "            return 0.0\n",
    "        \n",
    "        all_scores = []\n",
    "        for instruction, response in test_samples:\n",
    "            all_scores.append(self._validate_single(instruction, response))\n",
    "        \n",
    "        return np.mean(all_scores) if all_scores else 0.0\n",
    "\n",
    "def domain_shift_score(source_feats: torch.Tensor, target_feats: torch.Tensor) -> float:\n",
    "    if source_feats.numel() == 0 or target_feats.numel() == 0:\n",
    "        return float('nan') # Keep check for empty tensors\n",
    "    source_mean = source_feats.mean(dim=0)\n",
    "    target_mean = target_feats.mean(dim=0)\n",
    "    return torch.norm(source_mean - target_mean, p=2).item()\n",
    "\n",
    "\n",
    "class ForgettingMonitor:\n",
    "    # Using existing robust version that stores model instance, not state_dict\n",
    "    def __init__(self, original_model_instance: nn.Module):\n",
    "        self.original_model = original_model_instance\n",
    "        # Ensure original_model is in eval mode for consistent evaluation\n",
    "        self.original_model.eval()\n",
    "\n",
    "\n",
    "    def compute_forgetting(self, tuned_model: nn.Module, val_loader: torch.utils.data.DataLoader, device: torch.device) -> float:\n",
    "        # Ensure original_model is on the correct device\n",
    "        self.original_model.to(device)\n",
    "        \n",
    "        # tuned_model should also be in eval mode and on correct device\n",
    "        tuned_model.eval().to(device)\n",
    "\n",
    "        original_perf = evaluate_model(self.original_model, val_loader, device)\n",
    "        tuned_perf = evaluate_model(tuned_model, val_loader, device)\n",
    "        \n",
    "        # Restore tuned_model's mode if it was changed (evaluate_model should ideally handle this)\n",
    "        # For simplicity, assuming evaluate_model doesn't change it, or it's fine.\n",
    "\n",
    "        if original_perf is None or tuned_perf is None: # Check if evaluate_model failed\n",
    "             print(\"Warning: ForgettingMonitor received None performance. Returning NaN.\")\n",
    "             return float('nan')\n",
    "\n",
    "        if original_perf == 0: # Avoid division by zero\n",
    "            if tuned_perf < 0: return float('inf') # Performance dropped from zero to negative\n",
    "            if tuned_perf == 0: return 0.0 # No change from zero\n",
    "            return -float('inf') # Performance improved from zero (negative forgetting is improvement)\n",
    "        \n",
    "        return (original_perf - tuned_perf) / abs(original_perf) # Use abs for safety if perf can be negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Efficiency Benchmarking System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T12:55:20.370522Z",
     "iopub.status.busy": "2025-05-21T12:55:20.370348Z",
     "iopub.status.idle": "2025-05-21T12:55:20.387868Z",
     "shell.execute_reply": "2025-05-21T12:55:20.387393Z",
     "shell.execute_reply.started": "2025-05-21T12:55:20.370508Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def update_density(model: nn.Module) -> float:\n",
    "    total_params = 0\n",
    "    updated_lora_param_sum = 0 # Sum of numel for LoRA params considered updated\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad and 'lora' in name: # Consider only trainable LoRA parameters\n",
    "            if param.grad is not None:\n",
    "                updates_norm = torch.norm(param.grad.detach(), p=2)\n",
    "                # Density interpretation: 1.0 if param's gradient norm is significant, 0.0 otherwise\n",
    "                is_updated_significantly = updates_norm > 1e-6\n",
    "                if is_updated_significantly:\n",
    "                    updated_lora_param_sum += param.numel()\n",
    "            # else: if no grad, not updated\n",
    "        \n",
    "        # total_params includes ALL parameters in the model\n",
    "        total_params += param.numel()\n",
    "\n",
    "    if total_params == 0:\n",
    "        return 0.0\n",
    "    return updated_lora_param_sum / total_params\n",
    "\n",
    "\n",
    "# MemoryTracker\n",
    "class MemoryTracker:\n",
    "    def __init__(self):\n",
    "        self.peak_mem = 0 # Stores peak memory in bytes\n",
    "        # Reset CUDA peak stats at the start of tracking for this instance for more predictable one-shot track()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.reset_peak_memory_stats() \n",
    "            self.peak_mem = torch.cuda.max_memory_allocated() # Initial peak\n",
    "\n",
    "    def track(self) -> float: # Returns peak memory in GB\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize() # Ensure all ops are done before reading memory\n",
    "            # max_memory_allocated gives peak since last reset.\n",
    "            # Logic wants to track max over calls IF instance is reused.\n",
    "            # For single call MemoryTracker().track(), this is fine.\n",
    "            self.peak_mem = max(self.peak_mem, torch.cuda.max_memory_allocated())\n",
    "        return self.peak_mem / (1024 ** 3) # Return in GB\n",
    "\n",
    "\n",
    "# ThroughputBenchmark: Existing implementation is robust and matches core logic.\n",
    "class ThroughputBenchmark:\n",
    "    def __init__(self, batch_size: int):\n",
    "        self.batch_size = batch_size\n",
    "        self.timings = [] # Stores time per step\n",
    "\n",
    "    def log_step(self, start_time: float, end_time: float): # Parameters named start_time, end_time\n",
    "        self.timings.append(end_time - start_time)\n",
    "\n",
    "    def compute_throughput(self) -> float: # Samples/second\n",
    "        if not self.timings:\n",
    "            return 0.0\n",
    "        avg_time_per_step = np.mean(self.timings)\n",
    "        if avg_time_per_step == 0: # Avoid division by zero\n",
    "            return float('inf') if self.batch_size > 0 else 0.0\n",
    "        return self.batch_size / avg_time_per_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T12:55:20.388695Z",
     "iopub.status.busy": "2025-05-21T12:55:20.388527Z",
     "iopub.status.idle": "2025-05-21T12:55:20.405368Z",
     "shell.execute_reply": "2025-05-21T12:55:20.404809Z",
     "shell.execute_reply.started": "2025-05-21T12:55:20.388681Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "### Evaluation Protocol Implementation\n",
    "\n",
    "def grit_evaluation_loop(\n",
    "    model: nn.Module,\n",
    "    original_model: nn.Module, # For forgetting monitor\n",
    "    train_loader: torch.utils.data.DataLoader, \n",
    "    test_loader: torch.utils.data.DataLoader,\n",
    "    domains: list[tuple[torch.Tensor, torch.Tensor]], # List of (source_features, target_features)\n",
    "    pretrained_subspace: torch.Tensor, # For subspace alignment\n",
    "    criteria: dict, # For InstructionValidator\n",
    "    test_samples: list[tuple[str, str]], # For InstructionValidator: [(instr, model_resp), ...]\n",
    "    device: torch.device,\n",
    "    curvature_damping: float = 1e-3, # Added for CurvatureAnalyzer\n",
    "    fisher_rao_epsilon: float = 1e-6 # Added for fisher_rao_distance\n",
    "    ):\n",
    "\n",
    "    results = {}\n",
    "    model.eval() # Ensure model is in evaluation mode\n",
    "\n",
    "    # --- Convergence Metrics ---\n",
    "    print(\"Calculating Convergence Metrics...\")\n",
    "    \n",
    "    # 1.1 Loss Curvature Tracking\n",
    "    # The dataloader arg for track_landscape is not used by the CurvatureAnalyzer logic.\n",
    "    curvature_analyzer = CurvatureAnalyzer(model, damping=curvature_damping)\n",
    "    # track_landscape dataloader arg is for potential future use, not current.\n",
    "    curvature_data_per_layer = curvature_analyzer.track_landscape(dataloader=train_loader) \n",
    "    \n",
    "    max_overall_curvature = float('-inf')\n",
    "    min_overall_curvature = float('inf')\n",
    "    avg_trace_curvature = 0.0 # Initialize as float\n",
    "    num_layers_for_curvature = 0\n",
    "\n",
    "    if curvature_data_per_layer:\n",
    "        for layer_name, data in curvature_data_per_layer.items():\n",
    "            if data and not np.isnan(data['max']): # Check for NaN\n",
    "                max_overall_curvature = max(max_overall_curvature, data['max'])\n",
    "            if data and not np.isnan(data['min']):\n",
    "                min_overall_curvature = min(min_overall_curvature, data['min'])\n",
    "            if data and not np.isnan(data['trace']):\n",
    "                avg_trace_curvature += data['trace']\n",
    "                num_layers_for_curvature +=1\n",
    "        if num_layers_for_curvature > 0 :\n",
    "             avg_trace_curvature /= num_layers_for_curvature\n",
    "        else:\n",
    "             avg_trace_curvature = float('nan') # No valid layers found\n",
    "    \n",
    "    # Handle cases where no curvature data was computed\n",
    "    final_max_curvature = max_overall_curvature if max_overall_curvature != float('-inf') else float('nan')\n",
    "    final_min_curvature = min_overall_curvature if min_overall_curvature != float('inf') else float('nan')\n",
    "\n",
    "\n",
    "    # 1.2 Fisher-Rao Distance\n",
    "    fr_distance = float('nan') # Default to NaN\n",
    "    try:\n",
    "        # Robustly get features from dataset object within loader\n",
    "        train_features = getattr(train_loader.dataset, 'features', torch.empty(0, device=device))\n",
    "        test_features = getattr(test_loader.dataset, 'features', torch.empty(0, device=device))\n",
    "        if train_features.numel() > 0 and test_features.numel() > 0:\n",
    "            fr_distance = fisher_rao_distance(train_features.to(device), test_features.to(device), epsilon=fisher_rao_epsilon)\n",
    "        else:\n",
    "            print(\"Warning: Could not extract features for Fisher-Rao distance. Set to NaN.\")\n",
    "    except AttributeError:\n",
    "        print(\"Warning: '.features' attribute not found on datasets for Fisher-Rao. Set to NaN.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating Fisher-Rao distance: {e}. Set to NaN.\")\n",
    "\n",
    "    # 1.3 Subspace Alignment\n",
    "    # Assuming model has an attribute 'subspace' (e.g., extracted LoRA parameters as a matrix)\n",
    "    model_current_subspace = getattr(model, 'subspace', torch.empty(0, device=device))\n",
    "    sa_metrics = subspace_alignment(model_current_subspace.to(device), pretrained_subspace.to(device))\n",
    "\n",
    "    results['convergence'] = {\n",
    "        'max_curvature': final_max_curvature, # Aggregated max\n",
    "        'min_curvature': final_min_curvature, # Aggregated min \n",
    "        'avg_trace_curvature': avg_trace_curvature, # Aggregated avg trace\n",
    "        'fisher_rao': fr_distance,\n",
    "        'subspace_alignment': sa_metrics # sa_metrics is already a dict {'mean_angle', 'max_angle', 'min_angle'}\n",
    "    }\n",
    "\n",
    "    # --- Quality Assessment ---\n",
    "    print(\"Calculating Quality Assessment Metrics...\")\n",
    "    # 2.1 Instruction Following Accuracy\n",
    "    instruction_validator = InstructionValidator(criteria)\n",
    "    instr_acc = instruction_validator.validate(test_samples) # test_samples is list of (instr, resp)\n",
    "\n",
    "    # 2.2 Domain Adaptation Metrics\n",
    "    domain_adaptation_scores = []\n",
    "    if domains: # Check if domains list is not empty\n",
    "        for src_feats, tgt_feats in domains:\n",
    "            if src_feats.numel() > 0 and tgt_feats.numel() > 0:\n",
    "                domain_adaptation_scores.append(domain_shift_score(src_feats.to(device), tgt_feats.to(device)))\n",
    "            else:\n",
    "                domain_adaptation_scores.append(float('nan')) # Handle empty features per domain\n",
    "    else: # No domains provided\n",
    "        domain_adaptation_scores = [float('nan')]\n",
    "\n",
    "\n",
    "    # 2.3 Catastrophic Forgetting Tests\n",
    "    forgetting_monitor = ForgettingMonitor(original_model) # original_model passed as arg\n",
    "    forgetting_rate = forgetting_monitor.compute_forgetting(model, test_loader, device)\n",
    "    \n",
    "    results['quality'] = {\n",
    "        'instruction_acc': instr_acc,\n",
    "        'domain_scores': domain_adaptation_scores, \n",
    "        'forgetting_rate': forgetting_rate\n",
    "    }\n",
    "\n",
    "    # --- Efficiency Benchmarks ---\n",
    "    print(\"Calculating Efficiency Benchmarks...\")\n",
    "    # 3.1 Parameter Update Density\n",
    "    # update_density assumes gradients are present (e.g., after a backward pass if called during training)\n",
    "    # For a pure evaluation script, gradients might be zero or None.\n",
    "    # We'll call it, but it might return 0 if no grads.\n",
    "    param_density = update_density(model) \n",
    "\n",
    "    # 3.2 Memory Consumption\n",
    "    mem_tracker = MemoryTracker() # Instantiated here\n",
    "    peak_memory_gb = mem_tracker.track() \n",
    "\n",
    "    # 3.3 Training Throughput\n",
    "    # This is tricky in a standalone eval loop unless timings were previously logged\n",
    "    # or this loop itself performs timed operations.\n",
    "    # ThroughputBenchmark().compute_throughput() implies an existing instance or pre-logged steps.\n",
    "    # For a dummy call as in existing code:\n",
    "    throughput_analyzer = ThroughputBenchmark(batch_size=getattr(test_loader, 'batch_size', 1))\n",
    "    # Example: If this loop were to run some inferences, log them here.\n",
    "    # For now, it will return 0 or inf if no log_step calls.\n",
    "    # if hasattr(test_loader, 'batch_size'):\n",
    "    #    dummy_start_time = time.time()\n",
    "    #    # Simulate some work, e.g., one pass over test_loader for timing\n",
    "    #    # with torch.no_grad():\n",
    "    #    #     for _ in test_loader: pass \n",
    "    #    # throughput_analyzer.log_step(dummy_start_time, time.time())\n",
    "    # else:\n",
    "    #    print(\"Warning: test_loader has no batch_size, throughput may be 0 or incorrect.\")\n",
    "    \n",
    "    # result dict asks for 'samples_per_sec', which is what compute_throughput returns.\n",
    "    # 'throughput' variable name from snippet, key 'samples_per_sec' in output.\n",
    "    throughput_val = throughput_analyzer.compute_throughput()\n",
    "\n",
    "\n",
    "    results['efficiency'] = {\n",
    "        'update_density': param_density,\n",
    "        'memory_gb': peak_memory_gb,\n",
    "        'samples_per_sec': throughput_val\n",
    "    }\n",
    "    \n",
    "    model.train() # Restore model to train mode if it was changed\n",
    "    print(\"GRIT Evaluation Loop Finished.\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T12:55:20.406238Z",
     "iopub.status.busy": "2025-05-21T12:55:20.405987Z",
     "iopub.status.idle": "2025-05-21T12:55:20.449756Z",
     "shell.execute_reply": "2025-05-21T12:55:20.449239Z",
     "shell.execute_reply.started": "2025-05-21T12:55:20.406215Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running basic example for grit_evaluation_loop (with dummy data)...\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "print(\"Running basic example for grit_evaluation_loop (with dummy data)...\")\n",
    "\n",
    "# Dummy model setup (ensure KFAC factors A_factor, B_factor for CurvatureAnalyzer)\n",
    "class DummyLayerWithKFAC(nn.Module):\n",
    "    def __init__(self, in_f, out_f, device='cpu'):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_f, out_f)\n",
    "        # Simulate KFAC factors being present and initialized (non-empty)\n",
    "        self.register_buffer('A_factor', torch.eye(in_f, device=device) * 0.1 + torch.rand(in_f, in_f, device=device)*0.01)\n",
    "        self.register_buffer('B_factor', torch.eye(out_f, device=device) * 0.1 + torch.rand(out_f, out_f, device=device)*0.01)\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "class DummyModel(nn.Module):\n",
    "    def __init__(self, device='cpu'):\n",
    "        super().__init__()\n",
    "        self.layer1_kfac = DummyLayerWithKFAC(10, 20, device=device)\n",
    "        self.layer2_no_kfac = nn.Linear(20, 5) \n",
    "        self.layer3_kfac = DummyLayerWithKFAC(5,2, device=device)\n",
    "        # Dummy model subspace for subspace_alignment (e.g., flattened params of some layers)\n",
    "        # Shape: (param_dim, num_principal_components_or_rank)\n",
    "        self.subspace = torch.randn(10 * 20, 4, device=device) \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.layer1_kfac(x))\n",
    "        x = self.layer2_no_kfac(x)\n",
    "        x = self.layer3_kfac(x)\n",
    "        return x\n",
    "\n",
    "current_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {current_device}\")\n",
    "\n",
    "model_to_eval = DummyModel(device=current_device).to(current_device)\n",
    "original_model_for_forgetting = DummyModel(device=current_device).to(current_device) # Fresh instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T12:55:20.450558Z",
     "iopub.status.busy": "2025-05-21T12:55:20.450378Z",
     "iopub.status.idle": "2025-05-21T12:55:20.472512Z",
     "shell.execute_reply": "2025-05-21T12:55:20.471844Z",
     "shell.execute_reply.started": "2025-05-21T12:55:20.450544Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Dummy data and loaders\n",
    "class DummyDatasetWithFeatures(torch.utils.data.Dataset):\n",
    "    def __init__(self, num_samples, num_features, num_classes, device='cpu'):\n",
    "        self.num_samples = num_samples\n",
    "        # Simulate features for Fisher-Rao and domain shift\n",
    "        self.features = torch.randn(num_samples, num_features, device=device)\n",
    "        self.labels = torch.randint(0, num_classes, (num_samples,), device=device)\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    def __getitem__(self, idx):\n",
    "        # Dataloader will collate these. For model input, usually just features[idx]\n",
    "        return self.features[idx], self.labels[idx] \n",
    "\n",
    "train_dataset_inst = DummyDatasetWithFeatures(100, 10, 5, device=current_device)\n",
    "test_dataset_inst = DummyDatasetWithFeatures(50, 10, 5, device=current_device)\n",
    "# Ensure dummy model's input layer matches feature dim (10)\n",
    "train_loader_inst = torch.utils.data.DataLoader(train_dataset_inst, batch_size=10)\n",
    "test_loader_inst = torch.utils.data.DataLoader(test_dataset_inst, batch_size=10)\n",
    "\n",
    "# Dummy parameters for the grit_evaluation_loop\n",
    "dummy_domain_data = [\n",
    "    (torch.randn(50, 10, device=current_device), torch.randn(50, 10, device=current_device)), \n",
    "    (torch.randn(30, 10, device=current_device), torch.randn(30, 10, device=current_device))\n",
    "]\n",
    "# Pretrained subspace: e.g., SVD of initial LoRA weights or some reference subspace\n",
    "# Shape should match model.subspace for alignment. (param_dim, num_vectors)\n",
    "dummy_pretrained_model_subspace = torch.randn(10 * 20, 4, device=current_device) \n",
    "\n",
    "dummy_instruction_criteria = {\n",
    "    \"completeness\": r\"task is complete|done|finished\",\n",
    "    \"politeness\": r\"please|thank you\"\n",
    "}\n",
    "dummy_test_instruction_samples = [\n",
    "    (\"Instruction: Be polite and complete task.\", \"Response: The task is complete, thank you!\"),\n",
    "    (\"Instruction: Summarize.\", \"Response: Summary done.\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T12:55:20.473395Z",
     "iopub.status.busy": "2025-05-21T12:55:20.473181Z",
     "iopub.status.idle": "2025-05-21T12:55:20.621678Z",
     "shell.execute_reply": "2025-05-21T12:55:20.621046Z",
     "shell.execute_reply.started": "2025-05-21T12:55:20.473381Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Convergence Metrics...\n",
      "Calculating Quality Assessment Metrics...\n",
      "Warning: Using placeholder 'evaluate_model' in ForgettingMonitor. Please implement for actual metrics.\n",
      "Warning: Using placeholder 'evaluate_model' in ForgettingMonitor. Please implement for actual metrics.\n",
      "Calculating Efficiency Benchmarks...\n",
      "GRIT Evaluation Loop Finished.\n"
     ]
    }
   ],
   "source": [
    "# Simulate some gradients for update_density (e.g. from a backward pass)\n",
    "# This is artificial for a standalone eval script. In real use, grads would be from training.\n",
    "for name, param in model_to_eval.named_parameters():\n",
    "    if 'lora' in name and param.requires_grad: # Assuming some LoRA params might exist if model was LoRA-fied\n",
    "        param.grad = torch.rand_like(param) * 0.01\n",
    "    # For non-LoRA params, grads could also exist. update_density only looks at 'lora'.\n",
    "    # Let's make a KFAC layer's linear weight have \"lora\" in its name for testing update_density\n",
    "    # This is a hack for the dummy example.\n",
    "if hasattr(model_to_eval, 'layer1_kfac') and hasattr(model_to_eval.layer1_kfac, 'linear'):\n",
    "    # Give it a grad\n",
    "    model_to_eval.layer1_kfac.linear.weight.grad = torch.rand_like(model_to_eval.layer1_kfac.linear.weight) * 0.01\n",
    "    # Temporarily rename for update_density to pick up (VERY HACKY FOR DEMO)\n",
    "    # In a real scenario, LoRA layers would be named appropriately.\n",
    "    # This part is tricky for a generic dummy example without actual LoRA layers.\n",
    "    # We'll rely on 'lora' being in the name if we want update_density > 0.\n",
    "    # For now, let's assume no LoRA params by default in this DummyModel to avoid complex renaming.\n",
    "    # So, update_density will likely be 0 unless the model is modified to have 'lora' in names.\n",
    "    pass\n",
    "\n",
    "\n",
    "# Call the updated evaluation loop\n",
    "eval_results_output = grit_evaluation_loop(\n",
    "    model=model_to_eval,\n",
    "    original_model=original_model_for_forgetting,\n",
    "    train_loader=train_loader_inst,\n",
    "    test_loader=test_loader_inst,\n",
    "    domains=dummy_domain_data,\n",
    "    pretrained_subspace=dummy_pretrained_model_subspace,\n",
    "    criteria=dummy_instruction_criteria,\n",
    "    test_samples=dummy_test_instruction_samples,\n",
    "    device=current_device,\n",
    "    curvature_damping=1e-4, # example override\n",
    "    fisher_rao_epsilon=1e-5 # example override\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T12:55:20.622676Z",
     "iopub.status.busy": "2025-05-21T12:55:20.622447Z",
     "iopub.status.idle": "2025-05-21T12:55:20.627647Z",
     "shell.execute_reply": "2025-05-21T12:55:20.626966Z",
     "shell.execute_reply.started": "2025-05-21T12:55:20.622658Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n--- GRIT Evaluation Results (Dummy Run) ---\n",
      "{\n",
      "  \"convergence\": {\n",
      "    \"max_curvature\": NaN,\n",
      "    \"min_curvature\": NaN,\n",
      "    \"avg_trace_curvature\": 0.0,\n",
      "    \"fisher_rao\": NaN,\n",
      "    \"subspace_alignment\": {\n",
      "      \"mean_angle\": 0.00048828125,\n",
      "      \"max_angle\": 0.00048828125,\n",
      "      \"min_angle\": 0.00048828125\n",
      "    }\n",
      "  },\n",
      "  \"quality\": {\n",
      "    \"instruction_acc\": 0.75,\n",
      "    \"domain_scores\": [\n",
      "      0.7748472690582275,\n",
      "      0.5999931693077087\n",
      "    ],\n",
      "    \"forgetting_rate\": 0.0\n",
      "  },\n",
      "  \"efficiency\": {\n",
      "    \"update_density\": 0.0,\n",
      "    \"memory_gb\": 0.0865321159362793,\n",
      "    \"samples_per_sec\": 0.0\n",
      "  }\n",
      "}\n",
      "--- End of Dummy Run ---\n"
     ]
    }
   ],
   "source": [
    "print(\"\\\\n--- GRIT Evaluation Results (Dummy Run) ---\")\n",
    "import json\n",
    "# Custom handler for NaN/Inf float values for JSON serialization\n",
    "def json_default_handler(x):\n",
    "    if isinstance(x, float) and (np.isnan(x) or np.isinf(x)):\n",
    "        return str(x)\n",
    "    if isinstance(x, torch.Tensor): # Should not happen if .item() is used\n",
    "         return str(x.tolist()) if x.numel() > 1 else str(x.item())\n",
    "    return x\n",
    "\n",
    "print(json.dumps(eval_results_output, indent=2, default=json_default_handler))\n",
    "print(\"--- End of Dummy Run ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRIT Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class GRITAdapter(nn.Module):\n",
    "    def __init__(self, model: nn.Module,\n",
    "                 layers_to_adapt_substrings: list[str],\n",
    "                 rank_range: tuple[int, int] = (8, 64),\n",
    "                 kfac_ema_decay: float = 0.95,\n",
    "                 kfac_damping: tuple[float, float] = (1e-7, 1e-5),\n",
    "                 fisher_k: float = 0.1, # Fraction for eigen decomposition\n",
    "                 subspace_buffer_size: int = 1024,\n",
    "                 # Rank scheduling params\n",
    "                 rank_scheduler_warmup_steps: int = 1000,\n",
    "                 rank_scheduler_ema_alpha: float = 0.1,\n",
    "                 sv_tracker_window_size: int = 100,\n",
    "                 sv_history_min_samples: int = 5,\n",
    "                 # Neural reprojection params\n",
    "                 rho_eigen_sum: float = 0.9,\n",
    "                 # Parameter fusion params\n",
    "                 momentum_fusion_beta: float = 0.9,\n",
    "                 # Update params\n",
    "                 learning_rate: float = 0.001,\n",
    "                 damping_lora_fisher: float = 1e-6\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.layers_to_adapt_substrings = layers_to_adapt_substrings\n",
    "        self.rank_range = rank_range\n",
    "        self.kfac_ema_decay = kfac_ema_decay\n",
    "        self.kfac_damping = kfac_damping\n",
    "        self.fisher_k = fisher_k\n",
    "        self.subspace_buffer_size = subspace_buffer_size\n",
    "        self.rank_scheduler_warmup_steps = rank_scheduler_warmup_steps\n",
    "        self.rank_scheduler_ema_alpha = rank_scheduler_ema_alpha\n",
    "        self.sv_tracker_window_size = sv_tracker_window_size\n",
    "        self.sv_history_min_samples = sv_history_min_samples\n",
    "        self.rho_eigen_sum = rho_eigen_sum\n",
    "        self.momentum_fusion_beta = momentum_fusion_beta\n",
    "        self.learning_rate = learning_rate\n",
    "        self.damping_lora_fisher = damping_lora_fisher\n",
    "\n",
    "        self.all_kfac_handlers = []\n",
    "        self.fisher_engines = {} # Store FisherEigen instances per layer/block\n",
    "        self.subspace_manager = None # Will be initialized after KFAC instrumentation\n",
    "        self.sv_trackers = {} # For FlexLoRA layers\n",
    "        self.rank_schedulers = {} # For FlexLoRA layers\n",
    "        self.momentum_fuser = MomentumFusion(beta=self.momentum_fusion_beta)\n",
    "\n",
    "        self._initialize_grit()\n",
    "\n",
    "    def _initialize_grit(self):\n",
    "        # 1. Inject LoRA layers\n",
    "        lora_inject(self.model, self.layers_to_adapt_substrings, self.rank_range)\n",
    "\n",
    "        # 2. Instrument KFAC\n",
    "        # KFAC instrumentation might depend on the structure of the base model.\n",
    "        # This part might need to be more generic or configurable.\n",
    "        \n",
    "        model_blocks_attr_names = ['trf_blocks', 'blocks'] # Common names for transformer blocks\n",
    "        actual_blocks_attr_name = None\n",
    "        for attr_name in model_blocks_attr_names:\n",
    "            if hasattr(self.model, attr_name) and isinstance(getattr(self.model, attr_name), nn.ModuleList):\n",
    "                actual_blocks_attr_name = attr_name\n",
    "                break\n",
    "\n",
    "        if actual_blocks_attr_name:\n",
    "            model_blocks = getattr(self.model, actual_blocks_attr_name)\n",
    "            for i, block in enumerate(model_blocks):\n",
    "                # Ensure instrument_layer can find layers like 'attn.q_proj' or 'mlp' within the block\n",
    "                # This might require block to have conventional sub-module names or instrument_layer to be more flexible\n",
    "                block_kfac_handlers = instrument_layer(block) \n",
    "                self.all_kfac_handlers.extend(block_kfac_handlers)\n",
    "        else:\n",
    "            print(f\"Warning: GRITAdapter KFAC instrumentation could not find a suitable blocks attribute (tried {model_blocks_attr_names}). KFAC might not be fully set up.\")\n",
    "\n",
    "        # Initialize FisherEigen and SubspaceBuffer based on KFAC handlers\n",
    "        # This assumes KFAC handlers are for nn.Linear layers that were also LoRA adapted.\n",
    "        # The number of \"blocks\" for SubspaceBuffer should match num KFAC-instrumented entities.\n",
    "        num_kfac_instrumented_entities = len(self.all_kfac_handlers)\n",
    "        if num_kfac_instrumented_entities > 0:\n",
    "            # device = next(self.model.parameters()).device # Get model device\n",
    "            # Assuming all KFAC handlers and the model are on the same device\n",
    "            # Get device from the first KFAC handler's A_factor if available, else model param\n",
    "            if self.all_kfac_handlers and self.all_kfac_handlers[0].A_factor is not None:\n",
    "                 device = self.all_kfac_handlers[0].A_factor.device\n",
    "            else:\n",
    "                 try:\n",
    "                     device = next(self.model.parameters()).device\n",
    "                 except StopIteration:\n",
    "                     device = torch.device(\"cpu\") # Default if model has no parameters\n",
    "\n",
    "            self.subspace_manager = SubspaceBuffer(num_blocks=num_kfac_instrumented_entities,\n",
    "                                                   buffer_size=self.subspace_buffer_size,\n",
    "                                                   device=device) # Pass device\n",
    "            for i, handler in enumerate(self.all_kfac_handlers):\n",
    "                self.fisher_engines[handler.layer_type] = FisherEigen().to(device) # Ensure FisherEigen is on the correct device\n",
    "        else:\n",
    "            print(\"Warning: No KFAC handlers found. FisherEigen and SubspaceBuffer not initialized.\")\n",
    "\n",
    "\n",
    "        # Initialize SVTrackers and RankSchedulers for FlexLoRA layers\n",
    "        min_r, max_r = self.rank_range\n",
    "        initial_r = min_r # As per lora_inject\n",
    "        for name, module in self.model.named_modules():\n",
    "            if isinstance(module, FlexLoRA):\n",
    "                self.sv_trackers[name] = SVTracker(module, window_size=self.sv_tracker_window_size)\n",
    "                self.rank_schedulers[name] = RankScheduler(\n",
    "                    initial_rank=module.initial_rank, # Use FlexLoRA's actual initial_rank\n",
    "                    min_rank=module.min_rank,\n",
    "                    max_rank=module.max_rank,\n",
    "                    warmup_steps=self.rank_scheduler_warmup_steps,\n",
    "                    ema_alpha=self.rank_scheduler_ema_alpha\n",
    "                )\n",
    "\n",
    "    def _update_kfac_factors_and_fisher(self):\n",
    "        \"\"\"\n",
    "        Updates KFAC factors (implicitly via hooks during forward/backward),\n",
    "        then updates Fisher blocks and decomposes them.\n",
    "        This method assumes a forward and backward pass has just occurred.\n",
    "        \"\"\"\n",
    "        if not self.all_kfac_handlers or self.subspace_manager is None:\n",
    "            # print(\"KFAC or SubspaceManager not initialized. Skipping Fisher updates.\")\n",
    "            return\n",
    "\n",
    "        for i, handler in enumerate(self.all_kfac_handlers):\n",
    "            kfac_A = handler.A_factor\n",
    "            kfac_B = handler.B_factor\n",
    "            fisher_engine = self.fisher_engines.get(handler.layer_type)\n",
    "\n",
    "            if fisher_engine is None:\n",
    "                # print(f\"No FisherEngine for {handler.layer_type}. Skipping.\")\n",
    "                continue\n",
    "            \n",
    "            if kfac_A is None or kfac_B is None or kfac_A.numel() == 0 or kfac_B.numel() == 0:\n",
    "                # print(f\"Warning: KFAC factors for {handler.layer_type} are None or empty. Skipping Fisher update for this handler.\")\n",
    "                continue\n",
    "\n",
    "            fisher_engine.update_fisher(A=kfac_A, B=kfac_B)\n",
    "            if fisher_engine.fisher_block is not None and fisher_engine.fisher_block.numel() > 0:\n",
    "                eig_vals, eig_vecs = fisher_engine.decompose(k=self.fisher_k)\n",
    "                if eig_vecs is not None:\n",
    "                    self.subspace_manager.update_buffer(block_idx=i, new_eigenvectors=eig_vecs)\n",
    "                # else:\n",
    "                    # print(f\"Fisher decomposition failed for {handler.layer_type}. Subspace buffer not updated.\")\n",
    "            # else:\n",
    "                # print(f\"Fisher block for {handler.layer_type} is empty. Subspace buffer not updated.\")\n",
    "\n",
    "\n",
    "    def _apply_neural_reprojection(self):\n",
    "        for name, module in self.model.named_modules():\n",
    "            if isinstance(module, FlexLoRA):\n",
    "                if module.current_rank > 0: # Only reproject if LoRA is active\n",
    "                    # The neural_reprojection function takes the LoRA layer itself.\n",
    "                    # It uses compute_lora_fisher internally.\n",
    "                    neural_reprojection(\n",
    "                        lora_layer=module,\n",
    "                        rho_eigen_sum=self.rho_eigen_sum,\n",
    "                        damping_lora_fisher=self.damping_lora_fisher # Pass damping for its internal Fisher\n",
    "                    )\n",
    "\n",
    "    def _perform_rank_adjustment(self):\n",
    "        for name, module in self.model.named_modules():\n",
    "            if isinstance(module, FlexLoRA):\n",
    "                sv_tracker = self.sv_trackers.get(name)\n",
    "                rank_sched = self.rank_schedulers.get(name)\n",
    "                if sv_tracker and rank_sched:\n",
    "                    sv_tracker.forward() # Update SV history\n",
    "                    rank_adjustment(\n",
    "                        layer=module,\n",
    "                        sv_tracker=sv_tracker,\n",
    "                        rank_scheduler=rank_sched,\n",
    "                        sv_history_min_samples=self.sv_history_min_samples\n",
    "                    )\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"\n",
    "        Performs a single GRIT optimization step.\n",
    "        This should be called after model.backward() and before optimizer.zero_grad()\n",
    "        if an external optimizer is managing non-GRIT parameters.\n",
    "        If GRIT manages all trainable parameters (LoRA params), then this\n",
    "        function handles the update.\n",
    "\n",
    "        Args:\n",
    "            closure: An optional closure that re-evaluates the model and returns the loss.\n",
    "                     This is useful if KFAC factors need to be updated based on a fresh pass,\n",
    "                     though typically KFAC hooks handle this during the main forward/backward.\n",
    "        \"\"\"\n",
    "        # 1. Update KFAC factors (usually done by hooks) and then Fisher information\n",
    "        # This assumes forward/backward has just happened.\n",
    "        self._update_kfac_factors_and_fisher()\n",
    "\n",
    "        # 2. Natural Gradient Step for LoRA parameters\n",
    "        # This replaces the standard optimizer step for LoRA parameters.\n",
    "        natural_gradient_step(self.model, lr=self.learning_rate, damping_lora_fisher=self.damping_lora_fisher)\n",
    "\n",
    "        # 3. Neural Reprojection\n",
    "        self._apply_neural_reprojection()\n",
    "\n",
    "        # 4. Rank Adjustment (includes SV tracking)\n",
    "        self._perform_rank_adjustment()\n",
    "\n",
    "        # 5. Parameter Fusion (Momentum Smoothing)\n",
    "        self.momentum_fuser(self.model) # Apply momentum to LoRA parameters\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        \"\"\"Forward pass through the underlying model.\"\"\"\n",
    "        return self.model(*args, **kwargs)\n",
    "\n",
    "    def parameters(self, recurse: bool = True):\n",
    "        \"\"\"Returns parameters of the adapted model that are trainable (LoRA params).\"\"\"\n",
    "        return filter(lambda p: p.requires_grad, self.model.parameters(recurse))\n",
    "\n",
    "    def named_parameters(self, prefix: str = '', recurse: bool = True):\n",
    "        \"\"\"Returns named parameters of the adapted model that are trainable.\"\"\"\n",
    "        return filter(lambda kv: kv[1].requires_grad, self.model.named_parameters(prefix, recurse))\n",
    "\n",
    "    def state_dict(self, destination=None, prefix='', keep_vars=False):\n",
    "        \"\"\"Returns a state dict containing only the LoRA parameters.\"\"\"\n",
    "        # It's often better to save the state_dict of the original model,\n",
    "        # and then separately save LoRA parameters if needed, or re-inject on load.\n",
    "        # However, for a pure adapter, one might want to save only LoRA parts.\n",
    "        lora_state_dict = {}\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad: # Assuming only LoRA parameters are trainable\n",
    "                 if any(sub in name for sub in self.layers_to_adapt_substrings): # Further check if it's a LoRA param\n",
    "                    lora_state_dict[name] = param.data\n",
    "        return lora_state_dict # This might need refinement based on FlexLoRA naming\n",
    "\n",
    "    def load_state_dict(self, state_dict, strict=True):\n",
    "        \"\"\"Loads LoRA parameters into the model.\"\"\"\n",
    "        # This needs to carefully map state_dict keys to the FlexLoRA parameters.\n",
    "        # It's safer to load the full model and then re-apply GRIT,\n",
    "        # or to have a dedicated save/load for LoRA parameters.\n",
    "        # For now, this is a placeholder. A robust implementation would iterate\n",
    "        # through FlexLoRA modules and load 'lora_A' and 'lora_B'.\n",
    "        # self.model.load_state_dict(state_dict, strict=False) # Be careful with strict=False\n",
    "\n",
    "        # A more targeted way for FlexLoRA:\n",
    "        current_model_state_dict = self.model.state_dict()\n",
    "        for name, param_data in state_dict.items():\n",
    "            if name in current_model_state_dict:\n",
    "                # Check if this parameter belongs to a FlexLoRA module and should be loaded\n",
    "                # This assumes keys in state_dict directly match model param names (e.g., \"blocks.0.attn.q_proj.lora_A\")\n",
    "                module_path = name.split('.')[:-1] # e.g., ['blocks', '0', 'attn', 'q_proj']\n",
    "                param_short_name = name.split('.')[-1] # e.g., 'lora_A'\n",
    "                \n",
    "                try:\n",
    "                    parent_module_path = \".\".join(module_path)\n",
    "                    parent_module = self.model.get_submodule(parent_module_path)\n",
    "                    if isinstance(parent_module, FlexLoRA) and (param_short_name == 'lora_A' or param_short_name == 'lora_B'):\n",
    "                         current_model_state_dict[name].copy_(param_data)\n",
    "                    # else if not a FlexLoRA or not lora_A/B, it might be other trainable param if any\n",
    "                except AttributeError:\n",
    "                    if strict:\n",
    "                        raise RuntimeError(f\"Error loading {name} into GRITAdapter: module path not found.\")\n",
    "            elif strict:\n",
    "                raise RuntimeError(f\"Error loading {name} into GRITAdapter: parameter not found in model.\")\n",
    "        \n",
    "        # After potentially updating parts of the state dict, load it into the model.\n",
    "        # This approach is still a bit risky if state_dict contains non-LoRA params.\n",
    "        # A truly clean way is to ensure state_dict ONLY contains LoRA weights\n",
    "        # and load them specifically.\n",
    "        # For simplicity here, if state_dict is purely LoRA, this would be fine:\n",
    "        self.model.load_state_dict(state_dict, strict=strict)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
