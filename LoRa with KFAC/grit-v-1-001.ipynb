{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T05:33:53.228642Z",
     "iopub.status.busy": "2025-05-13T05:33:53.228362Z",
     "iopub.status.idle": "2025-05-13T05:35:16.029298Z",
     "shell.execute_reply": "2025-05-13T05:35:16.028376Z",
     "shell.execute_reply.started": "2025-05-13T05:33:53.228623Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install transformers peft datasets torch bitsandbytes tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T05:35:16.031235Z",
     "iopub.status.busy": "2025-05-13T05:35:16.030842Z",
     "iopub.status.idle": "2025-05-13T05:35:22.514534Z",
     "shell.execute_reply": "2025-05-13T05:35:22.513644Z",
     "shell.execute_reply.started": "2025-05-13T05:35:16.031211Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45.5\n"
     ]
    }
   ],
   "source": [
    "import bitsandbytes\n",
    "print(bitsandbytes.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T05:35:22.515653Z",
     "iopub.status.busy": "2025-05-13T05:35:22.515329Z",
     "iopub.status.idle": "2025-05-13T05:35:41.065950Z",
     "shell.execute_reply": "2025-05-13T05:35:41.065188Z",
     "shell.execute_reply.started": "2025-05-13T05:35:22.515633Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.4/21.4 MB\u001b[0m \u001b[31m74.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install wandb -qU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T08:54:10.998432Z",
     "iopub.status.busy": "2025-05-12T08:54:10.997545Z",
     "iopub.status.idle": "2025-05-12T08:54:12.690283Z",
     "shell.execute_reply": "2025-05-12T08:54:12.689540Z",
     "shell.execute_reply.started": "2025-05-12T08:54:10.998395Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T05:35:44.364625Z",
     "iopub.status.busy": "2025-05-13T05:35:44.364346Z",
     "iopub.status.idle": "2025-05-13T05:35:44.405053Z",
     "shell.execute_reply": "2025-05-13T05:35:44.403878Z",
     "shell.execute_reply.started": "2025-05-13T05:35:44.364600Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "wandb.login(key=\"your-secret-key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-13T05:35:44.405676Z",
     "iopub.status.idle": "2025-05-13T05:35:44.406063Z",
     "shell.execute_reply": "2025-05-13T05:35:44.405883Z",
     "shell.execute_reply.started": "2025-05-13T05:35:44.405863Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import io\n",
    "from PIL import Image\n",
    "\n",
    "def log_lora_weights_to_wandb(model, pattern=\"lora_A\"):\n",
    "    for name, param in model.named_parameters():\n",
    "        if pattern in name:\n",
    "            param_data = param.detach().cpu().numpy()\n",
    "            plt.figure(figsize=(6, 3))\n",
    "            plt.imshow(param_data, aspect='auto', cmap='viridis')\n",
    "            plt.colorbar()\n",
    "            plt.title(f\"{name} - shape: {param_data.shape}\")\n",
    "            plt.xlabel(\"Columns\")\n",
    "            plt.ylabel(\"Rows\")\n",
    "            plt.tight_layout()\n",
    "\n",
    "            # Save plot to an image buffer\n",
    "            buf = io.BytesIO()\n",
    "            plt.savefig(buf, format='png')\n",
    "            buf.seek(0)\n",
    "            image = Image.open(buf)\n",
    "\n",
    "            # Log to wandb\n",
    "            wandb.log({f\"{name}\": wandb.Image(image)})\n",
    "\n",
    "            plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T10:35:27.168349Z",
     "iopub.status.busy": "2025-05-12T10:35:27.168065Z",
     "iopub.status.idle": "2025-05-12T10:35:27.174296Z",
     "shell.execute_reply": "2025-05-12T10:35:27.173570Z",
     "shell.execute_reply.started": "2025-05-12T10:35:27.168330Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "\n",
    "class LogLoRAWeightsCallback(TrainerCallback):\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        for name, param in kwargs['model'].named_parameters():\n",
    "            if \"lora_A\" in name or \"lora_B\" in name:\n",
    "                wandb.log({f\"weights/{name}\": wandb.Histogram(param.detach().cpu().numpy())}, step=state.global_step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T11:13:31.772642Z",
     "iopub.status.busy": "2025-05-12T11:13:31.772338Z",
     "iopub.status.idle": "2025-05-12T11:15:34.791553Z",
     "shell.execute_reply": "2025-05-12T11:15:34.791046Z",
     "shell.execute_reply.started": "2025-05-12T11:13:31.772620Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import logging\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from datasets import load_dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "\n",
    "writer = SummaryWriter(log_dir=\"runs/lora_experiment\")\n",
    "\n",
    "\n",
    "\n",
    "for handler in logging.root.handlers[:]:\n",
    "    print(handler)\n",
    "    logging.root.removeHandler(handler)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"training.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class LoRAHook:\n",
    "    def __init__(self, writer = None):\n",
    "        self.inputs = {}\n",
    "        self.grads = {}\n",
    "\n",
    "    def capture_input(self, name):\n",
    "        \n",
    "        def hook(module, input, output):\n",
    "            \n",
    "            if input[0] is not None:\n",
    "                self.inputs[name] = input[0].detach()\n",
    "        return hook\n",
    "\n",
    "    def capture_grad(self, name):\n",
    "        def hook(module, grad_input, grad_output):\n",
    "            if grad_output[0] is not None:\n",
    "                self.grads[name] = grad_output[0].detach()\n",
    "               \n",
    "        return hook\n",
    "\n",
    "    def check_grad(self,name):\n",
    "        def hook(module, grad_input, grad_output):\n",
    "            print(f\"[HOOK] {name} got gradient: {grad_output[0] is not None}\")\n",
    "        return hook\n",
    "\n",
    "    \n",
    "\n",
    "    def attach(self, model):\n",
    "        for name, module in model.named_modules():\n",
    "            if \"lora_A\" in name or \"lora_B\" in name:\n",
    "                # logger.info(f\"Attaching hooks to {name}\")\n",
    "                module.register_forward_hook(self.capture_input(name))\n",
    "                module.register_full_backward_hook(self.capture_grad(name))\n",
    "                # module.register_full_backward_hook(self.check_grad(name))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def fisher_kfac_lora(input_act, output_grad, damping=1e-3):\n",
    "    # logger.info(f\"[DEBUG] input_act shape: {input_act.shape}\")\n",
    "    # logger.info(f\"[DEBUG] output_grad shape: {output_grad.shape}\")\n",
    "    try:\n",
    "        B, T, D_in = input_act.shape\n",
    "        _, _, D_out = output_grad.shape\n",
    "\n",
    "        # Flatten (B*T, D)\n",
    "        input_flat = input_act.reshape(B * T, D_in)\n",
    "        output_flat = output_grad.reshape(B * T, D_out)\n",
    "\n",
    "        A = (input_flat.T @ input_flat) / (B * T) + damping * torch.eye(D_in, device=input_act.device)\n",
    "        G = (output_flat.T @ output_flat) / (B * T) + damping * torch.eye(D_out, device=output_grad.device)\n",
    "\n",
    "        return A, G\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in fisher_kfac_lora: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def apply_natural_gradient(model, lora_hook, damping=1e-3):\n",
    "    logger.info(\"Applying Natural Gradient update to LoRA weights\")\n",
    "    device = model.device\n",
    "    for name, module in model.named_modules():\n",
    "        if \"lora_A\" in name or \"lora_B\" in name:\n",
    "            if name in lora_hook.inputs and name in lora_hook.grads:\n",
    "                a = lora_hook.inputs[name].to(device)\n",
    "                g = lora_hook.grads[name].to(device)\n",
    "                A, G = fisher_kfac_lora(a, g, damping)\n",
    "                if A is None or G is None:\n",
    "                    continue\n",
    "                try:\n",
    "                    A_inv = torch.linalg.inv(A)\n",
    "                    G_inv = torch.linalg.inv(G)\n",
    "                    weight = module.weight\n",
    "                    if weight.grad is None:\n",
    "                        logger.warning(f\"No gradient for {name}\")\n",
    "                        continue\n",
    "                    grad = weight.grad.detach()\n",
    "                    ng_update = A_inv @ grad @ G_inv\n",
    "                    with torch.no_grad():\n",
    "                        weight.add_(-0.01 * ng_update)\n",
    "                    logger.info(f\"Applied NG update to {name}\")\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error in NG update for {name}: {e}\")\n",
    "\n",
    "def generate_sample(model, tokenizer, prompt, max_length=50):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(**inputs, max_length=max_length, num_return_sequences=1)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "def main():\n",
    "    device = torch.device(\"cpu\")\n",
    "    logger.info(f\"Using device: {device}\")\n",
    "\n",
    "    model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "    logger.info(f\"Loading model: {model_name}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "\n",
    "\n",
    "    lora_config = LoraConfig(\n",
    "        r=8,\n",
    "        lora_alpha=32,\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.CAUSAL_LM\n",
    "    )\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    hyperparameters = {\n",
    "        \"model\": \"TinyLlama-1.1B-Chat-v1.0\",\n",
    "        \"lr\": 2e-4,\n",
    "        \"batch_size\": 1,\n",
    "        \"epochs\": 2,\n",
    "        \"lora_r\": 8,\n",
    "        \"lora_alpha\": 32,\n",
    "        \"lora_dropout\": 0.05,\n",
    "    }\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    \n",
    "    wandb.init(\n",
    "        project=\"tinyllama-lora-alpaca\",\n",
    "        name=f\"experiment-{timestamp}\",  \n",
    "        # sync_tensorboard=True,\n",
    "        config=hyperparameters  \n",
    "    )\n",
    "\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "    initial_weights = {}\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if \"lora_\" in name:\n",
    "            initial_weights[name] = param.detach().clone().to(param.device)\n",
    "\n",
    "\n",
    "    lora_hook = LoRAHook()\n",
    "    lora_hook.attach(model)\n",
    "\n",
    "    logger.info(\"Loading dataset\")\n",
    "    dataset = load_dataset(\"tatsu-lab/alpaca\", split=\"train[:1000]\")\n",
    "\n",
    "    def format_prompt(example):\n",
    "        instruction = example[\"instruction\"]\n",
    "        input_text = example[\"input\"]\n",
    "        response = example[\"output\"]\n",
    "        prompt = f\"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input_text}\\n\\n### Response:\\n{response}\" if input_text else f\"### Instruction:\\n{instruction}\\n\\n### Response:\\n{response}\"\n",
    "        return {\"text\": prompt}\n",
    "\n",
    "    dataset = dataset.map(format_prompt)\n",
    "\n",
    "    def tokenize(example):\n",
    "        output = tokenizer(\n",
    "            example[\"text\"],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=512,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        output[\"input_ids\"] = output[\"input_ids\"].squeeze().to(\"cpu\")\n",
    "        output[\"labels\"] = output[\"input_ids\"].clone()\n",
    "        return output\n",
    "\n",
    "    tokenized = dataset.map(tokenize, batched=True, remove_columns=dataset.column_names)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./lora-tinyllama-alpaca\",\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=1,\n",
    "        num_train_epochs=2,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=False,\n",
    "        bf16=False,\n",
    "        logging_steps=50,\n",
    "        save_steps=5,\n",
    "        save_total_limit=1,\n",
    "        report_to=\"wandb\",\n",
    "        label_names=[\"labels\"]\n",
    "    )\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "    def custom_data_collator(features):\n",
    "        batch = data_collator(features)\n",
    "        return {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "    logger.info(\"Starting training\")\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized,\n",
    "        processing_class=tokenizer,\n",
    "        data_collator=custom_data_collator,\n",
    "        callbacks=[LogLoRAWeightsCallback()] \n",
    "    )\n",
    "    trainer.train()\n",
    "\n",
    "    apply_natural_gradient(model, lora_hook)\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    import os\n",
    "\n",
    "    os.makedirs(\"lora_diff_plots\", exist_ok=True)\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if \"lora_\" in name and name in initial_weights:\n",
    "            before = initial_weights[name].to(param.device)\n",
    "            after = param.detach()\n",
    "    \n",
    "            # Difference mask (1 where changed)\n",
    "            diff_mask = (before != after).cpu().int()\n",
    "    \n",
    "            plt.figure(figsize=(6, 6))\n",
    "            plt.imshow(diff_mask, cmap=\"Greens\", interpolation=\"nearest\")\n",
    "            plt.title(f\"Changed Params in {name}\")\n",
    "            plt.axis(\"off\")\n",
    "    \n",
    "            # Save plot\n",
    "            plt.savefig(f\"lora_diff_plots/{name.replace('.', '_')}.png\")\n",
    "            plt.close()\n",
    "\n",
    "\n",
    "    logger.info(\"Analyzing LoRA weights\")\n",
    "    for name in lora_hook.inputs:\n",
    "        a = lora_hook.inputs[name].to(device)\n",
    "        g = lora_hook.grads[name].to(device)\n",
    "        A, G = fisher_kfac_lora(a, g)\n",
    "        if A is not None:\n",
    "            wandb.log({f\"{name}_A_norm\": torch.norm(A).item(), f\"{name}_G_norm\": torch.norm(G).item()})\n",
    "            # logger.info(f\"{name} — A shape: {A.shape}, G shape: {G.shape}\")\n",
    "\n",
    "    logger.info(\"Saving model\")\n",
    "    model.save_pretrained(\"./lora-tinyllama-alpaca\")\n",
    "    tokenizer.save_pretrained(\"./tinyllama-lora-alpaca\")\n",
    "\n",
    "    logger.info(\"Generating sample output\")\n",
    "    prompt = \"What is the capital of France?\"\n",
    "    response = generate_sample(model, tokenizer, prompt)\n",
    "    logger.info(f\"Sample prompt: {prompt}\\nResponse: {response}\")\n",
    "    wandb.log({\"sample_prompt\": prompt, \"sample_response\": response})  \n",
    "\n",
    "    wandb.finish()  # ✅ Finish WandB run\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-12T11:19:31.180997Z",
     "iopub.status.busy": "2025-05-12T11:19:31.180354Z",
     "iopub.status.idle": "2025-05-12T11:19:41.776141Z",
     "shell.execute_reply": "2025-05-12T11:19:41.775498Z",
     "shell.execute_reply.started": "2025-05-12T11:19:31.180971Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the capital of France?\n",
      "What is the currency of France?\n",
      "What is the population of France?\n",
      "What is the official language of France?\n",
      "What is the religion of France?\n",
      "What is the capital of Germany?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./tinyllama-lora-alpaca\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./lora-tinyllama-alpaca\")\n",
    "prompt = \"What is the capital of France?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_length=50)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T15:46:19.080089Z",
     "iopub.status.busy": "2025-05-07T15:46:19.079588Z",
     "iopub.status.idle": "2025-05-07T15:46:30.674025Z",
     "shell.execute_reply": "2025-05-07T15:46:30.673314Z",
     "shell.execute_reply.started": "2025-05-07T15:46:19.080069Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is 2 + 2?\n",
      "2 + 2 = 4\n",
      "\n",
      "### Response:\n",
      "The answer is 4. 2 + 2 is 4. This is the sum of 2 and 2.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./tinyllama-lora-alpaca\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./lora-tinyllama-alpaca\")\n",
    "prompt = \"What is 2 + 2?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_length=50)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
